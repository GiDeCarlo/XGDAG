{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n",
      "2.0.4\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import networkx as nx\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.nn.conv import SAGEConv\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(torch.__version__)\n",
    "print(torch_geometric.__version__)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN4L_Sage (nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(data.num_features, 16)\n",
    "        self.conv2 = SAGEConv(16, 16)\n",
    "        self.conv3 = SAGEConv(16, 16)\n",
    "        self.conv4 = SAGEConv(16, int(data.num_classes))\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv4(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "class GNN7L_Sage (nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(data.num_features, 16, aggr='max')\n",
    "        self.conv2 = SAGEConv(16, 16, aggr='max')\n",
    "        self.conv3 = SAGEConv(16, 16, aggr='max')\n",
    "        self.conv4 = SAGEConv(16, 16, aggr='max')\n",
    "        self.conv5 = SAGEConv(16, 16, aggr='max')\n",
    "        self.conv6 = SAGEConv(16, 16, aggr='max')\n",
    "        self.conv7 = SAGEConv(16, int(data.num_classes), aggr='max')\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = F.relu(self.conv5(x, edge_index))\n",
    "        x = F.relu(self.conv6(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv7(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class: it allows to translate a vector (Graph, Attributes, Labels)\n",
    "# into a dataset compatible with the PyTorch models.\n",
    "# \n",
    "# Parameters:\n",
    "# - G: NetworkX graph\n",
    "# - Labels: of the nodes used for classification\n",
    "# - attributes: List of the nodes' attributes\n",
    "\n",
    "class MyDataset(InMemoryDataset):\n",
    "  def __init__(self, G, labels, attributes, num_classes=2):\n",
    "    super(MyDataset, self).__init__('.', None, None, None)\n",
    "\n",
    "    # import data from the networkx graph with the attributes of the nodes\n",
    "    data = from_networkx(G, attributes)\n",
    "      \n",
    "    y = torch.from_numpy(labels).type(torch.long)\n",
    "\n",
    "    data.x = data.x.float()\n",
    "    data.y = y.clone().detach()\n",
    "    data.num_classes = num_classes\n",
    "\n",
    "    # Using train_test_split function from sklearn to stratify train/test/val sets\n",
    "    indices = range(G.number_of_nodes())\n",
    "    # Stratified split of train/test/val sets. Returned indices are used to create the masks\n",
    "    X_train, X_test, y_train, y_test, train_idx, test_idx = train_test_split(data.x, data.y, indices, test_size=0.3, stratify=labels, random_state=42)\n",
    "    # To create validation set, test set is splitted in half\n",
    "    X_test, X_val, y_test, y_val, test_idx, val_idx = train_test_split(X_test, y_test, test_idx, test_size=0.5, stratify=y_test, random_state=42)\n",
    "\n",
    "    n_nodes = G.number_of_nodes()\n",
    "    train_mask  = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "    test_mask   = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "    val_mask    = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "    \n",
    "    for idx in train_idx:\n",
    "      train_mask[idx] = True\n",
    "\n",
    "    for idx in test_idx:\n",
    "      test_mask[idx] = True\n",
    "    \n",
    "    for idx in val_idx:\n",
    "      val_mask[idx] = True\n",
    "\n",
    "    data['train_mask']  = train_mask\n",
    "    data['test_mask']   = test_mask\n",
    "    data['val_mask']    = val_mask\n",
    "\n",
    "    self.data, self.slices = self.collate([data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, epochs = 200, classes = ['0','1'], lr = 0.001, weight_decay=0, cm_title = 'GNN'):\n",
    "    title = cm_title + '_' + str(epochs) + '_' + str(weight_decay).replace('.', '_')\n",
    "\n",
    "    model_path  = 'Models/' + title\n",
    "    image_path  = 'Images/' + title\n",
    "    report_path = 'Reports/' + title + '.csv'\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    # optimizer = torch.optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    train_mask  = data['train_mask']\n",
    "    test_mask   = data['test_mask']\n",
    "    val_mask    = data['val_mask']\n",
    "\n",
    "    labels    = data.y\n",
    "    output = ''\n",
    "\n",
    "    # list to plot the train accuracy\n",
    "    train_acc_curve = []\n",
    "    train_lss_curve = []\n",
    "\n",
    "    best_train_acc  = 0\n",
    "    best_val_acc    = 0\n",
    "    # best_test_acc   = 0\n",
    "    best_train_lss  = 999\n",
    "    best_loss_epoch = 0\n",
    "\n",
    "    for e in tqdm(range(epochs+1)):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        logits      = model(data)\n",
    "        output      = logits.argmax(1)\n",
    "        #Â train_loss  = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
    "        train_loss  = F.nll_loss(logits[train_mask], labels[train_mask])\n",
    "        train_acc   = (output[train_mask] == labels[train_mask]).float().mean()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Append train acc. to plot curve later\n",
    "        train_acc_curve.append(train_acc.item())\n",
    "        train_lss_curve.append(train_loss.item())\n",
    "\n",
    "        if train_acc > best_train_acc:\n",
    "            best_train_acc = train_acc\n",
    "\n",
    "        # Evaluation and test\n",
    "        model.eval()\n",
    "        logits      = model(data)\n",
    "        output      = logits.argmax(1)\n",
    "        # val_loss    = F.cross_entropy(logits[val_mask], labels[val_mask])\n",
    "        val_loss    = F.nll_loss(logits[val_mask], labels[val_mask])\n",
    "        val_acc     = (output[val_mask] == labels[val_mask]).float().mean()\n",
    "        # test_acc    = (output[test_mask] == labels[test_mask]).float().mean()\n",
    "\n",
    "        # Update best test/val acc.\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "        \n",
    "        # if test_acc > best_test_acc:\n",
    "        #     best_test_acc = test_acc\n",
    "        \n",
    "        # Save model with best train loss\n",
    "        if train_loss < best_train_lss:\n",
    "            best_train_lss = train_loss\n",
    "            best_loss_epoch = e\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        if e % 20 == 0 or e == epochs:\n",
    "            print('[Epoch: {:04d}]'.format(e),\n",
    "            'train loss: {:.4f},'.format(train_loss.item()),\n",
    "            'train acc: {:.4f},'.format(train_acc.item()),\n",
    "            'val loss: {:.4f},'.format(val_loss.item()),\n",
    "            'val acc: {:.4f} '.format(val_acc.item()),\n",
    "            '(best train acc: {:.4f},'.format(best_train_acc.item()),\n",
    "            'best val acc: {:.4f},'.format(best_val_acc.item()),\n",
    "            'best train loss: {:.4f} '.format(best_train_lss.item()),\n",
    "            '@ epoch', best_loss_epoch ,')')\n",
    "    \n",
    "    # Plot training accuracy curve\n",
    "    plt.figure(figsize = (12,7))\n",
    "    plt.plot(train_acc_curve)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Train Accuracy')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize = (12,7))\n",
    "    plt.plot(train_lss_curve)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Train Loss')\n",
    "    plt.show()\n",
    "\n",
    "    # Load best model\n",
    "    loaded_model = GNN7L_Sage(data).to(device)\n",
    "    loaded_model.load_state_dict(torch.load(model_path))\n",
    "    loaded_model.eval()\n",
    "    logits = loaded_model(data)\n",
    "    output = logits.argmax(1)\n",
    "\n",
    "    print(classification_report(labels[test_mask].to('cpu'), output[test_mask].to('cpu')))\n",
    "\n",
    "    class_report = classification_report(labels[test_mask].to('cpu'), output[test_mask].to('cpu'), output_dict=True)\n",
    "    classification_report_dataframe = pd.DataFrame(class_report)\n",
    "    classification_report_dataframe.to_csv(report_path)\n",
    "\n",
    "    #Confusion Matrix\n",
    "    norms = [None, \"true\"]\n",
    "    for norm in norms:\n",
    "        cm = confusion_matrix(labels[test_mask].to('cpu'), output[test_mask].to('cpu'), normalize=norm)\n",
    "\n",
    "        plt.figure(figsize=(7,7))\n",
    "        \n",
    "        if norm == \"true\":\n",
    "            sn.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'BuPu', xticklabels = classes, yticklabels = classes)\n",
    "        else:\n",
    "            sn.heatmap(cm, annot=True, fmt=\".0f\", linewidths=.5, square = True, cmap = 'BuPu', xticklabels = classes, yticklabels = classes)\n",
    "        plt.title(cm_title)\n",
    "        plt.ylabel('Actual label')\n",
    "        plt.xlabel('Predicted label')\n",
    "\n",
    "        if norm == None:\n",
    "            plt.savefig(image_path + '_notNorm.png')\n",
    "        else:\n",
    "            plt.savefig(image_path + '_Norm.png')\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeDBIT Multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18736, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "G = nx.read_gml('Graphs/graph_with_normalized_nedbit.gml')\n",
    "\n",
    "seed_genes          = pd.read_csv('Datasets/C0006142_Malignant_neoplasm_of_breast_seed_genes.txt', header=None, sep=' ')\n",
    "seed_genes.columns  = [\"name\", \"GDA Score\"]\n",
    "seeds_list          = seed_genes[\"name\"].values.tolist()\n",
    "\n",
    "nedbit_scores = pd.read_csv('Datasets/C0006142_Malignant_neoplasm_of_breast_features_Score.csv')\n",
    "\n",
    "# Remove seed genes\n",
    "nedbit_scores_not_seed = nedbit_scores[~nedbit_scores['name'].isin(seeds_list)]\n",
    "print(nedbit_scores_not_seed.shape)\n",
    "\n",
    "# Sort scores for quartile division\n",
    "nedbit_scores_not_seed = nedbit_scores_not_seed.sort_values(by = \"out\", ascending = False)\n",
    "pseudo_labels = pd.qcut(x = nedbit_scores_not_seed[\"out\"], q = 4, labels = [\"RN\", \"LN\", \"WN\", \"LP\"])\n",
    "\n",
    "plt.hist(pseudo_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19761"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nedbit_scores_not_seed['label'] = pseudo_labels\n",
    "\n",
    "nedbit_scores_seed = nedbit_scores[nedbit_scores['name'].isin(seeds_list)]\n",
    "nedbit_scores_seed = nedbit_scores_seed.assign(label = 'P')\n",
    "\n",
    "# Convert dataframe to dict for searching nodes and their labels\n",
    "not_seed_labels = dict(zip(nedbit_scores_not_seed['name'], nedbit_scores_not_seed['label']))\n",
    "seed_labels     = dict(zip(nedbit_scores_seed['name'], nedbit_scores_seed['label']))\n",
    "\n",
    "labels_dict = {'P':0, 'LP': 1, 'WN': 2, 'LN': 3, 'RN': 4}\n",
    "labels = []\n",
    "\n",
    "for node in G:\n",
    "    if node in not_seed_labels:\n",
    "        labels.append(labels_dict[not_seed_labels[node]])\n",
    "    else:\n",
    "        labels.append(labels_dict[seed_labels[node]])\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOv0lEQVR4nO3dcaid9X3H8fen0Vqhkyq5upCb7ToIY1FoqyHLEIbUglktjX9USKE1DEeYWGjZoIv9Y6V/BPyrFMd0SFuMtKsEWmawlSFppRSc7tra2pg6s+k0GEzqaGvZcGi/++P8Boebc+85N957Tszv/YLDec73+T3n+Z6fJ588eZ5zjqkqJEl9eNesG5AkTY+hL0kdMfQlqSOGviR1xNCXpI5cMOsGxtm4cWMtLCzMug1Jekd56qmnflFVc0vr53zoLywssLi4OOs2JOkdJcl/jqp7ekeSOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpyzn8jV6uzsP87M9nvi3fdNJP9ztKs5hpmN989vr/Ot9fskb4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOTBz6STYk+XGSh9vjy5I8muT5dn/p0Ng7kxxP8lySG4fq1yZ5pq27O0nW9uVIklaymiP9zwDHhh7vB45U1VbgSHtMkm3AHuAqYBdwT5INbZt7gX3A1nbb9ba6lyStykShn2QeuAn4ylB5N3CwLR8Ebh6qP1hVb1TVC8BxYEeSTcAlVfV4VRXwwNA2kqQpmPRI/8vA54DfDtWuqKqTAO3+8lbfDLw8NO5Eq21uy0vrZ0iyL8liksXTp09P2KIkaZyxoZ/ko8CpqnpqwuccdZ6+VqifWay6r6q2V9X2ubm5CXcrSRrnggnGXAd8LMlHgPcAlyT5OvBqkk1VdbKdujnVxp8AtgxtPw+80urzI+qSpCkZe6RfVXdW1XxVLTC4QPu9qvokcBjY24btBR5qy4eBPUkuSnIlgwu2T7ZTQK8n2dk+tXPr0DaSpCmY5Eh/OXcBh5LcBrwE3AJQVUeTHAKeBd4E7qiqt9o2twP3AxcDj7SbJGlKVhX6VfUY8Fhbfg24YZlxB4ADI+qLwNWrbVKStDb8Rq4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktSRsaGf5D1JnkzykyRHk3yx1S9L8miS59v9pUPb3JnkeJLnktw4VL82yTNt3d1Jsj4vS5I0yiRH+m8AH6qq9wMfAHYl2QnsB45U1VbgSHtMkm3AHuAqYBdwT5IN7bnuBfYBW9tt19q9FEnSOGNDvwZ+0x5e2G4F7AYOtvpB4Oa2vBt4sKreqKoXgOPAjiSbgEuq6vGqKuCBoW0kSVMw0Tn9JBuSPA2cAh6tqieAK6rqJEC7v7wN3wy8PLT5iVbb3JaX1kftb1+SxSSLp0+fXsXLkSStZKLQr6q3quoDwDyDo/arVxg+6jx9rVAftb/7qmp7VW2fm5ubpEVJ0gRW9emdqvol8BiDc/GvtlM2tPtTbdgJYMvQZvPAK60+P6IuSZqSST69M5fkfW35YuDDwM+Bw8DeNmwv8FBbPgzsSXJRkisZXLB9sp0Cej3JzvapnVuHtpEkTcEFE4zZBBxsn8B5F3Coqh5O8jhwKMltwEvALQBVdTTJIeBZ4E3gjqp6qz3X7cD9wMXAI+0mSZqSsaFfVT8FPjii/hpwwzLbHAAOjKgvAitdD5AkrSO/kStJHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHVkbOgn2ZLk+0mOJTma5DOtflmSR5M83+4vHdrmziTHkzyX5Mah+rVJnmnr7k6S9XlZkqRRJjnSfxP466r6I2AncEeSbcB+4EhVbQWOtMe0dXuAq4BdwD1JNrTnuhfYB2xtt11r+FokSWOMDf2qOllVP2rLrwPHgM3AbuBgG3YQuLkt7wYerKo3quoF4DiwI8km4JKqeryqCnhgaBtJ0hSs6px+kgXgg8ATwBVVdRIGfzEAl7dhm4GXhzY70Wqb2/LS+qj97EuymGTx9OnTq2lRkrSCiUM/yXuBbwGfrapfrzR0RK1WqJ9ZrLqvqrZX1fa5ublJW5QkjTFR6Ce5kEHgf6Oqvt3Kr7ZTNrT7U61+AtgytPk88Eqrz4+oS5KmZJJP7wT4KnCsqr40tOowsLct7wUeGqrvSXJRkisZXLB9sp0Cej3Jzvactw5tI0maggsmGHMd8CngmSRPt9rngbuAQ0luA14CbgGoqqNJDgHPMvjkzx1V9Vbb7nbgfuBi4JF2kyRNydjQr6ofMvp8PMANy2xzADgwor4IXL2aBiVJa8dv5EpSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0ZG/pJvpbkVJKfDdUuS/Jokufb/aVD6+5McjzJc0luHKpfm+SZtu7uJFn7lyNJWskkR/r3A7uW1PYDR6pqK3CkPSbJNmAPcFXb5p4kG9o29wL7gK3ttvQ5JUnr7IJxA6rqB0kWlpR3A9e35YPAY8DftPqDVfUG8EKS48COJC8Cl1TV4wBJHgBuBh55269gBQv7v7OeT7+sF++6aSb7laRxzvac/hVVdRKg3V/e6puBl4fGnWi1zW15aV2SNEVrfSF31Hn6WqE++kmSfUkWkyyePn16zZqTpN6dbei/mmQTQLs/1eongC1D4+aBV1p9fkR9pKq6r6q2V9X2ubm5s2xRkrTU2Yb+YWBvW94LPDRU35PkoiRXMrhg+2Q7BfR6kp3tUzu3Dm0jSZqSsRdyk3yTwUXbjUlOAF8A7gIOJbkNeAm4BaCqjiY5BDwLvAncUVVvtae6ncEngS5mcAF3XS/iSpLONMmndz6xzKoblhl/ADgwor4IXL2q7iRJa8pv5EpSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI1MP/SS7kjyX5HiS/dPevyT1bKqhn2QD8PfAnwHbgE8k2TbNHiSpZ9M+0t8BHK+q/6iq/wUeBHZPuQdJ6laqano7Sz4O7Kqqv2iPPwX8cVV9esm4fcC+9vAPgefOcpcbgV+c5bbryb5Wx75Wx75W53zt6/eram5p8YK38YRnIyNqZ/ytU1X3Afe97Z0li1W1/e0+z1qzr9Wxr9Wxr9Xpra9pn945AWwZejwPvDLlHiSpW9MO/X8Ftia5Msm7gT3A4Sn3IEndmurpnap6M8mngX8GNgBfq6qj67jLt32KaJ3Y1+rY1+rY1+p01ddUL+RKkmbLb+RKUkcMfUnqyHkR+uN+2iEDd7f1P01yzTnS1/VJfpXk6Xb72yn09LUkp5L8bJn1s5qrcX1Nfa7afrck+X6SY0mOJvnMiDFTn7MJ+5rF++s9SZ5M8pPW1xdHjJnFfE3S10zeY23fG5L8OMnDI9at7XxV1Tv6xuCC8L8DfwC8G/gJsG3JmI8AjzD4nsBO4IlzpK/rgYenPF9/ClwD/GyZ9VOfqwn7mvpctf1uAq5py78D/Ns58v6apK9ZvL8CvLctXwg8Aew8B+Zrkr5m8h5r+/4r4B9H7X+t5+t8ONKf5KcddgMP1MC/AO9Lsukc6GvqquoHwH+tMGQWczVJXzNRVSer6kdt+XXgGLB5ybCpz9mEfU1dm4PftIcXttvST4vMYr4m6WsmkswDNwFfWWbIms7X+RD6m4GXhx6f4Mw3/yRjZtEXwJ+0f3I+kuSqde5pErOYq0nNdK6SLAAfZHCUOGymc7ZCXzCDOWunKp4GTgGPVtU5MV8T9AWzeY99Gfgc8Ntl1q/pfJ0PoT/JTztM9PMPa2ySff6Iwe9jvB/4O+Cf1rmnScxiriYx07lK8l7gW8Bnq+rXS1eP2GQqczamr5nMWVW9VVUfYPCN+x1Jrl4yZCbzNUFfU5+vJB8FTlXVUysNG1E76/k6H0J/kp92mMXPP4zdZ1X9+v//yVlV3wUuTLJxnfsa55z8qYxZzlWSCxkE6zeq6tsjhsxkzsb1Nev3V1X9EngM2LVk1UzfY8v1NaP5ug74WJIXGZwC/lCSry8Zs6bzdT6E/iQ/7XAYuLVdBd8J/KqqTs66ryS/myRteQeD/x6vrXNf48xirsaa1Vy1fX4VOFZVX1pm2NTnbJK+ZjFnSeaSvK8tXwx8GPj5kmGzmK+xfc1ivqrqzqqar6oFBhnxvar65JJhazpf0/6VzTVXy/y0Q5K/bOv/Afgugyvgx4H/Bv78HOnr48DtSd4E/gfYU+1y/XpJ8k0Gn1LYmOQE8AUGF7VmNlcT9jX1uWquAz4FPNPOBwN8Hvi9od5mMWeT9DWLOdsEHMzgf5j0LuBQVT086z+PE/Y1q/fYGdZzvvwZBknqyPlwekeSNCFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXk/wAFp4Zt8hthegAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "attributes = ['degree', 'ring', 'NetRank', 'NetShort', 'HeatDiff', 'InfoDiff']\n",
    "\n",
    "dataset_with_nedbit = MyDataset(G, labels, attributes, num_classes=5)\n",
    "data_with_nedbit = dataset_with_nedbit[0]\n",
    "\n",
    "plt.hist(labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_sage = GNN4L_Sage(data_with_nedbit).to(device)\n",
    "pred = train(gnn_sage, data_with_nedbit.to(device), 1000, cm_title='SAGE4L_multiclass_16HC_LSTM', classes=['P', 'LP', 'WN', 'LN', 'RN'], weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\pmaur\\miniconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       154\n",
      "           1       0.54      0.75      0.63       703\n",
      "           2       0.37      0.27      0.31       702\n",
      "           3       0.48      0.50      0.49       703\n",
      "           4       0.67      0.71      0.69       702\n",
      "\n",
      "    accuracy                           0.53      2964\n",
      "   macro avg       0.41      0.45      0.42      2964\n",
      "weighted avg       0.49      0.53      0.50      2964\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 40000\n",
    "cm_title='SAGE4L_multiclass_16HC'\n",
    "classes=['P', 'LP', 'WN', 'LN', 'RN']\n",
    "weight_decay=0.0005\n",
    "model = gnn_sage\n",
    "data = data_with_nedbit\n",
    "lr = 0.001\n",
    "labels    = data_with_nedbit.y\n",
    "\n",
    "title = cm_title + '_' + str(epochs) + '_' + str(weight_decay).replace('.', '_')\n",
    "\n",
    "model_path  = 'Models/' + title\n",
    "image_path  = 'Images/' + title\n",
    "report_path = 'Reports/' + title + '.csv'\n",
    "\n",
    "train_mask  = data['train_mask']\n",
    "test_mask   = data['test_mask']\n",
    "val_mask    = data['val_mask']\n",
    "\n",
    "# Load best model\n",
    "loaded_model = GNN4L_Sage(data).to(device)\n",
    "loaded_model.load_state_dict(torch.load(model_path))\n",
    "loaded_model.eval()\n",
    "logits = loaded_model(data)\n",
    "output = logits.argmax(1)\n",
    "\n",
    "print(classification_report(labels[test_mask].to('cpu'), output[test_mask].to('cpu')))\n",
    "\n",
    "class_report = classification_report(labels[test_mask].to('cpu'), output[test_mask].to('cpu'), output_dict=True)\n",
    "classification_report_dataframe = pd.DataFrame(class_report)\n",
    "classification_report_dataframe.to_csv(report_path)\n",
    "\n",
    "#Confusion Matrix\n",
    "norms = [None, \"true\"]\n",
    "for norm in norms:\n",
    "    cm = confusion_matrix(labels[test_mask].to('cpu'), output[test_mask].to('cpu'), normalize=norm)\n",
    "\n",
    "    plt.figure(figsize=(7,7))\n",
    "    \n",
    "    if norm == \"true\":\n",
    "        sn.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'BuPu', xticklabels = classes, yticklabels = classes)\n",
    "    else:\n",
    "        sn.heatmap(cm, annot=True, fmt=\".0f\", linewidths=.5, square = True, cmap = 'BuPu', xticklabels = classes, yticklabels = classes)\n",
    "    plt.title(cm_title)\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    if norm == None:\n",
    "        plt.savefig(image_path + '_notNorm.png')\n",
    "    else:\n",
    "        plt.savefig(image_path + '_Norm.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d766f45d334c428523a3e28d8344c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0000] train loss: 166.2329, train acc: 0.1386, val loss: 88.0574, val acc: 0.2381  (best train acc: 0.1386, best val acc: 0.2381, best train loss: 166.2329  @ epoch 0 )\n",
      "[Epoch: 0020] train loss: 9.6504, train acc: 0.2872, val loss: 4.2943, val acc: 0.2809  (best train acc: 0.2894, best val acc: 0.3133, best train loss: 9.6504  @ epoch 20 )\n",
      "[Epoch: 0040] train loss: 3.3633, train acc: 0.3047, val loss: 1.6617, val acc: 0.4098  (best train acc: 0.3623, best val acc: 0.4479, best train loss: 3.1851  @ epoch 39 )\n",
      "[Epoch: 0060] train loss: 1.9706, train acc: 0.3540, val loss: 1.4587, val acc: 0.3993  (best train acc: 0.3733, best val acc: 0.4479, best train loss: 1.9706  @ epoch 60 )\n",
      "[Epoch: 0080] train loss: 1.6541, train acc: 0.3965, val loss: 1.3354, val acc: 0.4344  (best train acc: 0.4111, best val acc: 0.4479, best train loss: 1.6061  @ epoch 74 )\n",
      "[Epoch: 0100] train loss: 1.4146, train acc: 0.4208, val loss: 1.3137, val acc: 0.4145  (best train acc: 0.4263, best val acc: 0.4479, best train loss: 1.3995  @ epoch 96 )\n",
      "[Epoch: 0120] train loss: 1.3611, train acc: 0.4249, val loss: 1.2975, val acc: 0.4128  (best train acc: 0.4485, best val acc: 0.4479, best train loss: 1.2878  @ epoch 117 )\n",
      "[Epoch: 0140] train loss: 1.3281, train acc: 0.4239, val loss: 1.2458, val acc: 0.4506  (best train acc: 0.4918, best val acc: 0.4870, best train loss: 1.1938  @ epoch 138 )\n",
      "[Epoch: 0160] train loss: 1.2569, train acc: 0.4744, val loss: 1.1854, val acc: 0.4961  (best train acc: 0.5136, best val acc: 0.5099, best train loss: 1.1762  @ epoch 155 )\n",
      "[Epoch: 0180] train loss: 1.2091, train acc: 0.4881, val loss: 1.1634, val acc: 0.5322  (best train acc: 0.5338, best val acc: 0.5518, best train loss: 1.1231  @ epoch 177 )\n",
      "[Epoch: 0200] train loss: 1.1114, train acc: 0.5535, val loss: 1.1080, val acc: 0.5703  (best train acc: 0.5629, best val acc: 0.5929, best train loss: 1.0881  @ epoch 198 )\n",
      "[Epoch: 0220] train loss: 1.1586, train acc: 0.5351, val loss: 1.0753, val acc: 0.5906  (best train acc: 0.5876, best val acc: 0.5980, best train loss: 1.0481  @ epoch 219 )\n",
      "[Epoch: 0240] train loss: 1.1188, train acc: 0.5463, val loss: 1.0808, val acc: 0.5696  (best train acc: 0.6089, best val acc: 0.6192, best train loss: 1.0047  @ epoch 236 )\n",
      "[Epoch: 0260] train loss: 1.1477, train acc: 0.5286, val loss: 1.0447, val acc: 0.6010  (best train acc: 0.6089, best val acc: 0.6290, best train loss: 1.0047  @ epoch 236 )\n",
      "[Epoch: 0280] train loss: 0.9854, train acc: 0.6181, val loss: 1.0211, val acc: 0.6364  (best train acc: 0.6181, best val acc: 0.6364, best train loss: 0.9854  @ epoch 280 )\n",
      "[Epoch: 0300] train loss: 1.0211, train acc: 0.5894, val loss: 1.0059, val acc: 0.6627  (best train acc: 0.6181, best val acc: 0.6627, best train loss: 0.9854  @ epoch 280 )\n",
      "[Epoch: 0320] train loss: 1.0130, train acc: 0.5934, val loss: 1.0045, val acc: 0.6381  (best train acc: 0.6181, best val acc: 0.6627, best train loss: 0.9854  @ epoch 280 )\n",
      "[Epoch: 0340] train loss: 1.0060, train acc: 0.5947, val loss: 0.9852, val acc: 0.6202  (best train acc: 0.6333, best val acc: 0.6627, best train loss: 0.9604  @ epoch 333 )\n",
      "[Epoch: 0360] train loss: 1.0370, train acc: 0.5897, val loss: 0.9603, val acc: 0.6364  (best train acc: 0.6333, best val acc: 0.6678, best train loss: 0.9391  @ epoch 351 )\n",
      "[Epoch: 0380] train loss: 0.9438, train acc: 0.6251, val loss: 0.9084, val acc: 0.6678  (best train acc: 0.6579, best val acc: 0.6759, best train loss: 0.9068  @ epoch 375 )\n",
      "[Epoch: 0400] train loss: 0.9765, train acc: 0.6210, val loss: 0.8962, val acc: 0.6776  (best train acc: 0.6579, best val acc: 0.6901, best train loss: 0.9026  @ epoch 383 )\n",
      "[Epoch: 0420] train loss: 0.8768, train acc: 0.6643, val loss: 0.8929, val acc: 0.6732  (best train acc: 0.6643, best val acc: 0.7099, best train loss: 0.8768  @ epoch 420 )\n",
      "[Epoch: 0440] train loss: 0.9478, train acc: 0.6450, val loss: 0.8543, val acc: 0.6890  (best train acc: 0.6840, best val acc: 0.7241, best train loss: 0.8638  @ epoch 439 )\n",
      "[Epoch: 0460] train loss: 0.9077, train acc: 0.6551, val loss: 0.8039, val acc: 0.7255  (best train acc: 0.6840, best val acc: 0.7272, best train loss: 0.8638  @ epoch 439 )\n",
      "[Epoch: 0480] train loss: 0.8854, train acc: 0.6661, val loss: 0.8162, val acc: 0.7332  (best train acc: 0.6840, best val acc: 0.7332, best train loss: 0.8384  @ epoch 469 )\n",
      "[Epoch: 0500] train loss: 1.1414, train acc: 0.5650, val loss: 0.7789, val acc: 0.7390  (best train acc: 0.7107, best val acc: 0.7460, best train loss: 0.8007  @ epoch 499 )\n",
      "[Epoch: 0520] train loss: 0.8339, train acc: 0.6760, val loss: 0.7456, val acc: 0.7639  (best train acc: 0.7107, best val acc: 0.7639, best train loss: 0.7865  @ epoch 515 )\n",
      "[Epoch: 0540] train loss: 0.8375, train acc: 0.6852, val loss: 0.7420, val acc: 0.7470  (best train acc: 0.7299, best val acc: 0.7639, best train loss: 0.7455  @ epoch 536 )\n",
      "[Epoch: 0560] train loss: 0.8266, train acc: 0.7019, val loss: 0.7037, val acc: 0.7595  (best train acc: 0.7299, best val acc: 0.7761, best train loss: 0.7455  @ epoch 536 )\n",
      "[Epoch: 0580] train loss: 0.7600, train acc: 0.7142, val loss: 0.6927, val acc: 0.7673  (best train acc: 0.7358, best val acc: 0.7848, best train loss: 0.7353  @ epoch 577 )\n",
      "[Epoch: 0600] train loss: 0.8719, train acc: 0.6861, val loss: 0.7196, val acc: 0.7218  (best train acc: 0.7405, best val acc: 0.7895, best train loss: 0.7035  @ epoch 583 )\n",
      "[Epoch: 0620] train loss: 0.7427, train acc: 0.7282, val loss: 0.6410, val acc: 0.7835  (best train acc: 0.7411, best val acc: 0.8020, best train loss: 0.6993  @ epoch 618 )\n",
      "[Epoch: 0640] train loss: 0.7587, train acc: 0.7285, val loss: 0.6098, val acc: 0.8071  (best train acc: 0.7561, best val acc: 0.8182, best train loss: 0.6596  @ epoch 635 )\n",
      "[Epoch: 0660] train loss: 0.6333, train acc: 0.7666, val loss: 0.5712, val acc: 0.8233  (best train acc: 0.7712, best val acc: 0.8233, best train loss: 0.6278  @ epoch 659 )\n",
      "[Epoch: 0680] train loss: 0.7306, train acc: 0.7410, val loss: 0.5877, val acc: 0.8027  (best train acc: 0.7809, best val acc: 0.8236, best train loss: 0.5935  @ epoch 678 )\n",
      "[Epoch: 0700] train loss: 0.6127, train acc: 0.7779, val loss: 0.5605, val acc: 0.8172  (best train acc: 0.7914, best val acc: 0.8304, best train loss: 0.5935  @ epoch 678 )\n",
      "[Epoch: 0720] train loss: 0.6058, train acc: 0.7692, val loss: 0.5235, val acc: 0.8358  (best train acc: 0.7914, best val acc: 0.8358, best train loss: 0.5935  @ epoch 678 )\n",
      "[Epoch: 0740] train loss: 0.6420, train acc: 0.7450, val loss: 0.5559, val acc: 0.8145  (best train acc: 0.7914, best val acc: 0.8374, best train loss: 0.5727  @ epoch 725 )\n",
      "[Epoch: 0760] train loss: 0.5631, train acc: 0.7961, val loss: 0.5168, val acc: 0.8250  (best train acc: 0.7995, best val acc: 0.8374, best train loss: 0.5555  @ epoch 752 )\n",
      "[Epoch: 0780] train loss: 0.5743, train acc: 0.7802, val loss: 0.5569, val acc: 0.7848  (best train acc: 0.8038, best val acc: 0.8442, best train loss: 0.5532  @ epoch 771 )\n",
      "[Epoch: 0800] train loss: 0.6044, train acc: 0.7742, val loss: 0.5315, val acc: 0.8145  (best train acc: 0.8038, best val acc: 0.8442, best train loss: 0.5487  @ epoch 794 )\n",
      "[Epoch: 0820] train loss: 0.6098, train acc: 0.7615, val loss: 0.5344, val acc: 0.8395  (best train acc: 0.8038, best val acc: 0.8462, best train loss: 0.5487  @ epoch 794 )\n",
      "[Epoch: 0840] train loss: 0.5521, train acc: 0.7927, val loss: 0.5082, val acc: 0.8422  (best train acc: 0.8038, best val acc: 0.8489, best train loss: 0.5475  @ epoch 832 )\n",
      "[Epoch: 0860] train loss: 0.5768, train acc: 0.7674, val loss: 0.4868, val acc: 0.8513  (best train acc: 0.8107, best val acc: 0.8513, best train loss: 0.5331  @ epoch 842 )\n",
      "[Epoch: 0880] train loss: 0.5407, train acc: 0.7978, val loss: 0.5045, val acc: 0.8314  (best train acc: 0.8107, best val acc: 0.8519, best train loss: 0.5167  @ epoch 875 )\n",
      "[Epoch: 0900] train loss: 0.5842, train acc: 0.7688, val loss: 0.4750, val acc: 0.8492  (best train acc: 0.8146, best val acc: 0.8540, best train loss: 0.5102  @ epoch 882 )\n",
      "[Epoch: 0920] train loss: 0.5532, train acc: 0.8052, val loss: 0.4524, val acc: 0.8567  (best train acc: 0.8146, best val acc: 0.8644, best train loss: 0.5083  @ epoch 913 )\n",
      "[Epoch: 0940] train loss: 0.5058, train acc: 0.8167, val loss: 0.4575, val acc: 0.8607  (best train acc: 0.8253, best val acc: 0.8644, best train loss: 0.4899  @ epoch 926 )\n",
      "[Epoch: 0960] train loss: 0.5349, train acc: 0.8010, val loss: 0.5043, val acc: 0.8155  (best train acc: 0.8253, best val acc: 0.8644, best train loss: 0.4871  @ epoch 946 )\n",
      "[Epoch: 0980] train loss: 0.5949, train acc: 0.7704, val loss: 0.4836, val acc: 0.8472  (best train acc: 0.8253, best val acc: 0.8651, best train loss: 0.4871  @ epoch 946 )\n",
      "[Epoch: 1000] train loss: 0.4960, train acc: 0.8258, val loss: 0.4428, val acc: 0.8702  (best train acc: 0.8340, best val acc: 0.8702, best train loss: 0.4704  @ epoch 997 )\n",
      "[Epoch: 1020] train loss: 0.5011, train acc: 0.8118, val loss: 0.4916, val acc: 0.8250  (best train acc: 0.8340, best val acc: 0.8702, best train loss: 0.4704  @ epoch 997 )\n",
      "[Epoch: 1040] train loss: 0.5448, train acc: 0.7862, val loss: 0.4407, val acc: 0.8600  (best train acc: 0.8340, best val acc: 0.8742, best train loss: 0.4704  @ epoch 997 )\n",
      "[Epoch: 1060] train loss: 0.5330, train acc: 0.8018, val loss: 0.4716, val acc: 0.8384  (best train acc: 0.8340, best val acc: 0.8742, best train loss: 0.4704  @ epoch 997 )\n",
      "[Epoch: 1080] train loss: 0.5259, train acc: 0.8156, val loss: 0.4550, val acc: 0.8445  (best train acc: 0.8340, best val acc: 0.8742, best train loss: 0.4704  @ epoch 997 )\n",
      "[Epoch: 1100] train loss: 0.5661, train acc: 0.7898, val loss: 0.4512, val acc: 0.8503  (best train acc: 0.8340, best val acc: 0.8766, best train loss: 0.4704  @ epoch 997 )\n",
      "[Epoch: 1120] train loss: 0.5410, train acc: 0.8033, val loss: 0.4407, val acc: 0.8661  (best train acc: 0.8340, best val acc: 0.8766, best train loss: 0.4676  @ epoch 1117 )\n",
      "[Epoch: 1140] train loss: 0.4801, train acc: 0.8262, val loss: 0.4341, val acc: 0.8604  (best train acc: 0.8340, best val acc: 0.8766, best train loss: 0.4676  @ epoch 1117 )\n",
      "[Epoch: 1160] train loss: 0.4771, train acc: 0.8168, val loss: 0.4358, val acc: 0.8597  (best train acc: 0.8443, best val acc: 0.8766, best train loss: 0.4404  @ epoch 1147 )\n",
      "[Epoch: 1180] train loss: 0.5134, train acc: 0.8066, val loss: 0.4358, val acc: 0.8631  (best train acc: 0.8443, best val acc: 0.8766, best train loss: 0.4404  @ epoch 1147 )\n",
      "[Epoch: 1200] train loss: 0.4902, train acc: 0.8137, val loss: 0.4204, val acc: 0.8668  (best train acc: 0.8443, best val acc: 0.8803, best train loss: 0.4404  @ epoch 1147 )\n",
      "[Epoch: 1220] train loss: 0.4652, train acc: 0.8341, val loss: 0.4137, val acc: 0.8762  (best train acc: 0.8443, best val acc: 0.8803, best train loss: 0.4400  @ epoch 1215 )\n",
      "[Epoch: 1240] train loss: 0.4635, train acc: 0.8284, val loss: 0.4035, val acc: 0.8752  (best train acc: 0.8443, best val acc: 0.8803, best train loss: 0.4400  @ epoch 1215 )\n",
      "[Epoch: 1260] train loss: 0.4656, train acc: 0.8315, val loss: 0.4103, val acc: 0.8587  (best train acc: 0.8443, best val acc: 0.8803, best train loss: 0.4400  @ epoch 1215 )\n",
      "[Epoch: 1280] train loss: 0.4708, train acc: 0.8237, val loss: 0.4090, val acc: 0.8695  (best train acc: 0.8443, best val acc: 0.8803, best train loss: 0.4311  @ epoch 1279 )\n",
      "[Epoch: 1300] train loss: 0.4794, train acc: 0.8197, val loss: 0.4231, val acc: 0.8600  (best train acc: 0.8443, best val acc: 0.8803, best train loss: 0.4311  @ epoch 1279 )\n",
      "[Epoch: 1320] train loss: 0.4873, train acc: 0.8111, val loss: 0.4343, val acc: 0.8492  (best train acc: 0.8443, best val acc: 0.8809, best train loss: 0.4311  @ epoch 1279 )\n",
      "[Epoch: 1340] train loss: 0.4544, train acc: 0.8332, val loss: 0.4002, val acc: 0.8661  (best train acc: 0.8477, best val acc: 0.8809, best train loss: 0.4311  @ epoch 1279 )\n",
      "[Epoch: 1360] train loss: 0.4959, train acc: 0.8107, val loss: 0.4041, val acc: 0.8755  (best train acc: 0.8477, best val acc: 0.8820, best train loss: 0.4311  @ epoch 1279 )\n",
      "[Epoch: 1380] train loss: 0.4528, train acc: 0.8246, val loss: 0.3999, val acc: 0.8664  (best train acc: 0.8477, best val acc: 0.8836, best train loss: 0.4311  @ epoch 1279 )\n",
      "[Epoch: 1400] train loss: 0.4697, train acc: 0.8269, val loss: 0.4331, val acc: 0.8472  (best train acc: 0.8477, best val acc: 0.8836, best train loss: 0.4288  @ epoch 1394 )\n",
      "[Epoch: 1420] train loss: 0.4252, train acc: 0.8469, val loss: 0.3975, val acc: 0.8705  (best train acc: 0.8477, best val acc: 0.8836, best train loss: 0.4252  @ epoch 1420 )\n",
      "[Epoch: 1440] train loss: 0.4443, train acc: 0.8327, val loss: 0.3962, val acc: 0.8691  (best train acc: 0.8485, best val acc: 0.8840, best train loss: 0.4193  @ epoch 1438 )\n",
      "[Epoch: 1460] train loss: 0.4433, train acc: 0.8373, val loss: 0.3816, val acc: 0.8840  (best train acc: 0.8485, best val acc: 0.8843, best train loss: 0.4193  @ epoch 1438 )\n",
      "[Epoch: 1480] train loss: 0.4459, train acc: 0.8324, val loss: 0.4185, val acc: 0.8523  (best train acc: 0.8485, best val acc: 0.8843, best train loss: 0.4193  @ epoch 1438 )\n",
      "[Epoch: 1500] train loss: 0.4534, train acc: 0.8237, val loss: 0.4215, val acc: 0.8503  (best train acc: 0.8485, best val acc: 0.8880, best train loss: 0.4193  @ epoch 1438 )\n",
      "[Epoch: 1520] train loss: 0.4238, train acc: 0.8413, val loss: 0.3681, val acc: 0.8789  (best train acc: 0.8514, best val acc: 0.8880, best train loss: 0.4143  @ epoch 1519 )\n",
      "[Epoch: 1540] train loss: 0.4672, train acc: 0.8248, val loss: 0.3662, val acc: 0.8863  (best train acc: 0.8514, best val acc: 0.8880, best train loss: 0.4143  @ epoch 1519 )\n",
      "[Epoch: 1560] train loss: 0.4529, train acc: 0.8295, val loss: 0.3867, val acc: 0.8708  (best train acc: 0.8514, best val acc: 0.8880, best train loss: 0.4143  @ epoch 1519 )\n",
      "[Epoch: 1580] train loss: 0.4486, train acc: 0.8252, val loss: 0.3700, val acc: 0.8806  (best train acc: 0.8514, best val acc: 0.8911, best train loss: 0.4143  @ epoch 1519 )\n",
      "[Epoch: 1600] train loss: 0.4777, train acc: 0.8117, val loss: 0.3790, val acc: 0.8732  (best train acc: 0.8530, best val acc: 0.8911, best train loss: 0.4070  @ epoch 1598 )\n",
      "[Epoch: 1620] train loss: 0.4556, train acc: 0.8216, val loss: 0.3805, val acc: 0.8732  (best train acc: 0.8534, best val acc: 0.8911, best train loss: 0.4053  @ epoch 1618 )\n",
      "[Epoch: 1640] train loss: 0.4383, train acc: 0.8287, val loss: 0.3866, val acc: 0.8715  (best train acc: 0.8534, best val acc: 0.8911, best train loss: 0.4053  @ epoch 1618 )\n",
      "[Epoch: 1660] train loss: 0.4897, train acc: 0.8212, val loss: 0.3707, val acc: 0.8843  (best train acc: 0.8534, best val acc: 0.8911, best train loss: 0.4053  @ epoch 1618 )\n",
      "[Epoch: 1680] train loss: 0.4146, train acc: 0.8539, val loss: 0.3789, val acc: 0.8806  (best train acc: 0.8539, best val acc: 0.8911, best train loss: 0.4053  @ epoch 1618 )\n",
      "[Epoch: 1700] train loss: 0.4099, train acc: 0.8496, val loss: 0.3709, val acc: 0.8809  (best train acc: 0.8539, best val acc: 0.8911, best train loss: 0.4053  @ epoch 1618 )\n",
      "[Epoch: 1720] train loss: 0.4183, train acc: 0.8475, val loss: 0.3652, val acc: 0.8850  (best train acc: 0.8539, best val acc: 0.8911, best train loss: 0.4053  @ epoch 1618 )\n",
      "[Epoch: 1740] train loss: 0.4151, train acc: 0.8397, val loss: 0.3665, val acc: 0.8725  (best train acc: 0.8539, best val acc: 0.8911, best train loss: 0.4053  @ epoch 1618 )\n",
      "[Epoch: 1760] train loss: 0.4249, train acc: 0.8382, val loss: 0.3753, val acc: 0.8671  (best train acc: 0.8539, best val acc: 0.8911, best train loss: 0.4004  @ epoch 1751 )\n",
      "[Epoch: 1780] train loss: 0.4092, train acc: 0.8529, val loss: 0.3551, val acc: 0.8833  (best train acc: 0.8539, best val acc: 0.8911, best train loss: 0.4004  @ epoch 1751 )\n",
      "[Epoch: 1800] train loss: 0.4067, train acc: 0.8498, val loss: 0.3674, val acc: 0.8833  (best train acc: 0.8539, best val acc: 0.8971, best train loss: 0.3953  @ epoch 1781 )\n",
      "[Epoch: 1820] train loss: 0.4138, train acc: 0.8454, val loss: 0.3585, val acc: 0.8857  (best train acc: 0.8539, best val acc: 0.8971, best train loss: 0.3953  @ epoch 1781 )\n",
      "[Epoch: 1840] train loss: 0.4193, train acc: 0.8413, val loss: 0.3579, val acc: 0.8843  (best train acc: 0.8592, best val acc: 0.8971, best train loss: 0.3953  @ epoch 1781 )\n",
      "[Epoch: 1860] train loss: 0.4147, train acc: 0.8466, val loss: 0.3562, val acc: 0.8840  (best train acc: 0.8592, best val acc: 0.8971, best train loss: 0.3913  @ epoch 1841 )\n",
      "[Epoch: 1880] train loss: 0.4397, train acc: 0.8438, val loss: 0.3635, val acc: 0.8857  (best train acc: 0.8592, best val acc: 0.8971, best train loss: 0.3913  @ epoch 1841 )\n",
      "[Epoch: 1900] train loss: 0.4147, train acc: 0.8426, val loss: 0.3778, val acc: 0.8715  (best train acc: 0.8592, best val acc: 0.8971, best train loss: 0.3913  @ epoch 1841 )\n",
      "[Epoch: 1920] train loss: 0.3963, train acc: 0.8527, val loss: 0.3521, val acc: 0.8850  (best train acc: 0.8592, best val acc: 0.8971, best train loss: 0.3782  @ epoch 1918 )\n",
      "[Epoch: 1940] train loss: 0.4008, train acc: 0.8516, val loss: 0.3401, val acc: 0.8948  (best train acc: 0.8592, best val acc: 0.8971, best train loss: 0.3782  @ epoch 1918 )\n",
      "[Epoch: 1960] train loss: 0.4025, train acc: 0.8456, val loss: 0.3531, val acc: 0.8840  (best train acc: 0.8592, best val acc: 0.8971, best train loss: 0.3782  @ epoch 1918 )\n",
      "[Epoch: 1980] train loss: 0.4181, train acc: 0.8448, val loss: 0.3710, val acc: 0.8739  (best train acc: 0.8613, best val acc: 0.8971, best train loss: 0.3782  @ epoch 1918 )\n",
      "[Epoch: 2000] train loss: 0.4469, train acc: 0.8223, val loss: 0.3667, val acc: 0.8779  (best train acc: 0.8613, best val acc: 0.8978, best train loss: 0.3782  @ epoch 1918 )\n",
      "[Epoch: 2020] train loss: 0.4135, train acc: 0.8425, val loss: 0.3577, val acc: 0.8772  (best train acc: 0.8613, best val acc: 0.8978, best train loss: 0.3782  @ epoch 1918 )\n",
      "[Epoch: 2040] train loss: 0.4234, train acc: 0.8407, val loss: 0.3450, val acc: 0.8894  (best train acc: 0.8613, best val acc: 0.8978, best train loss: 0.3782  @ epoch 1918 )\n",
      "[Epoch: 2060] train loss: 0.3821, train acc: 0.8535, val loss: 0.3879, val acc: 0.8600  (best train acc: 0.8613, best val acc: 0.8978, best train loss: 0.3782  @ epoch 1918 )\n",
      "[Epoch: 2080] train loss: 0.4232, train acc: 0.8318, val loss: 0.3578, val acc: 0.8793  (best train acc: 0.8613, best val acc: 0.8978, best train loss: 0.3782  @ epoch 1918 )\n",
      "[Epoch: 2100] train loss: 0.3907, train acc: 0.8480, val loss: 0.3472, val acc: 0.8880  (best train acc: 0.8613, best val acc: 0.8978, best train loss: 0.3782  @ epoch 1918 )\n",
      "[Epoch: 2120] train loss: 0.4092, train acc: 0.8492, val loss: 0.3629, val acc: 0.8786  (best train acc: 0.8613, best val acc: 0.8978, best train loss: 0.3782  @ epoch 1918 )\n",
      "[Epoch: 2140] train loss: 0.3945, train acc: 0.8456, val loss: 0.3322, val acc: 0.8938  (best train acc: 0.8629, best val acc: 0.8978, best train loss: 0.3757  @ epoch 2135 )\n",
      "[Epoch: 2160] train loss: 0.4268, train acc: 0.8378, val loss: 0.3840, val acc: 0.8691  (best train acc: 0.8629, best val acc: 0.8978, best train loss: 0.3757  @ epoch 2135 )\n",
      "[Epoch: 2180] train loss: 0.4043, train acc: 0.8433, val loss: 0.3649, val acc: 0.8728  (best train acc: 0.8629, best val acc: 0.8978, best train loss: 0.3757  @ epoch 2135 )\n",
      "[Epoch: 2200] train loss: 0.3832, train acc: 0.8583, val loss: 0.3386, val acc: 0.8890  (best train acc: 0.8629, best val acc: 0.8978, best train loss: 0.3757  @ epoch 2135 )\n",
      "[Epoch: 2220] train loss: 0.3679, train acc: 0.8635, val loss: 0.3372, val acc: 0.8951  (best train acc: 0.8635, best val acc: 0.8978, best train loss: 0.3679  @ epoch 2220 )\n",
      "[Epoch: 2240] train loss: 0.4267, train acc: 0.8516, val loss: 0.3465, val acc: 0.8904  (best train acc: 0.8635, best val acc: 0.8978, best train loss: 0.3679  @ epoch 2220 )\n",
      "[Epoch: 2260] train loss: 0.3878, train acc: 0.8540, val loss: 0.3448, val acc: 0.8938  (best train acc: 0.8635, best val acc: 0.8978, best train loss: 0.3627  @ epoch 2243 )\n",
      "[Epoch: 2280] train loss: 0.4303, train acc: 0.8420, val loss: 0.3694, val acc: 0.8806  (best train acc: 0.8635, best val acc: 0.8981, best train loss: 0.3627  @ epoch 2243 )\n",
      "[Epoch: 2300] train loss: 0.4103, train acc: 0.8409, val loss: 0.3555, val acc: 0.8870  (best train acc: 0.8656, best val acc: 0.8981, best train loss: 0.3627  @ epoch 2243 )\n",
      "[Epoch: 2320] train loss: 0.4215, train acc: 0.8266, val loss: 0.3466, val acc: 0.8948  (best train acc: 0.8656, best val acc: 0.8998, best train loss: 0.3627  @ epoch 2243 )\n",
      "[Epoch: 2340] train loss: 0.4001, train acc: 0.8438, val loss: 0.3384, val acc: 0.8954  (best train acc: 0.8656, best val acc: 0.9029, best train loss: 0.3627  @ epoch 2243 )\n",
      "[Epoch: 2360] train loss: 0.3711, train acc: 0.8613, val loss: 0.3724, val acc: 0.8752  (best train acc: 0.8656, best val acc: 0.9029, best train loss: 0.3627  @ epoch 2243 )\n",
      "[Epoch: 2380] train loss: 0.4263, train acc: 0.8293, val loss: 0.3415, val acc: 0.8904  (best train acc: 0.8656, best val acc: 0.9029, best train loss: 0.3627  @ epoch 2243 )\n",
      "[Epoch: 2400] train loss: 0.3988, train acc: 0.8481, val loss: 0.3654, val acc: 0.8786  (best train acc: 0.8687, best val acc: 0.9029, best train loss: 0.3417  @ epoch 2398 )\n",
      "[Epoch: 2420] train loss: 0.4219, train acc: 0.8431, val loss: 0.3461, val acc: 0.8863  (best train acc: 0.8687, best val acc: 0.9029, best train loss: 0.3417  @ epoch 2398 )\n",
      "[Epoch: 2440] train loss: 0.3716, train acc: 0.8595, val loss: 0.3378, val acc: 0.8901  (best train acc: 0.8687, best val acc: 0.9032, best train loss: 0.3417  @ epoch 2398 )\n",
      "[Epoch: 2460] train loss: 0.3678, train acc: 0.8655, val loss: 0.3306, val acc: 0.8971  (best train acc: 0.8687, best val acc: 0.9032, best train loss: 0.3417  @ epoch 2398 )\n",
      "[Epoch: 2480] train loss: 0.3761, train acc: 0.8527, val loss: 0.3241, val acc: 0.8968  (best train acc: 0.8687, best val acc: 0.9046, best train loss: 0.3417  @ epoch 2398 )\n",
      "[Epoch: 2500] train loss: 0.3966, train acc: 0.8394, val loss: 0.3529, val acc: 0.8847  (best train acc: 0.8704, best val acc: 0.9046, best train loss: 0.3417  @ epoch 2398 )\n",
      "[Epoch: 2520] train loss: 0.4242, train acc: 0.8292, val loss: 0.3474, val acc: 0.8914  (best train acc: 0.8704, best val acc: 0.9046, best train loss: 0.3417  @ epoch 2398 )\n",
      "[Epoch: 2540] train loss: 0.3692, train acc: 0.8618, val loss: 0.3460, val acc: 0.8890  (best train acc: 0.8704, best val acc: 0.9046, best train loss: 0.3417  @ epoch 2398 )\n",
      "[Epoch: 2560] train loss: 0.3797, train acc: 0.8613, val loss: 0.3293, val acc: 0.8965  (best train acc: 0.8704, best val acc: 0.9059, best train loss: 0.3417  @ epoch 2398 )\n",
      "[Epoch: 2580] train loss: 0.3750, train acc: 0.8595, val loss: 0.3565, val acc: 0.8907  (best train acc: 0.8704, best val acc: 0.9059, best train loss: 0.3417  @ epoch 2398 )\n",
      "[Epoch: 2600] train loss: 0.3839, train acc: 0.8501, val loss: 0.3113, val acc: 0.8981  (best train acc: 0.8704, best val acc: 0.9059, best train loss: 0.3417  @ epoch 2398 )\n",
      "[Epoch: 2620] train loss: 0.3754, train acc: 0.8536, val loss: 0.3220, val acc: 0.9019  (best train acc: 0.8741, best val acc: 0.9059, best train loss: 0.3417  @ epoch 2398 )\n",
      "[Epoch: 2640] train loss: 0.3941, train acc: 0.8495, val loss: 0.3201, val acc: 0.8975  (best train acc: 0.8741, best val acc: 0.9059, best train loss: 0.3417  @ epoch 2398 )\n",
      "[Epoch: 2660] train loss: 0.3943, train acc: 0.8556, val loss: 0.3235, val acc: 0.8978  (best train acc: 0.8741, best val acc: 0.9059, best train loss: 0.3417  @ epoch 2398 )\n",
      "[Epoch: 2680] train loss: 0.3652, train acc: 0.8616, val loss: 0.3205, val acc: 0.9019  (best train acc: 0.8746, best val acc: 0.9110, best train loss: 0.3417  @ epoch 2398 )\n",
      "[Epoch: 2700] train loss: 0.3573, train acc: 0.8691, val loss: 0.3369, val acc: 0.8894  (best train acc: 0.8746, best val acc: 0.9110, best train loss: 0.3417  @ epoch 2398 )\n",
      "[Epoch: 2720] train loss: 0.3624, train acc: 0.8566, val loss: 0.3197, val acc: 0.9042  (best train acc: 0.8746, best val acc: 0.9140, best train loss: 0.3417  @ epoch 2398 )\n",
      "[Epoch: 2740] train loss: 0.3625, train acc: 0.8629, val loss: 0.3214, val acc: 0.9008  (best train acc: 0.8746, best val acc: 0.9140, best train loss: 0.3417  @ epoch 2398 )\n",
      "[Epoch: 2760] train loss: 0.3670, train acc: 0.8613, val loss: 0.3251, val acc: 0.9015  (best train acc: 0.8746, best val acc: 0.9140, best train loss: 0.3417  @ epoch 2398 )\n",
      "[Epoch: 2780] train loss: 0.3486, train acc: 0.8660, val loss: 0.3351, val acc: 0.8941  (best train acc: 0.8746, best val acc: 0.9191, best train loss: 0.3360  @ epoch 2767 )\n",
      "[Epoch: 2800] train loss: 0.3831, train acc: 0.8580, val loss: 0.3268, val acc: 0.9022  (best train acc: 0.8790, best val acc: 0.9191, best train loss: 0.3338  @ epoch 2796 )\n",
      "[Epoch: 2820] train loss: 0.3932, train acc: 0.8473, val loss: 0.3637, val acc: 0.8786  (best train acc: 0.8790, best val acc: 0.9191, best train loss: 0.3300  @ epoch 2812 )\n",
      "[Epoch: 2840] train loss: 0.3482, train acc: 0.8657, val loss: 0.3111, val acc: 0.9035  (best train acc: 0.8790, best val acc: 0.9191, best train loss: 0.3300  @ epoch 2812 )\n",
      "[Epoch: 2860] train loss: 0.3643, train acc: 0.8615, val loss: 0.2961, val acc: 0.9106  (best train acc: 0.8793, best val acc: 0.9191, best train loss: 0.3300  @ epoch 2812 )\n",
      "[Epoch: 2880] train loss: 0.3562, train acc: 0.8641, val loss: 0.3194, val acc: 0.9052  (best train acc: 0.8793, best val acc: 0.9191, best train loss: 0.3254  @ epoch 2876 )\n",
      "[Epoch: 2900] train loss: 0.3499, train acc: 0.8681, val loss: 0.3082, val acc: 0.9137  (best train acc: 0.8814, best val acc: 0.9191, best train loss: 0.3247  @ epoch 2882 )\n",
      "[Epoch: 2920] train loss: 0.3339, train acc: 0.8707, val loss: 0.2896, val acc: 0.9214  (best train acc: 0.8824, best val acc: 0.9214, best train loss: 0.3247  @ epoch 2882 )\n",
      "[Epoch: 2940] train loss: 0.3625, train acc: 0.8621, val loss: 0.3193, val acc: 0.9049  (best train acc: 0.8824, best val acc: 0.9214, best train loss: 0.3231  @ epoch 2933 )\n",
      "[Epoch: 2960] train loss: 0.3599, train acc: 0.8572, val loss: 0.2971, val acc: 0.9130  (best train acc: 0.8824, best val acc: 0.9214, best train loss: 0.3219  @ epoch 2959 )\n",
      "[Epoch: 2980] train loss: 0.3242, train acc: 0.8785, val loss: 0.3053, val acc: 0.9099  (best train acc: 0.8824, best val acc: 0.9214, best train loss: 0.3219  @ epoch 2959 )\n",
      "[Epoch: 3000] train loss: 0.3508, train acc: 0.8688, val loss: 0.3033, val acc: 0.9164  (best train acc: 0.8824, best val acc: 0.9214, best train loss: 0.3219  @ epoch 2959 )\n",
      "[Epoch: 3020] train loss: 0.3613, train acc: 0.8657, val loss: 0.3127, val acc: 0.9120  (best train acc: 0.8824, best val acc: 0.9221, best train loss: 0.3191  @ epoch 3011 )\n",
      "[Epoch: 3040] train loss: 0.3531, train acc: 0.8590, val loss: 0.3216, val acc: 0.9073  (best train acc: 0.8824, best val acc: 0.9221, best train loss: 0.3191  @ epoch 3011 )\n",
      "[Epoch: 3060] train loss: 0.3383, train acc: 0.8681, val loss: 0.2885, val acc: 0.9137  (best train acc: 0.8824, best val acc: 0.9221, best train loss: 0.3191  @ epoch 3011 )\n",
      "[Epoch: 3080] train loss: 0.3799, train acc: 0.8540, val loss: 0.3186, val acc: 0.9012  (best train acc: 0.8835, best val acc: 0.9221, best train loss: 0.3173  @ epoch 3063 )\n",
      "[Epoch: 3100] train loss: 0.3413, train acc: 0.8719, val loss: 0.2954, val acc: 0.9194  (best train acc: 0.8839, best val acc: 0.9221, best train loss: 0.3173  @ epoch 3063 )\n",
      "[Epoch: 3120] train loss: 0.3260, train acc: 0.8752, val loss: 0.2953, val acc: 0.9133  (best train acc: 0.8839, best val acc: 0.9231, best train loss: 0.3115  @ epoch 3106 )\n",
      "[Epoch: 3140] train loss: 0.3276, train acc: 0.8712, val loss: 0.3331, val acc: 0.8924  (best train acc: 0.8839, best val acc: 0.9258, best train loss: 0.3115  @ epoch 3106 )\n",
      "[Epoch: 3160] train loss: 0.3824, train acc: 0.8467, val loss: 0.2966, val acc: 0.9147  (best train acc: 0.8839, best val acc: 0.9258, best train loss: 0.3115  @ epoch 3106 )\n",
      "[Epoch: 3180] train loss: 0.3194, train acc: 0.8765, val loss: 0.2919, val acc: 0.9143  (best train acc: 0.8839, best val acc: 0.9258, best train loss: 0.3115  @ epoch 3106 )\n",
      "[Epoch: 3200] train loss: 0.3293, train acc: 0.8796, val loss: 0.3053, val acc: 0.9116  (best train acc: 0.8878, best val acc: 0.9258, best train loss: 0.3048  @ epoch 3199 )\n",
      "[Epoch: 3220] train loss: 0.3203, train acc: 0.8799, val loss: 0.2982, val acc: 0.9170  (best train acc: 0.8884, best val acc: 0.9265, best train loss: 0.3048  @ epoch 3199 )\n",
      "[Epoch: 3240] train loss: 0.3100, train acc: 0.8810, val loss: 0.2809, val acc: 0.9214  (best train acc: 0.8884, best val acc: 0.9265, best train loss: 0.3048  @ epoch 3199 )\n",
      "[Epoch: 3260] train loss: 0.3243, train acc: 0.8806, val loss: 0.2749, val acc: 0.9255  (best train acc: 0.8884, best val acc: 0.9272, best train loss: 0.3048  @ epoch 3199 )\n",
      "[Epoch: 3280] train loss: 0.3322, train acc: 0.8799, val loss: 0.2621, val acc: 0.9255  (best train acc: 0.8884, best val acc: 0.9272, best train loss: 0.3048  @ epoch 3199 )\n",
      "[Epoch: 3300] train loss: 0.3371, train acc: 0.8646, val loss: 0.2733, val acc: 0.9241  (best train acc: 0.8884, best val acc: 0.9285, best train loss: 0.3012  @ epoch 3287 )\n",
      "[Epoch: 3320] train loss: 0.3272, train acc: 0.8694, val loss: 0.3010, val acc: 0.9106  (best train acc: 0.8884, best val acc: 0.9285, best train loss: 0.3012  @ epoch 3287 )\n",
      "[Epoch: 3340] train loss: 0.3249, train acc: 0.8812, val loss: 0.2858, val acc: 0.9194  (best train acc: 0.8901, best val acc: 0.9285, best train loss: 0.3012  @ epoch 3287 )\n",
      "[Epoch: 3360] train loss: 0.3049, train acc: 0.8882, val loss: 0.2811, val acc: 0.9133  (best train acc: 0.8918, best val acc: 0.9285, best train loss: 0.2947  @ epoch 3356 )\n",
      "[Epoch: 3380] train loss: 0.3093, train acc: 0.8796, val loss: 0.2834, val acc: 0.9150  (best train acc: 0.8918, best val acc: 0.9285, best train loss: 0.2947  @ epoch 3356 )\n",
      "[Epoch: 3400] train loss: 0.3071, train acc: 0.8835, val loss: 0.2693, val acc: 0.9211  (best train acc: 0.8918, best val acc: 0.9285, best train loss: 0.2947  @ epoch 3356 )\n",
      "[Epoch: 3420] train loss: 0.3719, train acc: 0.8519, val loss: 0.2888, val acc: 0.9248  (best train acc: 0.8918, best val acc: 0.9288, best train loss: 0.2947  @ epoch 3356 )\n",
      "[Epoch: 3440] train loss: 0.3333, train acc: 0.8644, val loss: 0.2948, val acc: 0.9164  (best train acc: 0.8918, best val acc: 0.9298, best train loss: 0.2947  @ epoch 3356 )\n",
      "[Epoch: 3460] train loss: 0.3031, train acc: 0.8856, val loss: 0.3082, val acc: 0.9039  (best train acc: 0.8928, best val acc: 0.9298, best train loss: 0.2936  @ epoch 3453 )\n",
      "[Epoch: 3480] train loss: 0.3026, train acc: 0.8973, val loss: 0.3097, val acc: 0.8995  (best train acc: 0.8973, best val acc: 0.9298, best train loss: 0.2936  @ epoch 3453 )\n",
      "[Epoch: 3500] train loss: 0.3094, train acc: 0.8784, val loss: 0.2849, val acc: 0.9113  (best train acc: 0.8996, best val acc: 0.9298, best train loss: 0.2793  @ epoch 3490 )\n",
      "[Epoch: 3520] train loss: 0.3368, train acc: 0.8837, val loss: 0.2947, val acc: 0.9184  (best train acc: 0.8996, best val acc: 0.9298, best train loss: 0.2793  @ epoch 3490 )\n",
      "[Epoch: 3540] train loss: 0.3049, train acc: 0.8939, val loss: 0.3102, val acc: 0.9032  (best train acc: 0.8996, best val acc: 0.9305, best train loss: 0.2793  @ epoch 3490 )\n",
      "[Epoch: 3560] train loss: 0.3172, train acc: 0.8828, val loss: 0.3193, val acc: 0.9076  (best train acc: 0.8996, best val acc: 0.9349, best train loss: 0.2793  @ epoch 3490 )\n",
      "[Epoch: 3580] train loss: 0.3823, train acc: 0.8632, val loss: 0.2726, val acc: 0.9191  (best train acc: 0.8996, best val acc: 0.9366, best train loss: 0.2793  @ epoch 3490 )\n",
      "[Epoch: 3600] train loss: 0.2851, train acc: 0.8940, val loss: 0.2617, val acc: 0.9204  (best train acc: 0.9007, best val acc: 0.9366, best train loss: 0.2758  @ epoch 3590 )\n",
      "[Epoch: 3620] train loss: 0.3141, train acc: 0.8801, val loss: 0.2756, val acc: 0.9248  (best train acc: 0.9036, best val acc: 0.9366, best train loss: 0.2711  @ epoch 3613 )\n",
      "[Epoch: 3640] train loss: 0.2989, train acc: 0.8901, val loss: 0.3041, val acc: 0.9039  (best train acc: 0.9036, best val acc: 0.9366, best train loss: 0.2711  @ epoch 3613 )\n",
      "[Epoch: 3660] train loss: 0.3536, train acc: 0.8675, val loss: 0.3420, val acc: 0.8668  (best train acc: 0.9036, best val acc: 0.9366, best train loss: 0.2711  @ epoch 3613 )\n",
      "[Epoch: 3680] train loss: 0.2912, train acc: 0.8919, val loss: 0.2533, val acc: 0.9322  (best train acc: 0.9036, best val acc: 0.9369, best train loss: 0.2711  @ epoch 3613 )\n",
      "[Epoch: 3700] train loss: 0.2908, train acc: 0.8887, val loss: 0.2610, val acc: 0.9272  (best train acc: 0.9036, best val acc: 0.9369, best train loss: 0.2711  @ epoch 3613 )\n",
      "[Epoch: 3720] train loss: 0.2994, train acc: 0.8822, val loss: 0.2680, val acc: 0.9211  (best train acc: 0.9036, best val acc: 0.9369, best train loss: 0.2637  @ epoch 3706 )\n",
      "[Epoch: 3740] train loss: 0.2905, train acc: 0.8867, val loss: 0.2517, val acc: 0.9272  (best train acc: 0.9059, best val acc: 0.9369, best train loss: 0.2637  @ epoch 3706 )\n",
      "[Epoch: 3760] train loss: 0.2767, train acc: 0.9023, val loss: 0.2442, val acc: 0.9342  (best train acc: 0.9078, best val acc: 0.9369, best train loss: 0.2637  @ epoch 3706 )\n",
      "[Epoch: 3780] train loss: 0.2722, train acc: 0.9036, val loss: 0.2491, val acc: 0.9268  (best train acc: 0.9078, best val acc: 0.9393, best train loss: 0.2637  @ epoch 3706 )\n",
      "[Epoch: 3800] train loss: 0.2785, train acc: 0.8997, val loss: 0.2498, val acc: 0.9339  (best train acc: 0.9078, best val acc: 0.9393, best train loss: 0.2534  @ epoch 3790 )\n",
      "[Epoch: 3820] train loss: 0.2725, train acc: 0.8974, val loss: 0.2486, val acc: 0.9322  (best train acc: 0.9085, best val acc: 0.9393, best train loss: 0.2534  @ epoch 3790 )\n",
      "[Epoch: 3840] train loss: 0.2705, train acc: 0.9036, val loss: 0.2552, val acc: 0.9352  (best train acc: 0.9085, best val acc: 0.9393, best train loss: 0.2534  @ epoch 3790 )\n",
      "[Epoch: 3860] train loss: 0.2659, train acc: 0.9010, val loss: 0.2803, val acc: 0.9143  (best train acc: 0.9085, best val acc: 0.9393, best train loss: 0.2534  @ epoch 3790 )\n",
      "[Epoch: 3880] train loss: 0.2855, train acc: 0.8971, val loss: 0.3021, val acc: 0.8985  (best train acc: 0.9085, best val acc: 0.9406, best train loss: 0.2534  @ epoch 3790 )\n",
      "[Epoch: 3900] train loss: 0.3230, train acc: 0.8769, val loss: 0.2933, val acc: 0.9059  (best train acc: 0.9085, best val acc: 0.9413, best train loss: 0.2534  @ epoch 3790 )\n",
      "[Epoch: 3920] train loss: 0.2802, train acc: 0.9058, val loss: 0.2795, val acc: 0.9130  (best train acc: 0.9096, best val acc: 0.9413, best train loss: 0.2485  @ epoch 3914 )\n",
      "[Epoch: 3940] train loss: 0.2883, train acc: 0.8884, val loss: 0.2594, val acc: 0.9329  (best train acc: 0.9096, best val acc: 0.9413, best train loss: 0.2480  @ epoch 3934 )\n",
      "[Epoch: 3960] train loss: 0.2762, train acc: 0.8955, val loss: 0.2677, val acc: 0.9076  (best train acc: 0.9106, best val acc: 0.9423, best train loss: 0.2450  @ epoch 3951 )\n",
      "[Epoch: 3980] train loss: 0.2758, train acc: 0.9001, val loss: 0.2794, val acc: 0.9120  (best train acc: 0.9106, best val acc: 0.9423, best train loss: 0.2416  @ epoch 3976 )\n",
      "[Epoch: 4000] train loss: 0.2673, train acc: 0.9028, val loss: 0.3012, val acc: 0.9069  (best train acc: 0.9106, best val acc: 0.9423, best train loss: 0.2416  @ epoch 3976 )\n",
      "[Epoch: 4020] train loss: 0.2520, train acc: 0.9039, val loss: 0.2481, val acc: 0.9312  (best train acc: 0.9115, best val acc: 0.9423, best train loss: 0.2416  @ epoch 3976 )\n",
      "[Epoch: 4040] train loss: 0.3010, train acc: 0.8887, val loss: 0.2520, val acc: 0.9305  (best train acc: 0.9115, best val acc: 0.9423, best train loss: 0.2416  @ epoch 3976 )\n",
      "[Epoch: 4060] train loss: 0.3026, train acc: 0.8878, val loss: 0.2683, val acc: 0.9261  (best train acc: 0.9115, best val acc: 0.9423, best train loss: 0.2416  @ epoch 3976 )\n",
      "[Epoch: 4080] train loss: 0.2583, train acc: 0.9078, val loss: 0.2865, val acc: 0.9059  (best train acc: 0.9115, best val acc: 0.9423, best train loss: 0.2416  @ epoch 3976 )\n",
      "[Epoch: 4100] train loss: 0.2539, train acc: 0.9018, val loss: 0.2657, val acc: 0.9177  (best train acc: 0.9115, best val acc: 0.9423, best train loss: 0.2416  @ epoch 3976 )\n",
      "[Epoch: 4120] train loss: 0.2927, train acc: 0.8916, val loss: 0.2598, val acc: 0.9352  (best train acc: 0.9115, best val acc: 0.9423, best train loss: 0.2416  @ epoch 3976 )\n",
      "[Epoch: 4140] train loss: 0.2753, train acc: 0.8996, val loss: 0.2837, val acc: 0.9089  (best train acc: 0.9115, best val acc: 0.9423, best train loss: 0.2416  @ epoch 3976 )\n",
      "[Epoch: 4160] train loss: 0.2780, train acc: 0.8845, val loss: 0.2412, val acc: 0.9379  (best train acc: 0.9140, best val acc: 0.9423, best train loss: 0.2410  @ epoch 4159 )\n",
      "[Epoch: 4180] train loss: 0.2636, train acc: 0.8999, val loss: 0.3102, val acc: 0.8948  (best train acc: 0.9146, best val acc: 0.9423, best train loss: 0.2390  @ epoch 4166 )\n",
      "[Epoch: 4200] train loss: 0.2473, train acc: 0.9094, val loss: 0.2486, val acc: 0.9322  (best train acc: 0.9146, best val acc: 0.9423, best train loss: 0.2390  @ epoch 4166 )\n",
      "[Epoch: 4220] train loss: 0.2499, train acc: 0.9121, val loss: 0.2470, val acc: 0.9322  (best train acc: 0.9146, best val acc: 0.9423, best train loss: 0.2390  @ epoch 4166 )\n",
      "[Epoch: 4240] train loss: 0.2597, train acc: 0.9031, val loss: 0.2923, val acc: 0.9012  (best train acc: 0.9146, best val acc: 0.9433, best train loss: 0.2390  @ epoch 4166 )\n",
      "[Epoch: 4260] train loss: 0.2651, train acc: 0.9043, val loss: 0.2725, val acc: 0.9130  (best train acc: 0.9146, best val acc: 0.9433, best train loss: 0.2390  @ epoch 4166 )\n",
      "[Epoch: 4280] train loss: 0.2947, train acc: 0.8834, val loss: 0.2417, val acc: 0.9292  (best train acc: 0.9146, best val acc: 0.9433, best train loss: 0.2390  @ epoch 4166 )\n",
      "[Epoch: 4300] train loss: 0.2428, train acc: 0.9124, val loss: 0.2454, val acc: 0.9315  (best train acc: 0.9146, best val acc: 0.9457, best train loss: 0.2390  @ epoch 4166 )\n",
      "[Epoch: 4320] train loss: 0.2664, train acc: 0.9011, val loss: 0.2637, val acc: 0.9322  (best train acc: 0.9146, best val acc: 0.9457, best train loss: 0.2390  @ epoch 4166 )\n",
      "[Epoch: 4340] train loss: 0.2757, train acc: 0.8956, val loss: 0.2450, val acc: 0.9339  (best train acc: 0.9146, best val acc: 0.9457, best train loss: 0.2390  @ epoch 4166 )\n",
      "[Epoch: 4360] train loss: 0.2728, train acc: 0.8926, val loss: 0.2745, val acc: 0.9157  (best train acc: 0.9155, best val acc: 0.9457, best train loss: 0.2285  @ epoch 4343 )\n",
      "[Epoch: 4380] train loss: 0.2689, train acc: 0.9028, val loss: 0.2796, val acc: 0.9093  (best train acc: 0.9155, best val acc: 0.9457, best train loss: 0.2285  @ epoch 4343 )\n",
      "[Epoch: 4400] train loss: 0.2409, train acc: 0.9161, val loss: 0.3024, val acc: 0.8924  (best train acc: 0.9161, best val acc: 0.9457, best train loss: 0.2285  @ epoch 4343 )\n",
      "[Epoch: 4420] train loss: 0.2638, train acc: 0.9044, val loss: 0.2770, val acc: 0.9073  (best train acc: 0.9161, best val acc: 0.9457, best train loss: 0.2285  @ epoch 4343 )\n",
      "[Epoch: 4440] train loss: 0.2913, train acc: 0.8940, val loss: 0.2629, val acc: 0.9221  (best train acc: 0.9164, best val acc: 0.9457, best train loss: 0.2285  @ epoch 4343 )\n",
      "[Epoch: 4460] train loss: 0.2580, train acc: 0.9044, val loss: 0.2423, val acc: 0.9346  (best train acc: 0.9164, best val acc: 0.9457, best train loss: 0.2285  @ epoch 4343 )\n",
      "[Epoch: 4480] train loss: 0.2733, train acc: 0.8931, val loss: 0.2405, val acc: 0.9312  (best train acc: 0.9164, best val acc: 0.9457, best train loss: 0.2285  @ epoch 4343 )\n",
      "[Epoch: 4500] train loss: 0.2477, train acc: 0.9133, val loss: 0.2319, val acc: 0.9356  (best train acc: 0.9164, best val acc: 0.9457, best train loss: 0.2285  @ epoch 4343 )\n",
      "[Epoch: 4520] train loss: 0.2655, train acc: 0.9032, val loss: 0.2520, val acc: 0.9234  (best train acc: 0.9164, best val acc: 0.9457, best train loss: 0.2285  @ epoch 4343 )\n",
      "[Epoch: 4540] train loss: 0.2622, train acc: 0.9047, val loss: 0.2726, val acc: 0.9137  (best train acc: 0.9164, best val acc: 0.9457, best train loss: 0.2285  @ epoch 4343 )\n",
      "[Epoch: 4560] train loss: 0.2425, train acc: 0.9079, val loss: 0.2602, val acc: 0.9123  (best train acc: 0.9164, best val acc: 0.9457, best train loss: 0.2285  @ epoch 4343 )\n",
      "[Epoch: 4580] train loss: 0.2624, train acc: 0.8976, val loss: 0.2072, val acc: 0.9457  (best train acc: 0.9180, best val acc: 0.9457, best train loss: 0.2285  @ epoch 4343 )\n",
      "[Epoch: 4600] train loss: 0.2315, train acc: 0.9154, val loss: 0.2341, val acc: 0.9406  (best train acc: 0.9180, best val acc: 0.9457, best train loss: 0.2285  @ epoch 4343 )\n",
      "[Epoch: 4620] train loss: 0.2444, train acc: 0.9054, val loss: 0.2369, val acc: 0.9261  (best train acc: 0.9180, best val acc: 0.9464, best train loss: 0.2285  @ epoch 4343 )\n",
      "[Epoch: 4640] train loss: 0.2602, train acc: 0.9033, val loss: 0.2598, val acc: 0.9231  (best train acc: 0.9180, best val acc: 0.9464, best train loss: 0.2285  @ epoch 4343 )\n",
      "[Epoch: 4660] train loss: 0.2451, train acc: 0.9091, val loss: 0.2474, val acc: 0.9305  (best train acc: 0.9180, best val acc: 0.9464, best train loss: 0.2285  @ epoch 4343 )\n",
      "[Epoch: 4680] train loss: 0.2489, train acc: 0.9109, val loss: 0.2621, val acc: 0.9157  (best train acc: 0.9180, best val acc: 0.9464, best train loss: 0.2285  @ epoch 4343 )\n",
      "[Epoch: 4700] train loss: 0.2672, train acc: 0.8953, val loss: 0.2685, val acc: 0.9137  (best train acc: 0.9180, best val acc: 0.9464, best train loss: 0.2285  @ epoch 4343 )\n",
      "[Epoch: 4720] train loss: 0.2641, train acc: 0.9016, val loss: 0.2844, val acc: 0.9073  (best train acc: 0.9182, best val acc: 0.9464, best train loss: 0.2285  @ epoch 4343 )\n",
      "[Epoch: 4740] train loss: 0.2744, train acc: 0.9011, val loss: 0.2617, val acc: 0.9187  (best train acc: 0.9211, best val acc: 0.9464, best train loss: 0.2259  @ epoch 4728 )\n",
      "[Epoch: 4760] train loss: 0.2727, train acc: 0.8968, val loss: 0.2423, val acc: 0.9356  (best train acc: 0.9211, best val acc: 0.9464, best train loss: 0.2259  @ epoch 4728 )\n",
      "[Epoch: 4780] train loss: 0.2565, train acc: 0.9026, val loss: 0.2457, val acc: 0.9356  (best train acc: 0.9211, best val acc: 0.9464, best train loss: 0.2259  @ epoch 4728 )\n",
      "[Epoch: 4800] train loss: 0.2508, train acc: 0.9045, val loss: 0.2466, val acc: 0.9153  (best train acc: 0.9211, best val acc: 0.9464, best train loss: 0.2259  @ epoch 4728 )\n",
      "[Epoch: 4820] train loss: 0.2619, train acc: 0.8976, val loss: 0.2119, val acc: 0.9403  (best train acc: 0.9211, best val acc: 0.9464, best train loss: 0.2259  @ epoch 4728 )\n",
      "[Epoch: 4840] train loss: 0.2986, train acc: 0.8852, val loss: 0.2289, val acc: 0.9295  (best train acc: 0.9211, best val acc: 0.9464, best train loss: 0.2259  @ epoch 4728 )\n",
      "[Epoch: 4860] train loss: 0.2635, train acc: 0.8965, val loss: 0.2521, val acc: 0.9197  (best train acc: 0.9211, best val acc: 0.9464, best train loss: 0.2259  @ epoch 4728 )\n",
      "[Epoch: 4880] train loss: 0.2399, train acc: 0.9042, val loss: 0.2565, val acc: 0.9157  (best train acc: 0.9211, best val acc: 0.9464, best train loss: 0.2259  @ epoch 4728 )\n",
      "[Epoch: 4900] train loss: 0.2436, train acc: 0.9117, val loss: 0.2176, val acc: 0.9403  (best train acc: 0.9211, best val acc: 0.9464, best train loss: 0.2259  @ epoch 4728 )\n",
      "[Epoch: 4920] train loss: 0.2474, train acc: 0.9033, val loss: 0.2748, val acc: 0.9073  (best train acc: 0.9211, best val acc: 0.9464, best train loss: 0.2259  @ epoch 4728 )\n",
      "[Epoch: 4940] train loss: 0.2936, train acc: 0.8829, val loss: 0.2515, val acc: 0.9251  (best train acc: 0.9211, best val acc: 0.9464, best train loss: 0.2259  @ epoch 4728 )\n",
      "[Epoch: 4960] train loss: 0.2557, train acc: 0.9062, val loss: 0.2338, val acc: 0.9336  (best train acc: 0.9211, best val acc: 0.9464, best train loss: 0.2259  @ epoch 4728 )\n",
      "[Epoch: 4980] train loss: 0.2812, train acc: 0.8953, val loss: 0.2561, val acc: 0.9194  (best train acc: 0.9211, best val acc: 0.9464, best train loss: 0.2259  @ epoch 4728 )\n",
      "[Epoch: 5000] train loss: 0.2389, train acc: 0.9124, val loss: 0.2614, val acc: 0.9130  (best train acc: 0.9211, best val acc: 0.9464, best train loss: 0.2259  @ epoch 4728 )\n",
      "[Epoch: 5020] train loss: 0.2419, train acc: 0.9088, val loss: 0.2569, val acc: 0.9268  (best train acc: 0.9211, best val acc: 0.9464, best train loss: 0.2259  @ epoch 4728 )\n",
      "[Epoch: 5040] train loss: 0.2687, train acc: 0.8983, val loss: 0.2229, val acc: 0.9406  (best train acc: 0.9211, best val acc: 0.9464, best train loss: 0.2248  @ epoch 5036 )\n",
      "[Epoch: 5060] train loss: 0.2497, train acc: 0.9052, val loss: 0.2452, val acc: 0.9207  (best train acc: 0.9211, best val acc: 0.9464, best train loss: 0.2182  @ epoch 5041 )\n",
      "[Epoch: 5080] train loss: 0.2422, train acc: 0.9130, val loss: 0.2145, val acc: 0.9336  (best train acc: 0.9219, best val acc: 0.9464, best train loss: 0.2182  @ epoch 5041 )\n",
      "[Epoch: 5100] train loss: 0.2947, train acc: 0.8957, val loss: 0.2441, val acc: 0.9332  (best train acc: 0.9219, best val acc: 0.9464, best train loss: 0.2108  @ epoch 5082 )\n",
      "[Epoch: 5120] train loss: 0.2362, train acc: 0.9093, val loss: 0.2353, val acc: 0.9393  (best train acc: 0.9219, best val acc: 0.9464, best train loss: 0.2108  @ epoch 5082 )\n",
      "[Epoch: 5140] train loss: 0.2584, train acc: 0.9019, val loss: 0.2583, val acc: 0.9153  (best train acc: 0.9219, best val acc: 0.9464, best train loss: 0.2108  @ epoch 5082 )\n",
      "[Epoch: 5160] train loss: 0.2243, train acc: 0.9124, val loss: 0.2253, val acc: 0.9292  (best train acc: 0.9234, best val acc: 0.9464, best train loss: 0.2108  @ epoch 5082 )\n",
      "[Epoch: 5180] train loss: 0.2698, train acc: 0.8908, val loss: 0.2383, val acc: 0.9329  (best train acc: 0.9234, best val acc: 0.9464, best train loss: 0.2108  @ epoch 5082 )\n",
      "[Epoch: 5200] train loss: 0.2531, train acc: 0.9123, val loss: 0.2392, val acc: 0.9366  (best train acc: 0.9239, best val acc: 0.9464, best train loss: 0.2108  @ epoch 5082 )\n",
      "[Epoch: 5220] train loss: 0.2401, train acc: 0.9177, val loss: 0.2591, val acc: 0.9170  (best train acc: 0.9239, best val acc: 0.9484, best train loss: 0.2108  @ epoch 5082 )\n",
      "[Epoch: 5240] train loss: 0.2472, train acc: 0.9069, val loss: 0.2509, val acc: 0.9231  (best train acc: 0.9239, best val acc: 0.9484, best train loss: 0.2108  @ epoch 5082 )\n",
      "[Epoch: 5260] train loss: 0.2498, train acc: 0.9041, val loss: 0.2289, val acc: 0.9339  (best train acc: 0.9267, best val acc: 0.9484, best train loss: 0.2083  @ epoch 5246 )\n",
      "[Epoch: 5280] train loss: 0.2174, train acc: 0.9250, val loss: 0.2373, val acc: 0.9369  (best train acc: 0.9267, best val acc: 0.9484, best train loss: 0.2083  @ epoch 5246 )\n",
      "[Epoch: 5300] train loss: 0.2295, train acc: 0.9199, val loss: 0.2277, val acc: 0.9329  (best train acc: 0.9267, best val acc: 0.9484, best train loss: 0.2083  @ epoch 5246 )\n",
      "[Epoch: 5320] train loss: 0.2391, train acc: 0.9083, val loss: 0.2671, val acc: 0.9110  (best train acc: 0.9267, best val acc: 0.9484, best train loss: 0.2083  @ epoch 5246 )\n",
      "[Epoch: 5340] train loss: 0.2615, train acc: 0.9014, val loss: 0.2811, val acc: 0.9005  (best train acc: 0.9267, best val acc: 0.9484, best train loss: 0.2083  @ epoch 5246 )\n",
      "[Epoch: 5360] train loss: 0.2862, train acc: 0.8845, val loss: 0.2988, val acc: 0.8860  (best train acc: 0.9267, best val acc: 0.9484, best train loss: 0.2083  @ epoch 5246 )\n",
      "[Epoch: 5380] train loss: 0.2305, train acc: 0.9127, val loss: 0.2343, val acc: 0.9302  (best train acc: 0.9267, best val acc: 0.9484, best train loss: 0.2083  @ epoch 5246 )\n",
      "[Epoch: 5400] train loss: 0.2986, train acc: 0.9002, val loss: 0.2786, val acc: 0.9073  (best train acc: 0.9267, best val acc: 0.9484, best train loss: 0.2083  @ epoch 5246 )\n",
      "[Epoch: 5420] train loss: 0.2422, train acc: 0.9104, val loss: 0.2365, val acc: 0.9288  (best train acc: 0.9267, best val acc: 0.9484, best train loss: 0.2083  @ epoch 5246 )\n",
      "[Epoch: 5440] train loss: 0.2248, train acc: 0.9213, val loss: 0.2308, val acc: 0.9325  (best train acc: 0.9267, best val acc: 0.9484, best train loss: 0.2083  @ epoch 5246 )\n",
      "[Epoch: 5460] train loss: 0.2447, train acc: 0.9063, val loss: 0.2097, val acc: 0.9444  (best train acc: 0.9267, best val acc: 0.9484, best train loss: 0.2083  @ epoch 5246 )\n",
      "[Epoch: 5480] train loss: 0.2282, train acc: 0.9189, val loss: 0.2526, val acc: 0.9180  (best train acc: 0.9267, best val acc: 0.9484, best train loss: 0.2083  @ epoch 5246 )\n",
      "[Epoch: 5500] train loss: 0.2269, train acc: 0.9114, val loss: 0.2541, val acc: 0.9157  (best train acc: 0.9267, best val acc: 0.9484, best train loss: 0.2083  @ epoch 5246 )\n",
      "[Epoch: 5520] train loss: 0.2465, train acc: 0.9114, val loss: 0.2313, val acc: 0.9363  (best train acc: 0.9267, best val acc: 0.9484, best train loss: 0.2083  @ epoch 5246 )\n",
      "[Epoch: 5540] train loss: 0.2404, train acc: 0.9102, val loss: 0.2296, val acc: 0.9369  (best train acc: 0.9267, best val acc: 0.9484, best train loss: 0.2083  @ epoch 5246 )\n",
      "[Epoch: 5560] train loss: 0.2145, train acc: 0.9195, val loss: 0.2340, val acc: 0.9191  (best train acc: 0.9267, best val acc: 0.9484, best train loss: 0.2083  @ epoch 5246 )\n",
      "[Epoch: 5580] train loss: 0.2331, train acc: 0.9085, val loss: 0.2082, val acc: 0.9342  (best train acc: 0.9267, best val acc: 0.9484, best train loss: 0.2083  @ epoch 5246 )\n",
      "[Epoch: 5600] train loss: 0.2251, train acc: 0.9203, val loss: 0.2502, val acc: 0.9207  (best train acc: 0.9267, best val acc: 0.9484, best train loss: 0.2083  @ epoch 5246 )\n",
      "[Epoch: 5620] train loss: 0.2270, train acc: 0.9187, val loss: 0.2632, val acc: 0.9130  (best train acc: 0.9267, best val acc: 0.9484, best train loss: 0.2083  @ epoch 5246 )\n",
      "[Epoch: 5640] train loss: 0.2237, train acc: 0.9140, val loss: 0.2252, val acc: 0.9464  (best train acc: 0.9267, best val acc: 0.9487, best train loss: 0.2083  @ epoch 5246 )\n",
      "[Epoch: 5660] train loss: 0.2459, train acc: 0.9005, val loss: 0.2951, val acc: 0.8981  (best train acc: 0.9269, best val acc: 0.9487, best train loss: 0.2022  @ epoch 5654 )\n",
      "[Epoch: 5680] train loss: 0.2367, train acc: 0.9122, val loss: 0.2438, val acc: 0.9234  (best train acc: 0.9269, best val acc: 0.9487, best train loss: 0.2022  @ epoch 5654 )\n",
      "[Epoch: 5700] train loss: 0.2452, train acc: 0.9062, val loss: 0.2374, val acc: 0.9305  (best train acc: 0.9269, best val acc: 0.9487, best train loss: 0.2022  @ epoch 5654 )\n",
      "[Epoch: 5720] train loss: 0.2196, train acc: 0.9171, val loss: 0.2385, val acc: 0.9221  (best train acc: 0.9269, best val acc: 0.9487, best train loss: 0.2022  @ epoch 5654 )\n",
      "[Epoch: 5740] train loss: 0.2213, train acc: 0.9168, val loss: 0.2239, val acc: 0.9315  (best train acc: 0.9269, best val acc: 0.9487, best train loss: 0.2022  @ epoch 5654 )\n",
      "[Epoch: 5760] train loss: 0.2338, train acc: 0.9162, val loss: 0.2261, val acc: 0.9386  (best train acc: 0.9269, best val acc: 0.9487, best train loss: 0.2022  @ epoch 5654 )\n",
      "[Epoch: 5780] train loss: 0.2902, train acc: 0.8942, val loss: 0.2501, val acc: 0.9201  (best train acc: 0.9269, best val acc: 0.9487, best train loss: 0.2022  @ epoch 5654 )\n",
      "[Epoch: 5800] train loss: 0.2322, train acc: 0.9195, val loss: 0.2309, val acc: 0.9373  (best train acc: 0.9269, best val acc: 0.9487, best train loss: 0.2022  @ epoch 5654 )\n",
      "[Epoch: 5820] train loss: 0.2242, train acc: 0.9188, val loss: 0.2811, val acc: 0.9005  (best train acc: 0.9269, best val acc: 0.9487, best train loss: 0.2022  @ epoch 5654 )\n",
      "[Epoch: 5840] train loss: 0.2512, train acc: 0.9078, val loss: 0.2359, val acc: 0.9379  (best train acc: 0.9269, best val acc: 0.9487, best train loss: 0.2022  @ epoch 5654 )\n",
      "[Epoch: 5860] train loss: 0.2597, train acc: 0.9005, val loss: 0.2370, val acc: 0.9228  (best train acc: 0.9269, best val acc: 0.9494, best train loss: 0.2022  @ epoch 5654 )\n",
      "[Epoch: 5880] train loss: 0.2255, train acc: 0.9196, val loss: 0.2225, val acc: 0.9410  (best train acc: 0.9285, best val acc: 0.9494, best train loss: 0.2022  @ epoch 5654 )\n",
      "[Epoch: 5900] train loss: 0.2399, train acc: 0.9075, val loss: 0.2759, val acc: 0.9126  (best train acc: 0.9285, best val acc: 0.9494, best train loss: 0.2022  @ epoch 5654 )\n",
      "[Epoch: 5920] train loss: 0.2482, train acc: 0.9081, val loss: 0.2630, val acc: 0.9157  (best train acc: 0.9285, best val acc: 0.9494, best train loss: 0.2022  @ epoch 5654 )\n",
      "[Epoch: 5940] train loss: 0.2552, train acc: 0.8997, val loss: 0.2297, val acc: 0.9332  (best train acc: 0.9285, best val acc: 0.9494, best train loss: 0.2022  @ epoch 5654 )\n",
      "[Epoch: 5960] train loss: 0.2436, train acc: 0.9036, val loss: 0.2064, val acc: 0.9430  (best train acc: 0.9285, best val acc: 0.9494, best train loss: 0.2022  @ epoch 5654 )\n",
      "[Epoch: 5980] train loss: 0.2393, train acc: 0.9117, val loss: 0.2445, val acc: 0.9319  (best train acc: 0.9285, best val acc: 0.9494, best train loss: 0.2022  @ epoch 5654 )\n",
      "[Epoch: 6000] train loss: 0.2304, train acc: 0.9189, val loss: 0.2276, val acc: 0.9339  (best train acc: 0.9285, best val acc: 0.9494, best train loss: 0.2022  @ epoch 5654 )\n",
      "[Epoch: 6020] train loss: 0.2143, train acc: 0.9166, val loss: 0.2417, val acc: 0.9238  (best train acc: 0.9285, best val acc: 0.9494, best train loss: 0.2022  @ epoch 5654 )\n",
      "[Epoch: 6040] train loss: 0.2236, train acc: 0.9156, val loss: 0.2169, val acc: 0.9396  (best train acc: 0.9285, best val acc: 0.9494, best train loss: 0.2022  @ epoch 5654 )\n",
      "[Epoch: 6060] train loss: 0.2617, train acc: 0.8984, val loss: 0.2479, val acc: 0.9106  (best train acc: 0.9285, best val acc: 0.9494, best train loss: 0.2022  @ epoch 5654 )\n",
      "[Epoch: 6080] train loss: 0.2139, train acc: 0.9210, val loss: 0.2154, val acc: 0.9440  (best train acc: 0.9285, best val acc: 0.9494, best train loss: 0.2022  @ epoch 5654 )\n",
      "[Epoch: 6100] train loss: 0.2388, train acc: 0.9141, val loss: 0.2409, val acc: 0.9258  (best train acc: 0.9285, best val acc: 0.9494, best train loss: 0.2022  @ epoch 5654 )\n",
      "[Epoch: 6120] train loss: 0.2193, train acc: 0.9177, val loss: 0.2095, val acc: 0.9487  (best train acc: 0.9285, best val acc: 0.9494, best train loss: 0.2022  @ epoch 5654 )\n",
      "[Epoch: 6140] train loss: 0.2266, train acc: 0.9143, val loss: 0.2282, val acc: 0.9278  (best train acc: 0.9285, best val acc: 0.9497, best train loss: 0.2022  @ epoch 5654 )\n",
      "[Epoch: 6160] train loss: 0.2391, train acc: 0.9072, val loss: 0.2219, val acc: 0.9251  (best train acc: 0.9285, best val acc: 0.9497, best train loss: 0.2022  @ epoch 5654 )\n",
      "[Epoch: 6180] train loss: 0.2162, train acc: 0.9199, val loss: 0.2309, val acc: 0.9265  (best train acc: 0.9285, best val acc: 0.9497, best train loss: 0.2022  @ epoch 5654 )\n",
      "[Epoch: 6200] train loss: 0.2461, train acc: 0.9037, val loss: 0.2329, val acc: 0.9298  (best train acc: 0.9285, best val acc: 0.9497, best train loss: 0.2022  @ epoch 5654 )\n",
      "[Epoch: 6220] train loss: 0.2248, train acc: 0.9147, val loss: 0.2332, val acc: 0.9342  (best train acc: 0.9294, best val acc: 0.9497, best train loss: 0.1986  @ epoch 6205 )\n",
      "[Epoch: 6240] train loss: 0.2096, train acc: 0.9250, val loss: 0.2309, val acc: 0.9164  (best train acc: 0.9294, best val acc: 0.9497, best train loss: 0.1986  @ epoch 6205 )\n",
      "[Epoch: 6260] train loss: 0.2260, train acc: 0.9123, val loss: 0.2404, val acc: 0.9228  (best train acc: 0.9294, best val acc: 0.9497, best train loss: 0.1986  @ epoch 6205 )\n",
      "[Epoch: 6280] train loss: 0.2145, train acc: 0.9187, val loss: 0.2208, val acc: 0.9376  (best train acc: 0.9294, best val acc: 0.9497, best train loss: 0.1986  @ epoch 6205 )\n",
      "[Epoch: 6300] train loss: 0.2360, train acc: 0.9150, val loss: 0.2799, val acc: 0.8971  (best train acc: 0.9294, best val acc: 0.9497, best train loss: 0.1975  @ epoch 6283 )\n",
      "[Epoch: 6320] train loss: 0.2650, train acc: 0.8936, val loss: 0.2054, val acc: 0.9346  (best train acc: 0.9294, best val acc: 0.9497, best train loss: 0.1975  @ epoch 6283 )\n",
      "[Epoch: 6340] train loss: 0.2189, train acc: 0.9187, val loss: 0.1978, val acc: 0.9437  (best train acc: 0.9294, best val acc: 0.9497, best train loss: 0.1975  @ epoch 6283 )\n",
      "[Epoch: 6360] train loss: 0.2543, train acc: 0.8955, val loss: 0.2382, val acc: 0.9298  (best train acc: 0.9294, best val acc: 0.9497, best train loss: 0.1975  @ epoch 6283 )\n",
      "[Epoch: 6380] train loss: 0.2307, train acc: 0.9227, val loss: 0.2558, val acc: 0.9106  (best train acc: 0.9294, best val acc: 0.9497, best train loss: 0.1975  @ epoch 6283 )\n",
      "[Epoch: 6400] train loss: 0.2069, train acc: 0.9201, val loss: 0.2180, val acc: 0.9376  (best train acc: 0.9294, best val acc: 0.9497, best train loss: 0.1975  @ epoch 6283 )\n",
      "[Epoch: 6420] train loss: 0.2344, train acc: 0.9129, val loss: 0.2292, val acc: 0.9248  (best train acc: 0.9294, best val acc: 0.9497, best train loss: 0.1975  @ epoch 6283 )\n",
      "[Epoch: 6440] train loss: 0.2121, train acc: 0.9221, val loss: 0.2555, val acc: 0.9137  (best train acc: 0.9294, best val acc: 0.9504, best train loss: 0.1969  @ epoch 6426 )\n",
      "[Epoch: 6460] train loss: 0.2189, train acc: 0.9221, val loss: 0.2066, val acc: 0.9427  (best train acc: 0.9294, best val acc: 0.9504, best train loss: 0.1969  @ epoch 6426 )\n",
      "[Epoch: 6480] train loss: 0.2365, train acc: 0.9077, val loss: 0.1958, val acc: 0.9420  (best train acc: 0.9297, best val acc: 0.9504, best train loss: 0.1890  @ epoch 6471 )\n",
      "[Epoch: 6500] train loss: 0.2161, train acc: 0.9164, val loss: 0.2224, val acc: 0.9325  (best train acc: 0.9297, best val acc: 0.9504, best train loss: 0.1890  @ epoch 6471 )\n",
      "[Epoch: 6520] train loss: 0.2276, train acc: 0.9139, val loss: 0.2204, val acc: 0.9339  (best train acc: 0.9305, best val acc: 0.9504, best train loss: 0.1890  @ epoch 6471 )\n",
      "[Epoch: 6540] train loss: 0.2012, train acc: 0.9250, val loss: 0.2095, val acc: 0.9470  (best train acc: 0.9305, best val acc: 0.9504, best train loss: 0.1890  @ epoch 6471 )\n",
      "[Epoch: 6560] train loss: 0.1961, train acc: 0.9263, val loss: 0.2168, val acc: 0.9470  (best train acc: 0.9305, best val acc: 0.9514, best train loss: 0.1890  @ epoch 6471 )\n",
      "[Epoch: 6580] train loss: 0.2142, train acc: 0.9157, val loss: 0.2483, val acc: 0.9214  (best train acc: 0.9305, best val acc: 0.9514, best train loss: 0.1890  @ epoch 6471 )\n",
      "[Epoch: 6600] train loss: 0.2132, train acc: 0.9219, val loss: 0.2276, val acc: 0.9278  (best train acc: 0.9305, best val acc: 0.9514, best train loss: 0.1890  @ epoch 6471 )\n",
      "[Epoch: 6620] train loss: 0.2158, train acc: 0.9202, val loss: 0.2193, val acc: 0.9393  (best train acc: 0.9305, best val acc: 0.9514, best train loss: 0.1890  @ epoch 6471 )\n",
      "[Epoch: 6640] train loss: 0.2223, train acc: 0.9106, val loss: 0.2296, val acc: 0.9234  (best train acc: 0.9305, best val acc: 0.9514, best train loss: 0.1890  @ epoch 6471 )\n",
      "[Epoch: 6660] train loss: 0.2233, train acc: 0.9199, val loss: 0.2402, val acc: 0.9201  (best train acc: 0.9305, best val acc: 0.9514, best train loss: 0.1890  @ epoch 6471 )\n",
      "[Epoch: 6680] train loss: 0.2258, train acc: 0.9188, val loss: 0.2072, val acc: 0.9342  (best train acc: 0.9323, best val acc: 0.9514, best train loss: 0.1890  @ epoch 6471 )\n",
      "[Epoch: 6700] train loss: 0.2287, train acc: 0.9115, val loss: 0.2215, val acc: 0.9410  (best train acc: 0.9323, best val acc: 0.9514, best train loss: 0.1890  @ epoch 6471 )\n",
      "[Epoch: 6720] train loss: 0.2168, train acc: 0.9220, val loss: 0.2574, val acc: 0.9140  (best train acc: 0.9323, best val acc: 0.9514, best train loss: 0.1890  @ epoch 6471 )\n",
      "[Epoch: 6740] train loss: 0.2013, train acc: 0.9217, val loss: 0.1979, val acc: 0.9464  (best train acc: 0.9323, best val acc: 0.9514, best train loss: 0.1890  @ epoch 6471 )\n",
      "[Epoch: 6760] train loss: 0.2072, train acc: 0.9217, val loss: 0.2144, val acc: 0.9305  (best train acc: 0.9323, best val acc: 0.9514, best train loss: 0.1890  @ epoch 6471 )\n",
      "[Epoch: 6780] train loss: 0.2286, train acc: 0.9097, val loss: 0.2274, val acc: 0.9251  (best train acc: 0.9323, best val acc: 0.9514, best train loss: 0.1890  @ epoch 6471 )\n",
      "[Epoch: 6800] train loss: 0.1999, train acc: 0.9246, val loss: 0.2476, val acc: 0.9150  (best train acc: 0.9323, best val acc: 0.9514, best train loss: 0.1890  @ epoch 6471 )\n",
      "[Epoch: 6820] train loss: 0.2272, train acc: 0.9186, val loss: 0.2016, val acc: 0.9430  (best train acc: 0.9323, best val acc: 0.9514, best train loss: 0.1890  @ epoch 6471 )\n",
      "[Epoch: 6840] train loss: 0.2127, train acc: 0.9252, val loss: 0.2179, val acc: 0.9393  (best train acc: 0.9323, best val acc: 0.9514, best train loss: 0.1890  @ epoch 6471 )\n",
      "[Epoch: 6860] train loss: 0.2032, train acc: 0.9303, val loss: 0.2242, val acc: 0.9325  (best train acc: 0.9323, best val acc: 0.9514, best train loss: 0.1890  @ epoch 6471 )\n",
      "[Epoch: 6880] train loss: 0.2254, train acc: 0.9182, val loss: 0.2195, val acc: 0.9457  (best train acc: 0.9323, best val acc: 0.9514, best train loss: 0.1890  @ epoch 6471 )\n",
      "[Epoch: 6900] train loss: 0.2266, train acc: 0.9169, val loss: 0.2054, val acc: 0.9440  (best train acc: 0.9323, best val acc: 0.9514, best train loss: 0.1890  @ epoch 6471 )\n",
      "[Epoch: 6920] train loss: 0.2008, train acc: 0.9262, val loss: 0.2240, val acc: 0.9315  (best train acc: 0.9323, best val acc: 0.9514, best train loss: 0.1890  @ epoch 6471 )\n",
      "[Epoch: 6940] train loss: 0.2178, train acc: 0.9179, val loss: 0.2461, val acc: 0.9177  (best train acc: 0.9341, best val acc: 0.9514, best train loss: 0.1841  @ epoch 6931 )\n",
      "[Epoch: 6960] train loss: 0.2009, train acc: 0.9250, val loss: 0.2109, val acc: 0.9390  (best train acc: 0.9341, best val acc: 0.9514, best train loss: 0.1841  @ epoch 6931 )\n",
      "[Epoch: 6980] train loss: 0.2125, train acc: 0.9173, val loss: 0.1979, val acc: 0.9390  (best train acc: 0.9341, best val acc: 0.9514, best train loss: 0.1841  @ epoch 6931 )\n",
      "[Epoch: 7000] train loss: 0.2378, train acc: 0.9117, val loss: 0.2284, val acc: 0.9319  (best train acc: 0.9341, best val acc: 0.9514, best train loss: 0.1841  @ epoch 6931 )\n",
      "[Epoch: 7020] train loss: 0.2088, train acc: 0.9239, val loss: 0.2294, val acc: 0.9211  (best train acc: 0.9341, best val acc: 0.9514, best train loss: 0.1841  @ epoch 6931 )\n",
      "[Epoch: 7040] train loss: 0.2169, train acc: 0.9232, val loss: 0.2303, val acc: 0.9241  (best train acc: 0.9341, best val acc: 0.9514, best train loss: 0.1841  @ epoch 6931 )\n",
      "[Epoch: 7060] train loss: 0.2254, train acc: 0.9215, val loss: 0.2232, val acc: 0.9352  (best train acc: 0.9341, best val acc: 0.9514, best train loss: 0.1841  @ epoch 6931 )\n",
      "[Epoch: 7080] train loss: 0.1961, train acc: 0.9325, val loss: 0.2206, val acc: 0.9319  (best train acc: 0.9341, best val acc: 0.9514, best train loss: 0.1841  @ epoch 6931 )\n",
      "[Epoch: 7100] train loss: 0.2328, train acc: 0.9098, val loss: 0.2285, val acc: 0.9278  (best train acc: 0.9368, best val acc: 0.9514, best train loss: 0.1841  @ epoch 6931 )\n",
      "[Epoch: 7120] train loss: 0.1812, train acc: 0.9383, val loss: 0.2062, val acc: 0.9410  (best train acc: 0.9383, best val acc: 0.9514, best train loss: 0.1812  @ epoch 7120 )\n",
      "[Epoch: 7140] train loss: 0.2097, train acc: 0.9234, val loss: 0.2039, val acc: 0.9437  (best train acc: 0.9383, best val acc: 0.9514, best train loss: 0.1812  @ epoch 7120 )\n",
      "[Epoch: 7160] train loss: 0.2059, train acc: 0.9295, val loss: 0.2220, val acc: 0.9288  (best train acc: 0.9383, best val acc: 0.9514, best train loss: 0.1812  @ epoch 7120 )\n",
      "[Epoch: 7180] train loss: 0.2211, train acc: 0.9146, val loss: 0.2192, val acc: 0.9312  (best train acc: 0.9383, best val acc: 0.9514, best train loss: 0.1812  @ epoch 7120 )\n",
      "[Epoch: 7200] train loss: 0.1873, train acc: 0.9331, val loss: 0.2050, val acc: 0.9467  (best train acc: 0.9383, best val acc: 0.9514, best train loss: 0.1812  @ epoch 7120 )\n",
      "[Epoch: 7220] train loss: 0.2029, train acc: 0.9228, val loss: 0.1943, val acc: 0.9440  (best train acc: 0.9383, best val acc: 0.9514, best train loss: 0.1812  @ epoch 7120 )\n",
      "[Epoch: 7240] train loss: 0.2083, train acc: 0.9220, val loss: 0.2285, val acc: 0.9336  (best train acc: 0.9383, best val acc: 0.9514, best train loss: 0.1812  @ epoch 7120 )\n",
      "[Epoch: 7260] train loss: 0.2126, train acc: 0.9271, val loss: 0.2421, val acc: 0.9157  (best train acc: 0.9383, best val acc: 0.9514, best train loss: 0.1812  @ epoch 7120 )\n",
      "[Epoch: 7280] train loss: 0.2097, train acc: 0.9235, val loss: 0.1980, val acc: 0.9491  (best train acc: 0.9383, best val acc: 0.9514, best train loss: 0.1812  @ epoch 7120 )\n",
      "[Epoch: 7300] train loss: 0.2132, train acc: 0.9210, val loss: 0.2064, val acc: 0.9450  (best train acc: 0.9383, best val acc: 0.9514, best train loss: 0.1812  @ epoch 7120 )\n",
      "[Epoch: 7320] train loss: 0.1938, train acc: 0.9310, val loss: 0.2314, val acc: 0.9231  (best train acc: 0.9383, best val acc: 0.9514, best train loss: 0.1812  @ epoch 7120 )\n",
      "[Epoch: 7340] train loss: 0.2038, train acc: 0.9253, val loss: 0.2481, val acc: 0.9160  (best train acc: 0.9383, best val acc: 0.9514, best train loss: 0.1812  @ epoch 7120 )\n",
      "[Epoch: 7360] train loss: 0.1944, train acc: 0.9316, val loss: 0.2069, val acc: 0.9390  (best train acc: 0.9383, best val acc: 0.9514, best train loss: 0.1812  @ epoch 7120 )\n",
      "[Epoch: 7380] train loss: 0.2193, train acc: 0.9264, val loss: 0.1990, val acc: 0.9450  (best train acc: 0.9383, best val acc: 0.9514, best train loss: 0.1812  @ epoch 7120 )\n",
      "[Epoch: 7400] train loss: 0.2422, train acc: 0.9001, val loss: 0.2100, val acc: 0.9339  (best train acc: 0.9383, best val acc: 0.9514, best train loss: 0.1809  @ epoch 7398 )\n",
      "[Epoch: 7420] train loss: 0.2086, train acc: 0.9256, val loss: 0.2309, val acc: 0.9278  (best train acc: 0.9383, best val acc: 0.9514, best train loss: 0.1809  @ epoch 7398 )\n",
      "[Epoch: 7440] train loss: 0.1989, train acc: 0.9300, val loss: 0.2075, val acc: 0.9420  (best train acc: 0.9383, best val acc: 0.9514, best train loss: 0.1809  @ epoch 7398 )\n",
      "[Epoch: 7460] train loss: 0.2263, train acc: 0.9164, val loss: 0.2247, val acc: 0.9339  (best train acc: 0.9383, best val acc: 0.9514, best train loss: 0.1809  @ epoch 7398 )\n",
      "[Epoch: 7480] train loss: 0.2324, train acc: 0.9082, val loss: 0.2586, val acc: 0.9120  (best train acc: 0.9383, best val acc: 0.9514, best train loss: 0.1809  @ epoch 7398 )\n",
      "[Epoch: 7500] train loss: 0.1880, train acc: 0.9333, val loss: 0.2089, val acc: 0.9376  (best train acc: 0.9383, best val acc: 0.9518, best train loss: 0.1809  @ epoch 7398 )\n",
      "[Epoch: 7520] train loss: 0.2098, train acc: 0.9251, val loss: 0.2114, val acc: 0.9393  (best train acc: 0.9383, best val acc: 0.9518, best train loss: 0.1809  @ epoch 7398 )\n",
      "[Epoch: 7540] train loss: 0.1948, train acc: 0.9240, val loss: 0.2152, val acc: 0.9329  (best train acc: 0.9383, best val acc: 0.9518, best train loss: 0.1809  @ epoch 7398 )\n",
      "[Epoch: 7560] train loss: 0.2221, train acc: 0.9192, val loss: 0.2208, val acc: 0.9282  (best train acc: 0.9383, best val acc: 0.9518, best train loss: 0.1809  @ epoch 7398 )\n",
      "[Epoch: 7580] train loss: 0.2075, train acc: 0.9229, val loss: 0.2471, val acc: 0.9207  (best train acc: 0.9383, best val acc: 0.9518, best train loss: 0.1809  @ epoch 7398 )\n",
      "[Epoch: 7600] train loss: 0.1791, train acc: 0.9360, val loss: 0.2391, val acc: 0.9187  (best train acc: 0.9383, best val acc: 0.9518, best train loss: 0.1791  @ epoch 7600 )\n",
      "[Epoch: 7620] train loss: 0.1882, train acc: 0.9302, val loss: 0.2235, val acc: 0.9285  (best train acc: 0.9383, best val acc: 0.9518, best train loss: 0.1791  @ epoch 7600 )\n",
      "[Epoch: 7640] train loss: 0.1969, train acc: 0.9325, val loss: 0.2143, val acc: 0.9315  (best train acc: 0.9383, best val acc: 0.9518, best train loss: 0.1791  @ epoch 7600 )\n",
      "[Epoch: 7660] train loss: 0.2421, train acc: 0.9045, val loss: 0.1978, val acc: 0.9477  (best train acc: 0.9383, best val acc: 0.9518, best train loss: 0.1791  @ epoch 7600 )\n",
      "[Epoch: 7680] train loss: 0.2047, train acc: 0.9229, val loss: 0.2021, val acc: 0.9393  (best train acc: 0.9387, best val acc: 0.9518, best train loss: 0.1791  @ epoch 7600 )\n",
      "[Epoch: 7700] train loss: 0.1850, train acc: 0.9346, val loss: 0.1992, val acc: 0.9457  (best train acc: 0.9387, best val acc: 0.9518, best train loss: 0.1791  @ epoch 7600 )\n",
      "[Epoch: 7720] train loss: 0.2186, train acc: 0.9175, val loss: 0.2506, val acc: 0.9042  (best train acc: 0.9387, best val acc: 0.9518, best train loss: 0.1778  @ epoch 7718 )\n",
      "[Epoch: 7740] train loss: 0.2098, train acc: 0.9231, val loss: 0.2176, val acc: 0.9322  (best train acc: 0.9387, best val acc: 0.9528, best train loss: 0.1728  @ epoch 7729 )\n",
      "[Epoch: 7760] train loss: 0.2223, train acc: 0.9217, val loss: 0.2037, val acc: 0.9477  (best train acc: 0.9387, best val acc: 0.9528, best train loss: 0.1728  @ epoch 7729 )\n",
      "[Epoch: 7780] train loss: 0.1981, train acc: 0.9211, val loss: 0.1889, val acc: 0.9481  (best train acc: 0.9387, best val acc: 0.9528, best train loss: 0.1728  @ epoch 7729 )\n",
      "[Epoch: 7800] train loss: 0.2022, train acc: 0.9261, val loss: 0.2182, val acc: 0.9359  (best train acc: 0.9387, best val acc: 0.9528, best train loss: 0.1728  @ epoch 7729 )\n",
      "[Epoch: 7820] train loss: 0.2132, train acc: 0.9160, val loss: 0.2101, val acc: 0.9420  (best train acc: 0.9387, best val acc: 0.9528, best train loss: 0.1728  @ epoch 7729 )\n",
      "[Epoch: 7840] train loss: 0.2088, train acc: 0.9232, val loss: 0.1945, val acc: 0.9356  (best train acc: 0.9387, best val acc: 0.9528, best train loss: 0.1728  @ epoch 7729 )\n",
      "[Epoch: 7860] train loss: 0.1946, train acc: 0.9319, val loss: 0.2006, val acc: 0.9444  (best train acc: 0.9387, best val acc: 0.9528, best train loss: 0.1728  @ epoch 7729 )\n",
      "[Epoch: 7880] train loss: 0.1876, train acc: 0.9349, val loss: 0.2045, val acc: 0.9413  (best train acc: 0.9387, best val acc: 0.9528, best train loss: 0.1728  @ epoch 7729 )\n",
      "[Epoch: 7900] train loss: 0.1916, train acc: 0.9274, val loss: 0.1988, val acc: 0.9454  (best train acc: 0.9387, best val acc: 0.9528, best train loss: 0.1728  @ epoch 7729 )\n",
      "[Epoch: 7920] train loss: 0.1920, train acc: 0.9288, val loss: 0.2057, val acc: 0.9464  (best train acc: 0.9387, best val acc: 0.9528, best train loss: 0.1728  @ epoch 7729 )\n",
      "[Epoch: 7940] train loss: 0.2192, train acc: 0.9297, val loss: 0.1971, val acc: 0.9417  (best train acc: 0.9387, best val acc: 0.9528, best train loss: 0.1728  @ epoch 7729 )\n",
      "[Epoch: 7960] train loss: 0.2449, train acc: 0.9075, val loss: 0.2018, val acc: 0.9467  (best train acc: 0.9387, best val acc: 0.9528, best train loss: 0.1728  @ epoch 7729 )\n",
      "[Epoch: 7980] train loss: 0.2003, train acc: 0.9344, val loss: 0.2370, val acc: 0.9245  (best train acc: 0.9387, best val acc: 0.9528, best train loss: 0.1728  @ epoch 7729 )\n",
      "[Epoch: 8000] train loss: 0.2254, train acc: 0.9268, val loss: 0.2092, val acc: 0.9393  (best train acc: 0.9387, best val acc: 0.9528, best train loss: 0.1728  @ epoch 7729 )\n",
      "[Epoch: 8020] train loss: 0.1740, train acc: 0.9361, val loss: 0.1890, val acc: 0.9447  (best train acc: 0.9404, best val acc: 0.9531, best train loss: 0.1717  @ epoch 8001 )\n",
      "[Epoch: 8040] train loss: 0.2021, train acc: 0.9280, val loss: 0.2205, val acc: 0.9342  (best train acc: 0.9404, best val acc: 0.9531, best train loss: 0.1717  @ epoch 8001 )\n",
      "[Epoch: 8060] train loss: 0.2035, train acc: 0.9249, val loss: 0.2008, val acc: 0.9457  (best train acc: 0.9404, best val acc: 0.9531, best train loss: 0.1717  @ epoch 8001 )\n",
      "[Epoch: 8080] train loss: 0.2080, train acc: 0.9176, val loss: 0.1993, val acc: 0.9427  (best train acc: 0.9404, best val acc: 0.9531, best train loss: 0.1717  @ epoch 8001 )\n",
      "[Epoch: 8100] train loss: 0.1902, train acc: 0.9247, val loss: 0.1936, val acc: 0.9494  (best train acc: 0.9404, best val acc: 0.9531, best train loss: 0.1717  @ epoch 8001 )\n",
      "[Epoch: 8120] train loss: 0.1884, train acc: 0.9357, val loss: 0.2152, val acc: 0.9346  (best train acc: 0.9404, best val acc: 0.9531, best train loss: 0.1717  @ epoch 8001 )\n",
      "[Epoch: 8140] train loss: 0.1911, train acc: 0.9285, val loss: 0.2086, val acc: 0.9359  (best train acc: 0.9404, best val acc: 0.9531, best train loss: 0.1653  @ epoch 8128 )\n",
      "[Epoch: 8160] train loss: 0.1977, train acc: 0.9258, val loss: 0.1986, val acc: 0.9457  (best train acc: 0.9404, best val acc: 0.9531, best train loss: 0.1653  @ epoch 8128 )\n",
      "[Epoch: 8180] train loss: 0.2013, train acc: 0.9283, val loss: 0.1995, val acc: 0.9413  (best train acc: 0.9404, best val acc: 0.9531, best train loss: 0.1653  @ epoch 8128 )\n",
      "[Epoch: 8200] train loss: 0.1944, train acc: 0.9315, val loss: 0.2145, val acc: 0.9336  (best train acc: 0.9404, best val acc: 0.9531, best train loss: 0.1653  @ epoch 8128 )\n",
      "[Epoch: 8220] train loss: 0.1846, train acc: 0.9268, val loss: 0.1997, val acc: 0.9413  (best train acc: 0.9404, best val acc: 0.9531, best train loss: 0.1653  @ epoch 8128 )\n",
      "[Epoch: 8240] train loss: 0.1805, train acc: 0.9328, val loss: 0.1909, val acc: 0.9417  (best train acc: 0.9407, best val acc: 0.9531, best train loss: 0.1653  @ epoch 8128 )\n",
      "[Epoch: 8260] train loss: 0.1852, train acc: 0.9324, val loss: 0.1971, val acc: 0.9464  (best train acc: 0.9407, best val acc: 0.9531, best train loss: 0.1653  @ epoch 8128 )\n",
      "[Epoch: 8280] train loss: 0.2386, train acc: 0.9072, val loss: 0.2079, val acc: 0.9265  (best train acc: 0.9407, best val acc: 0.9531, best train loss: 0.1653  @ epoch 8128 )\n",
      "[Epoch: 8300] train loss: 0.1751, train acc: 0.9355, val loss: 0.1981, val acc: 0.9406  (best train acc: 0.9407, best val acc: 0.9535, best train loss: 0.1653  @ epoch 8128 )\n",
      "[Epoch: 8320] train loss: 0.1947, train acc: 0.9291, val loss: 0.2227, val acc: 0.9282  (best train acc: 0.9407, best val acc: 0.9535, best train loss: 0.1653  @ epoch 8128 )\n",
      "[Epoch: 8340] train loss: 0.2183, train acc: 0.9214, val loss: 0.2023, val acc: 0.9359  (best train acc: 0.9407, best val acc: 0.9535, best train loss: 0.1653  @ epoch 8128 )\n",
      "[Epoch: 8360] train loss: 0.1745, train acc: 0.9383, val loss: 0.1869, val acc: 0.9474  (best train acc: 0.9409, best val acc: 0.9535, best train loss: 0.1653  @ epoch 8128 )\n",
      "[Epoch: 8380] train loss: 0.1863, train acc: 0.9344, val loss: 0.2026, val acc: 0.9369  (best train acc: 0.9409, best val acc: 0.9545, best train loss: 0.1653  @ epoch 8128 )\n",
      "[Epoch: 8400] train loss: 0.2028, train acc: 0.9203, val loss: 0.2062, val acc: 0.9349  (best train acc: 0.9409, best val acc: 0.9545, best train loss: 0.1653  @ epoch 8128 )\n",
      "[Epoch: 8420] train loss: 0.1801, train acc: 0.9349, val loss: 0.2299, val acc: 0.9234  (best train acc: 0.9409, best val acc: 0.9545, best train loss: 0.1653  @ epoch 8128 )\n",
      "[Epoch: 8440] train loss: 0.2049, train acc: 0.9291, val loss: 0.1935, val acc: 0.9406  (best train acc: 0.9409, best val acc: 0.9545, best train loss: 0.1653  @ epoch 8128 )\n",
      "[Epoch: 8460] train loss: 0.1841, train acc: 0.9325, val loss: 0.1939, val acc: 0.9467  (best train acc: 0.9409, best val acc: 0.9545, best train loss: 0.1653  @ epoch 8128 )\n",
      "[Epoch: 8480] train loss: 0.1866, train acc: 0.9286, val loss: 0.1768, val acc: 0.9464  (best train acc: 0.9409, best val acc: 0.9545, best train loss: 0.1653  @ epoch 8128 )\n",
      "[Epoch: 8500] train loss: 0.1862, train acc: 0.9318, val loss: 0.2030, val acc: 0.9393  (best train acc: 0.9409, best val acc: 0.9545, best train loss: 0.1653  @ epoch 8128 )\n",
      "[Epoch: 8520] train loss: 0.1981, train acc: 0.9231, val loss: 0.2050, val acc: 0.9457  (best train acc: 0.9409, best val acc: 0.9545, best train loss: 0.1653  @ epoch 8128 )\n",
      "[Epoch: 8540] train loss: 0.1757, train acc: 0.9349, val loss: 0.1920, val acc: 0.9474  (best train acc: 0.9409, best val acc: 0.9545, best train loss: 0.1653  @ epoch 8128 )\n",
      "[Epoch: 8560] train loss: 0.1782, train acc: 0.9388, val loss: 0.2165, val acc: 0.9292  (best train acc: 0.9409, best val acc: 0.9545, best train loss: 0.1653  @ epoch 8128 )\n",
      "[Epoch: 8580] train loss: 0.1746, train acc: 0.9347, val loss: 0.2374, val acc: 0.9191  (best train acc: 0.9409, best val acc: 0.9545, best train loss: 0.1626  @ epoch 8565 )\n",
      "[Epoch: 8600] train loss: 0.2324, train acc: 0.9158, val loss: 0.1908, val acc: 0.9460  (best train acc: 0.9409, best val acc: 0.9545, best train loss: 0.1626  @ epoch 8565 )\n",
      "[Epoch: 8620] train loss: 0.1930, train acc: 0.9276, val loss: 0.2484, val acc: 0.9130  (best train acc: 0.9409, best val acc: 0.9545, best train loss: 0.1626  @ epoch 8565 )\n",
      "[Epoch: 8640] train loss: 0.2355, train acc: 0.9218, val loss: 0.2042, val acc: 0.9410  (best train acc: 0.9417, best val acc: 0.9545, best train loss: 0.1626  @ epoch 8565 )\n",
      "[Epoch: 8660] train loss: 0.1759, train acc: 0.9328, val loss: 0.2061, val acc: 0.9329  (best train acc: 0.9424, best val acc: 0.9545, best train loss: 0.1626  @ epoch 8565 )\n",
      "[Epoch: 8680] train loss: 0.1693, train acc: 0.9376, val loss: 0.1885, val acc: 0.9454  (best train acc: 0.9424, best val acc: 0.9545, best train loss: 0.1602  @ epoch 8671 )\n",
      "[Epoch: 8700] train loss: 0.1955, train acc: 0.9208, val loss: 0.2274, val acc: 0.9275  (best train acc: 0.9424, best val acc: 0.9545, best train loss: 0.1602  @ epoch 8671 )\n",
      "[Epoch: 8720] train loss: 0.1972, train acc: 0.9255, val loss: 0.2140, val acc: 0.9410  (best train acc: 0.9424, best val acc: 0.9545, best train loss: 0.1602  @ epoch 8671 )\n",
      "[Epoch: 8740] train loss: 0.1832, train acc: 0.9259, val loss: 0.1921, val acc: 0.9521  (best train acc: 0.9424, best val acc: 0.9545, best train loss: 0.1602  @ epoch 8671 )\n",
      "[Epoch: 8760] train loss: 0.2218, train acc: 0.9051, val loss: 0.1937, val acc: 0.9383  (best train acc: 0.9424, best val acc: 0.9545, best train loss: 0.1602  @ epoch 8671 )\n",
      "[Epoch: 8780] train loss: 0.2052, train acc: 0.9166, val loss: 0.1841, val acc: 0.9481  (best train acc: 0.9424, best val acc: 0.9545, best train loss: 0.1602  @ epoch 8671 )\n",
      "[Epoch: 8800] train loss: 0.2061, train acc: 0.9163, val loss: 0.2057, val acc: 0.9265  (best train acc: 0.9424, best val acc: 0.9545, best train loss: 0.1602  @ epoch 8671 )\n",
      "[Epoch: 8820] train loss: 0.1738, train acc: 0.9375, val loss: 0.1939, val acc: 0.9413  (best train acc: 0.9424, best val acc: 0.9545, best train loss: 0.1602  @ epoch 8671 )\n",
      "[Epoch: 8840] train loss: 0.1822, train acc: 0.9309, val loss: 0.2026, val acc: 0.9298  (best train acc: 0.9424, best val acc: 0.9545, best train loss: 0.1602  @ epoch 8671 )\n",
      "[Epoch: 8860] train loss: 0.2071, train acc: 0.9200, val loss: 0.2286, val acc: 0.9197  (best train acc: 0.9424, best val acc: 0.9545, best train loss: 0.1602  @ epoch 8671 )\n",
      "[Epoch: 8880] train loss: 0.1891, train acc: 0.9257, val loss: 0.1882, val acc: 0.9494  (best train acc: 0.9424, best val acc: 0.9545, best train loss: 0.1602  @ epoch 8671 )\n",
      "[Epoch: 8900] train loss: 0.1685, train acc: 0.9412, val loss: 0.1921, val acc: 0.9440  (best train acc: 0.9424, best val acc: 0.9545, best train loss: 0.1602  @ epoch 8671 )\n",
      "[Epoch: 8920] train loss: 0.1875, train acc: 0.9307, val loss: 0.1950, val acc: 0.9444  (best train acc: 0.9424, best val acc: 0.9545, best train loss: 0.1602  @ epoch 8671 )\n",
      "[Epoch: 8940] train loss: 0.1678, train acc: 0.9362, val loss: 0.1929, val acc: 0.9454  (best train acc: 0.9430, best val acc: 0.9545, best train loss: 0.1602  @ epoch 8671 )\n",
      "[Epoch: 8960] train loss: 0.1968, train acc: 0.9235, val loss: 0.1973, val acc: 0.9369  (best train acc: 0.9430, best val acc: 0.9545, best train loss: 0.1602  @ epoch 8671 )\n",
      "[Epoch: 8980] train loss: 0.1949, train acc: 0.9255, val loss: 0.1978, val acc: 0.9390  (best train acc: 0.9430, best val acc: 0.9545, best train loss: 0.1602  @ epoch 8671 )\n",
      "[Epoch: 9000] train loss: 0.1876, train acc: 0.9307, val loss: 0.2060, val acc: 0.9369  (best train acc: 0.9430, best val acc: 0.9545, best train loss: 0.1602  @ epoch 8671 )\n",
      "[Epoch: 9020] train loss: 0.2037, train acc: 0.9143, val loss: 0.1916, val acc: 0.9497  (best train acc: 0.9430, best val acc: 0.9545, best train loss: 0.1602  @ epoch 8671 )\n",
      "[Epoch: 9040] train loss: 0.1843, train acc: 0.9308, val loss: 0.2001, val acc: 0.9319  (best train acc: 0.9430, best val acc: 0.9545, best train loss: 0.1602  @ epoch 8671 )\n",
      "[Epoch: 9060] train loss: 0.1914, train acc: 0.9256, val loss: 0.1762, val acc: 0.9477  (best train acc: 0.9430, best val acc: 0.9545, best train loss: 0.1602  @ epoch 8671 )\n",
      "[Epoch: 9080] train loss: 0.2623, train acc: 0.8925, val loss: 0.1982, val acc: 0.9433  (best train acc: 0.9430, best val acc: 0.9545, best train loss: 0.1602  @ epoch 8671 )\n",
      "[Epoch: 9100] train loss: 0.1843, train acc: 0.9282, val loss: 0.2018, val acc: 0.9366  (best train acc: 0.9430, best val acc: 0.9545, best train loss: 0.1602  @ epoch 8671 )\n",
      "[Epoch: 9120] train loss: 0.1978, train acc: 0.9232, val loss: 0.2087, val acc: 0.9379  (best train acc: 0.9430, best val acc: 0.9545, best train loss: 0.1602  @ epoch 8671 )\n",
      "[Epoch: 9140] train loss: 0.1907, train acc: 0.9357, val loss: 0.2054, val acc: 0.9410  (best train acc: 0.9430, best val acc: 0.9545, best train loss: 0.1602  @ epoch 8671 )\n",
      "[Epoch: 9160] train loss: 0.1826, train acc: 0.9285, val loss: 0.1933, val acc: 0.9396  (best train acc: 0.9430, best val acc: 0.9545, best train loss: 0.1602  @ epoch 8671 )\n",
      "[Epoch: 9180] train loss: 0.1905, train acc: 0.9285, val loss: 0.2215, val acc: 0.9224  (best train acc: 0.9430, best val acc: 0.9545, best train loss: 0.1602  @ epoch 8671 )\n",
      "[Epoch: 9200] train loss: 0.1924, train acc: 0.9265, val loss: 0.1941, val acc: 0.9450  (best train acc: 0.9430, best val acc: 0.9545, best train loss: 0.1602  @ epoch 8671 )\n",
      "[Epoch: 9220] train loss: 0.1945, train acc: 0.9241, val loss: 0.2050, val acc: 0.9454  (best train acc: 0.9430, best val acc: 0.9545, best train loss: 0.1602  @ epoch 8671 )\n",
      "[Epoch: 9240] train loss: 0.1659, train acc: 0.9391, val loss: 0.1924, val acc: 0.9420  (best train acc: 0.9430, best val acc: 0.9545, best train loss: 0.1602  @ epoch 8671 )\n",
      "[Epoch: 9260] train loss: 0.1719, train acc: 0.9369, val loss: 0.1845, val acc: 0.9447  (best train acc: 0.9430, best val acc: 0.9545, best train loss: 0.1566  @ epoch 9242 )\n",
      "[Epoch: 9280] train loss: 0.1731, train acc: 0.9341, val loss: 0.2040, val acc: 0.9312  (best train acc: 0.9439, best val acc: 0.9545, best train loss: 0.1566  @ epoch 9242 )\n",
      "[Epoch: 9300] train loss: 0.1790, train acc: 0.9344, val loss: 0.1807, val acc: 0.9494  (best train acc: 0.9439, best val acc: 0.9545, best train loss: 0.1566  @ epoch 9242 )\n",
      "[Epoch: 9320] train loss: 0.1868, train acc: 0.9279, val loss: 0.2234, val acc: 0.9248  (best train acc: 0.9439, best val acc: 0.9545, best train loss: 0.1566  @ epoch 9242 )\n",
      "[Epoch: 9340] train loss: 0.1895, train acc: 0.9289, val loss: 0.1933, val acc: 0.9406  (best train acc: 0.9439, best val acc: 0.9545, best train loss: 0.1566  @ epoch 9242 )\n",
      "[Epoch: 9360] train loss: 0.1833, train acc: 0.9255, val loss: 0.1712, val acc: 0.9514  (best train acc: 0.9439, best val acc: 0.9545, best train loss: 0.1566  @ epoch 9242 )\n",
      "[Epoch: 9380] train loss: 0.2004, train acc: 0.9229, val loss: 0.2256, val acc: 0.9207  (best train acc: 0.9439, best val acc: 0.9545, best train loss: 0.1566  @ epoch 9242 )\n",
      "[Epoch: 9400] train loss: 0.2017, train acc: 0.9241, val loss: 0.2046, val acc: 0.9359  (best train acc: 0.9439, best val acc: 0.9545, best train loss: 0.1566  @ epoch 9242 )\n",
      "[Epoch: 9420] train loss: 0.2143, train acc: 0.9166, val loss: 0.1841, val acc: 0.9474  (best train acc: 0.9439, best val acc: 0.9545, best train loss: 0.1566  @ epoch 9242 )\n",
      "[Epoch: 9440] train loss: 0.1789, train acc: 0.9359, val loss: 0.1865, val acc: 0.9413  (best train acc: 0.9439, best val acc: 0.9545, best train loss: 0.1566  @ epoch 9242 )\n",
      "[Epoch: 9460] train loss: 0.2051, train acc: 0.9239, val loss: 0.1963, val acc: 0.9464  (best train acc: 0.9439, best val acc: 0.9545, best train loss: 0.1566  @ epoch 9242 )\n",
      "[Epoch: 9480] train loss: 0.1877, train acc: 0.9311, val loss: 0.1841, val acc: 0.9474  (best train acc: 0.9439, best val acc: 0.9545, best train loss: 0.1566  @ epoch 9242 )\n",
      "[Epoch: 9500] train loss: 0.1849, train acc: 0.9289, val loss: 0.1932, val acc: 0.9481  (best train acc: 0.9439, best val acc: 0.9545, best train loss: 0.1566  @ epoch 9242 )\n",
      "[Epoch: 9520] train loss: 0.2210, train acc: 0.9216, val loss: 0.2016, val acc: 0.9359  (best train acc: 0.9439, best val acc: 0.9545, best train loss: 0.1566  @ epoch 9242 )\n",
      "[Epoch: 9540] train loss: 0.1755, train acc: 0.9325, val loss: 0.1856, val acc: 0.9396  (best train acc: 0.9439, best val acc: 0.9545, best train loss: 0.1566  @ epoch 9242 )\n",
      "[Epoch: 9560] train loss: 0.1627, train acc: 0.9383, val loss: 0.1946, val acc: 0.9383  (best train acc: 0.9439, best val acc: 0.9545, best train loss: 0.1562  @ epoch 9555 )\n",
      "[Epoch: 9580] train loss: 0.1974, train acc: 0.9313, val loss: 0.1898, val acc: 0.9423  (best train acc: 0.9439, best val acc: 0.9545, best train loss: 0.1562  @ epoch 9555 )\n",
      "[Epoch: 9600] train loss: 0.2579, train acc: 0.8955, val loss: 0.1814, val acc: 0.9470  (best train acc: 0.9439, best val acc: 0.9545, best train loss: 0.1562  @ epoch 9555 )\n",
      "[Epoch: 9620] train loss: 0.2187, train acc: 0.9216, val loss: 0.2166, val acc: 0.9312  (best train acc: 0.9439, best val acc: 0.9545, best train loss: 0.1562  @ epoch 9555 )\n",
      "[Epoch: 9640] train loss: 0.1670, train acc: 0.9367, val loss: 0.1982, val acc: 0.9403  (best train acc: 0.9439, best val acc: 0.9545, best train loss: 0.1562  @ epoch 9555 )\n",
      "[Epoch: 9660] train loss: 0.1723, train acc: 0.9347, val loss: 0.2397, val acc: 0.9147  (best train acc: 0.9439, best val acc: 0.9545, best train loss: 0.1562  @ epoch 9555 )\n",
      "[Epoch: 9680] train loss: 0.1647, train acc: 0.9394, val loss: 0.1806, val acc: 0.9467  (best train acc: 0.9439, best val acc: 0.9545, best train loss: 0.1562  @ epoch 9555 )\n",
      "[Epoch: 9700] train loss: 0.1782, train acc: 0.9387, val loss: 0.1840, val acc: 0.9477  (best train acc: 0.9439, best val acc: 0.9545, best train loss: 0.1562  @ epoch 9555 )\n",
      "[Epoch: 9720] train loss: 0.1592, train acc: 0.9412, val loss: 0.2359, val acc: 0.9164  (best train acc: 0.9439, best val acc: 0.9545, best train loss: 0.1562  @ epoch 9555 )\n",
      "[Epoch: 9740] train loss: 0.1788, train acc: 0.9396, val loss: 0.1913, val acc: 0.9390  (best train acc: 0.9439, best val acc: 0.9545, best train loss: 0.1549  @ epoch 9725 )\n",
      "[Epoch: 9760] train loss: 0.1678, train acc: 0.9373, val loss: 0.1992, val acc: 0.9325  (best train acc: 0.9450, best val acc: 0.9545, best train loss: 0.1537  @ epoch 9759 )\n",
      "[Epoch: 9780] train loss: 0.1756, train acc: 0.9351, val loss: 0.1774, val acc: 0.9491  (best train acc: 0.9450, best val acc: 0.9545, best train loss: 0.1537  @ epoch 9759 )\n",
      "[Epoch: 9800] train loss: 0.1622, train acc: 0.9430, val loss: 0.2285, val acc: 0.9180  (best train acc: 0.9450, best val acc: 0.9545, best train loss: 0.1537  @ epoch 9759 )\n",
      "[Epoch: 9820] train loss: 0.1775, train acc: 0.9391, val loss: 0.1897, val acc: 0.9437  (best train acc: 0.9450, best val acc: 0.9545, best train loss: 0.1537  @ epoch 9759 )\n",
      "[Epoch: 9840] train loss: 0.1814, train acc: 0.9317, val loss: 0.1962, val acc: 0.9406  (best train acc: 0.9450, best val acc: 0.9545, best train loss: 0.1537  @ epoch 9759 )\n",
      "[Epoch: 9860] train loss: 0.1925, train acc: 0.9268, val loss: 0.2099, val acc: 0.9349  (best train acc: 0.9450, best val acc: 0.9545, best train loss: 0.1537  @ epoch 9759 )\n",
      "[Epoch: 9880] train loss: 0.1617, train acc: 0.9408, val loss: 0.2027, val acc: 0.9359  (best train acc: 0.9450, best val acc: 0.9545, best train loss: 0.1537  @ epoch 9759 )\n",
      "[Epoch: 9900] train loss: 0.1786, train acc: 0.9362, val loss: 0.1905, val acc: 0.9420  (best train acc: 0.9450, best val acc: 0.9545, best train loss: 0.1537  @ epoch 9759 )\n",
      "[Epoch: 9920] train loss: 0.1757, train acc: 0.9312, val loss: 0.2013, val acc: 0.9373  (best train acc: 0.9450, best val acc: 0.9551, best train loss: 0.1537  @ epoch 9759 )\n",
      "[Epoch: 9940] train loss: 0.2056, train acc: 0.9138, val loss: 0.1838, val acc: 0.9440  (best train acc: 0.9450, best val acc: 0.9551, best train loss: 0.1537  @ epoch 9759 )\n",
      "[Epoch: 9960] train loss: 0.1927, train acc: 0.9231, val loss: 0.1678, val acc: 0.9491  (best train acc: 0.9450, best val acc: 0.9551, best train loss: 0.1537  @ epoch 9759 )\n",
      "[Epoch: 9980] train loss: 0.2149, train acc: 0.9216, val loss: 0.1851, val acc: 0.9440  (best train acc: 0.9450, best val acc: 0.9551, best train loss: 0.1537  @ epoch 9759 )\n",
      "[Epoch: 10000] train loss: 0.1939, train acc: 0.9242, val loss: 0.2099, val acc: 0.9305  (best train acc: 0.9450, best val acc: 0.9551, best train loss: 0.1537  @ epoch 9759 )\n",
      "[Epoch: 10020] train loss: 0.1917, train acc: 0.9305, val loss: 0.1893, val acc: 0.9376  (best train acc: 0.9450, best val acc: 0.9551, best train loss: 0.1537  @ epoch 9759 )\n",
      "[Epoch: 10040] train loss: 0.1914, train acc: 0.9264, val loss: 0.2137, val acc: 0.9359  (best train acc: 0.9450, best val acc: 0.9551, best train loss: 0.1537  @ epoch 9759 )\n",
      "[Epoch: 10060] train loss: 0.1868, train acc: 0.9307, val loss: 0.1851, val acc: 0.9460  (best train acc: 0.9450, best val acc: 0.9551, best train loss: 0.1537  @ epoch 9759 )\n",
      "[Epoch: 10080] train loss: 0.1667, train acc: 0.9390, val loss: 0.1953, val acc: 0.9373  (best train acc: 0.9450, best val acc: 0.9551, best train loss: 0.1537  @ epoch 9759 )\n",
      "[Epoch: 10100] train loss: 0.2207, train acc: 0.9175, val loss: 0.1900, val acc: 0.9433  (best train acc: 0.9450, best val acc: 0.9551, best train loss: 0.1537  @ epoch 9759 )\n",
      "[Epoch: 10120] train loss: 0.1600, train acc: 0.9408, val loss: 0.1956, val acc: 0.9373  (best train acc: 0.9450, best val acc: 0.9551, best train loss: 0.1537  @ epoch 9759 )\n",
      "[Epoch: 10140] train loss: 0.1976, train acc: 0.9231, val loss: 0.1953, val acc: 0.9390  (best train acc: 0.9450, best val acc: 0.9551, best train loss: 0.1537  @ epoch 9759 )\n",
      "[Epoch: 10160] train loss: 0.1705, train acc: 0.9348, val loss: 0.1717, val acc: 0.9541  (best train acc: 0.9450, best val acc: 0.9551, best train loss: 0.1537  @ epoch 9759 )\n",
      "[Epoch: 10180] train loss: 0.1775, train acc: 0.9299, val loss: 0.1900, val acc: 0.9349  (best train acc: 0.9450, best val acc: 0.9551, best train loss: 0.1537  @ epoch 9759 )\n",
      "[Epoch: 10200] train loss: 0.1790, train acc: 0.9259, val loss: 0.1856, val acc: 0.9417  (best train acc: 0.9450, best val acc: 0.9551, best train loss: 0.1537  @ epoch 9759 )\n",
      "[Epoch: 10220] train loss: 0.1575, train acc: 0.9404, val loss: 0.1700, val acc: 0.9484  (best train acc: 0.9450, best val acc: 0.9551, best train loss: 0.1537  @ epoch 9759 )\n",
      "[Epoch: 10240] train loss: 0.1806, train acc: 0.9323, val loss: 0.1900, val acc: 0.9474  (best train acc: 0.9450, best val acc: 0.9551, best train loss: 0.1537  @ epoch 9759 )\n",
      "[Epoch: 10260] train loss: 0.1916, train acc: 0.9205, val loss: 0.1993, val acc: 0.9383  (best train acc: 0.9450, best val acc: 0.9551, best train loss: 0.1532  @ epoch 10242 )\n",
      "[Epoch: 10280] train loss: 0.1792, train acc: 0.9391, val loss: 0.1993, val acc: 0.9420  (best train acc: 0.9450, best val acc: 0.9551, best train loss: 0.1532  @ epoch 10242 )\n",
      "[Epoch: 10300] train loss: 0.1675, train acc: 0.9370, val loss: 0.2044, val acc: 0.9278  (best train acc: 0.9450, best val acc: 0.9551, best train loss: 0.1532  @ epoch 10242 )\n",
      "[Epoch: 10320] train loss: 0.1775, train acc: 0.9317, val loss: 0.1915, val acc: 0.9369  (best train acc: 0.9450, best val acc: 0.9551, best train loss: 0.1532  @ epoch 10242 )\n",
      "[Epoch: 10340] train loss: 0.1869, train acc: 0.9279, val loss: 0.2394, val acc: 0.9130  (best train acc: 0.9450, best val acc: 0.9582, best train loss: 0.1532  @ epoch 10242 )\n",
      "[Epoch: 10360] train loss: 0.1812, train acc: 0.9370, val loss: 0.2082, val acc: 0.9373  (best train acc: 0.9450, best val acc: 0.9582, best train loss: 0.1532  @ epoch 10242 )\n",
      "[Epoch: 10380] train loss: 0.1633, train acc: 0.9359, val loss: 0.1905, val acc: 0.9359  (best train acc: 0.9450, best val acc: 0.9582, best train loss: 0.1532  @ epoch 10242 )\n",
      "[Epoch: 10400] train loss: 0.1702, train acc: 0.9370, val loss: 0.1880, val acc: 0.9477  (best train acc: 0.9450, best val acc: 0.9582, best train loss: 0.1532  @ epoch 10242 )\n",
      "[Epoch: 10420] train loss: 0.1859, train acc: 0.9336, val loss: 0.1845, val acc: 0.9474  (best train acc: 0.9450, best val acc: 0.9582, best train loss: 0.1532  @ epoch 10242 )\n",
      "[Epoch: 10440] train loss: 0.1740, train acc: 0.9317, val loss: 0.1781, val acc: 0.9457  (best train acc: 0.9450, best val acc: 0.9582, best train loss: 0.1532  @ epoch 10242 )\n",
      "[Epoch: 10460] train loss: 0.1661, train acc: 0.9391, val loss: 0.1750, val acc: 0.9444  (best train acc: 0.9450, best val acc: 0.9582, best train loss: 0.1532  @ epoch 10242 )\n",
      "[Epoch: 10480] train loss: 0.1896, train acc: 0.9246, val loss: 0.1884, val acc: 0.9484  (best train acc: 0.9450, best val acc: 0.9582, best train loss: 0.1532  @ epoch 10242 )\n",
      "[Epoch: 10500] train loss: 0.1831, train acc: 0.9277, val loss: 0.1859, val acc: 0.9481  (best train acc: 0.9450, best val acc: 0.9582, best train loss: 0.1532  @ epoch 10242 )\n",
      "[Epoch: 10520] train loss: 0.1807, train acc: 0.9315, val loss: 0.1923, val acc: 0.9369  (best train acc: 0.9450, best val acc: 0.9582, best train loss: 0.1532  @ epoch 10242 )\n",
      "[Epoch: 10540] train loss: 0.1756, train acc: 0.9314, val loss: 0.1950, val acc: 0.9430  (best train acc: 0.9450, best val acc: 0.9582, best train loss: 0.1532  @ epoch 10242 )\n",
      "[Epoch: 10560] train loss: 0.1685, train acc: 0.9388, val loss: 0.2164, val acc: 0.9248  (best train acc: 0.9451, best val acc: 0.9582, best train loss: 0.1492  @ epoch 10557 )\n",
      "[Epoch: 10580] train loss: 0.1659, train acc: 0.9422, val loss: 0.2182, val acc: 0.9312  (best train acc: 0.9451, best val acc: 0.9582, best train loss: 0.1492  @ epoch 10557 )\n",
      "[Epoch: 10600] train loss: 0.1940, train acc: 0.9258, val loss: 0.2044, val acc: 0.9406  (best train acc: 0.9451, best val acc: 0.9582, best train loss: 0.1492  @ epoch 10557 )\n",
      "[Epoch: 10620] train loss: 0.1644, train acc: 0.9397, val loss: 0.2091, val acc: 0.9332  (best train acc: 0.9451, best val acc: 0.9582, best train loss: 0.1492  @ epoch 10557 )\n",
      "[Epoch: 10640] train loss: 0.1862, train acc: 0.9285, val loss: 0.1808, val acc: 0.9514  (best train acc: 0.9451, best val acc: 0.9582, best train loss: 0.1492  @ epoch 10557 )\n",
      "[Epoch: 10660] train loss: 0.1668, train acc: 0.9379, val loss: 0.1780, val acc: 0.9447  (best train acc: 0.9451, best val acc: 0.9582, best train loss: 0.1492  @ epoch 10557 )\n",
      "[Epoch: 10680] train loss: 0.2006, train acc: 0.9194, val loss: 0.1792, val acc: 0.9514  (best train acc: 0.9457, best val acc: 0.9582, best train loss: 0.1492  @ epoch 10557 )\n",
      "[Epoch: 10700] train loss: 0.1756, train acc: 0.9281, val loss: 0.2191, val acc: 0.9224  (best train acc: 0.9457, best val acc: 0.9582, best train loss: 0.1492  @ epoch 10557 )\n",
      "[Epoch: 10720] train loss: 0.1677, train acc: 0.9365, val loss: 0.1828, val acc: 0.9450  (best train acc: 0.9457, best val acc: 0.9582, best train loss: 0.1492  @ epoch 10557 )\n",
      "[Epoch: 10740] train loss: 0.1674, train acc: 0.9341, val loss: 0.1808, val acc: 0.9437  (best train acc: 0.9457, best val acc: 0.9582, best train loss: 0.1492  @ epoch 10557 )\n",
      "[Epoch: 10760] train loss: 0.1930, train acc: 0.9302, val loss: 0.1875, val acc: 0.9454  (best train acc: 0.9457, best val acc: 0.9582, best train loss: 0.1492  @ epoch 10557 )\n",
      "[Epoch: 10780] train loss: 0.1642, train acc: 0.9357, val loss: 0.1740, val acc: 0.9521  (best train acc: 0.9457, best val acc: 0.9582, best train loss: 0.1492  @ epoch 10557 )\n",
      "[Epoch: 10800] train loss: 0.1635, train acc: 0.9388, val loss: 0.1678, val acc: 0.9501  (best train acc: 0.9457, best val acc: 0.9582, best train loss: 0.1492  @ epoch 10557 )\n",
      "[Epoch: 10820] train loss: 0.2083, train acc: 0.9227, val loss: 0.1909, val acc: 0.9450  (best train acc: 0.9457, best val acc: 0.9582, best train loss: 0.1492  @ epoch 10557 )\n",
      "[Epoch: 10840] train loss: 0.1678, train acc: 0.9365, val loss: 0.1865, val acc: 0.9413  (best train acc: 0.9457, best val acc: 0.9582, best train loss: 0.1492  @ epoch 10557 )\n",
      "[Epoch: 10860] train loss: 0.1659, train acc: 0.9344, val loss: 0.2067, val acc: 0.9258  (best train acc: 0.9457, best val acc: 0.9582, best train loss: 0.1492  @ epoch 10557 )\n",
      "[Epoch: 10880] train loss: 0.1631, train acc: 0.9392, val loss: 0.1834, val acc: 0.9376  (best train acc: 0.9466, best val acc: 0.9582, best train loss: 0.1492  @ epoch 10557 )\n",
      "[Epoch: 10900] train loss: 0.1635, train acc: 0.9367, val loss: 0.1952, val acc: 0.9302  (best train acc: 0.9466, best val acc: 0.9582, best train loss: 0.1492  @ epoch 10557 )\n",
      "[Epoch: 10920] train loss: 0.1597, train acc: 0.9427, val loss: 0.1989, val acc: 0.9295  (best train acc: 0.9466, best val acc: 0.9582, best train loss: 0.1492  @ epoch 10557 )\n",
      "[Epoch: 10940] train loss: 0.1976, train acc: 0.9231, val loss: 0.1688, val acc: 0.9444  (best train acc: 0.9466, best val acc: 0.9582, best train loss: 0.1492  @ epoch 10557 )\n",
      "[Epoch: 10960] train loss: 0.1596, train acc: 0.9406, val loss: 0.1752, val acc: 0.9423  (best train acc: 0.9466, best val acc: 0.9582, best train loss: 0.1492  @ epoch 10557 )\n",
      "[Epoch: 10980] train loss: 0.1630, train acc: 0.9390, val loss: 0.1896, val acc: 0.9400  (best train acc: 0.9466, best val acc: 0.9582, best train loss: 0.1492  @ epoch 10557 )\n",
      "[Epoch: 11000] train loss: 0.1715, train acc: 0.9346, val loss: 0.1699, val acc: 0.9487  (best train acc: 0.9466, best val acc: 0.9582, best train loss: 0.1492  @ epoch 10557 )\n",
      "[Epoch: 11020] train loss: 0.1948, train acc: 0.9280, val loss: 0.2249, val acc: 0.9164  (best train acc: 0.9466, best val acc: 0.9582, best train loss: 0.1492  @ epoch 10557 )\n",
      "[Epoch: 11040] train loss: 0.1604, train acc: 0.9366, val loss: 0.1744, val acc: 0.9450  (best train acc: 0.9466, best val acc: 0.9582, best train loss: 0.1492  @ epoch 11031 )\n",
      "[Epoch: 11060] train loss: 0.1823, train acc: 0.9297, val loss: 0.1878, val acc: 0.9342  (best train acc: 0.9471, best val acc: 0.9582, best train loss: 0.1492  @ epoch 11031 )\n",
      "[Epoch: 11080] train loss: 0.1593, train acc: 0.9400, val loss: 0.1639, val acc: 0.9511  (best train acc: 0.9471, best val acc: 0.9582, best train loss: 0.1492  @ epoch 11031 )\n",
      "[Epoch: 11100] train loss: 0.1633, train acc: 0.9393, val loss: 0.1649, val acc: 0.9494  (best train acc: 0.9471, best val acc: 0.9582, best train loss: 0.1492  @ epoch 11031 )\n",
      "[Epoch: 11120] train loss: 0.1905, train acc: 0.9325, val loss: 0.1841, val acc: 0.9430  (best train acc: 0.9471, best val acc: 0.9582, best train loss: 0.1492  @ epoch 11031 )\n",
      "[Epoch: 11140] train loss: 0.1571, train acc: 0.9391, val loss: 0.1602, val acc: 0.9538  (best train acc: 0.9471, best val acc: 0.9582, best train loss: 0.1471  @ epoch 11137 )\n",
      "[Epoch: 11160] train loss: 0.1723, train acc: 0.9297, val loss: 0.1839, val acc: 0.9373  (best train acc: 0.9471, best val acc: 0.9582, best train loss: 0.1471  @ epoch 11137 )\n",
      "[Epoch: 11180] train loss: 0.1662, train acc: 0.9418, val loss: 0.1683, val acc: 0.9474  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1445  @ epoch 11166 )\n",
      "[Epoch: 11200] train loss: 0.1605, train acc: 0.9421, val loss: 0.1794, val acc: 0.9359  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1445  @ epoch 11166 )\n",
      "[Epoch: 11220] train loss: 0.1640, train acc: 0.9378, val loss: 0.1687, val acc: 0.9410  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1445  @ epoch 11166 )\n",
      "[Epoch: 11240] train loss: 0.1657, train acc: 0.9381, val loss: 0.1821, val acc: 0.9403  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1445  @ epoch 11166 )\n",
      "[Epoch: 11260] train loss: 0.1531, train acc: 0.9422, val loss: 0.1685, val acc: 0.9467  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1445  @ epoch 11166 )\n",
      "[Epoch: 11280] train loss: 0.2201, train acc: 0.9155, val loss: 0.1723, val acc: 0.9417  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1445  @ epoch 11166 )\n",
      "[Epoch: 11300] train loss: 0.1652, train acc: 0.9360, val loss: 0.2182, val acc: 0.9214  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1445  @ epoch 11166 )\n",
      "[Epoch: 11320] train loss: 0.1575, train acc: 0.9415, val loss: 0.1739, val acc: 0.9373  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1445  @ epoch 11166 )\n",
      "[Epoch: 11340] train loss: 0.1633, train acc: 0.9387, val loss: 0.1612, val acc: 0.9501  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1445  @ epoch 11166 )\n",
      "[Epoch: 11360] train loss: 0.1713, train acc: 0.9359, val loss: 0.1980, val acc: 0.9309  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1445  @ epoch 11166 )\n",
      "[Epoch: 11380] train loss: 0.1739, train acc: 0.9310, val loss: 0.1772, val acc: 0.9369  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1445  @ epoch 11166 )\n",
      "[Epoch: 11400] train loss: 0.1644, train acc: 0.9359, val loss: 0.2209, val acc: 0.9204  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1445  @ epoch 11166 )\n",
      "[Epoch: 11420] train loss: 0.1820, train acc: 0.9236, val loss: 0.1593, val acc: 0.9511  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1445  @ epoch 11166 )\n",
      "[Epoch: 11440] train loss: 0.1700, train acc: 0.9320, val loss: 0.1635, val acc: 0.9444  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1445  @ epoch 11166 )\n",
      "[Epoch: 11460] train loss: 0.1619, train acc: 0.9356, val loss: 0.1882, val acc: 0.9322  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1445  @ epoch 11166 )\n",
      "[Epoch: 11480] train loss: 0.1653, train acc: 0.9351, val loss: 0.1638, val acc: 0.9450  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1443  @ epoch 11462 )\n",
      "[Epoch: 11500] train loss: 0.1470, train acc: 0.9466, val loss: 0.1607, val acc: 0.9514  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1443  @ epoch 11462 )\n",
      "[Epoch: 11520] train loss: 0.1568, train acc: 0.9456, val loss: 0.1806, val acc: 0.9346  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1443  @ epoch 11462 )\n",
      "[Epoch: 11540] train loss: 0.1597, train acc: 0.9409, val loss: 0.1625, val acc: 0.9501  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1443  @ epoch 11462 )\n",
      "[Epoch: 11560] train loss: 0.1702, train acc: 0.9354, val loss: 0.1572, val acc: 0.9538  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1443  @ epoch 11462 )\n",
      "[Epoch: 11580] train loss: 0.1583, train acc: 0.9391, val loss: 0.1795, val acc: 0.9356  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1443  @ epoch 11462 )\n",
      "[Epoch: 11600] train loss: 0.1562, train acc: 0.9391, val loss: 0.1568, val acc: 0.9437  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1443  @ epoch 11462 )\n",
      "[Epoch: 11620] train loss: 0.1596, train acc: 0.9378, val loss: 0.1532, val acc: 0.9487  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1443  @ epoch 11462 )\n",
      "[Epoch: 11640] train loss: 0.1884, train acc: 0.9272, val loss: 0.1853, val acc: 0.9336  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1443  @ epoch 11462 )\n",
      "[Epoch: 11660] train loss: 0.1745, train acc: 0.9367, val loss: 0.1864, val acc: 0.9430  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1443  @ epoch 11462 )\n",
      "[Epoch: 11680] train loss: 0.1807, train acc: 0.9319, val loss: 0.1607, val acc: 0.9484  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1443  @ epoch 11462 )\n",
      "[Epoch: 11700] train loss: 0.1513, train acc: 0.9430, val loss: 0.1652, val acc: 0.9470  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1443  @ epoch 11462 )\n",
      "[Epoch: 11720] train loss: 0.2555, train acc: 0.9100, val loss: 0.2043, val acc: 0.9194  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1443  @ epoch 11462 )\n",
      "[Epoch: 11740] train loss: 0.1833, train acc: 0.9251, val loss: 0.1692, val acc: 0.9460  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1443  @ epoch 11462 )\n",
      "[Epoch: 11760] train loss: 0.1635, train acc: 0.9401, val loss: 0.1645, val acc: 0.9508  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1443  @ epoch 11462 )\n",
      "[Epoch: 11780] train loss: 0.1473, train acc: 0.9450, val loss: 0.1657, val acc: 0.9457  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1443  @ epoch 11462 )\n",
      "[Epoch: 11800] train loss: 0.1629, train acc: 0.9375, val loss: 0.1642, val acc: 0.9467  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1443  @ epoch 11462 )\n",
      "[Epoch: 11820] train loss: 0.1544, train acc: 0.9378, val loss: 0.1836, val acc: 0.9373  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1443  @ epoch 11462 )\n",
      "[Epoch: 11840] train loss: 0.1631, train acc: 0.9408, val loss: 0.1793, val acc: 0.9376  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1443  @ epoch 11462 )\n",
      "[Epoch: 11860] train loss: 0.1545, train acc: 0.9374, val loss: 0.1512, val acc: 0.9518  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1443  @ epoch 11462 )\n",
      "[Epoch: 11880] train loss: 0.1704, train acc: 0.9330, val loss: 0.2123, val acc: 0.9174  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1443  @ epoch 11462 )\n",
      "[Epoch: 11900] train loss: 0.1751, train acc: 0.9326, val loss: 0.2131, val acc: 0.9133  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1443  @ epoch 11462 )\n",
      "[Epoch: 11920] train loss: 0.1516, train acc: 0.9445, val loss: 0.1894, val acc: 0.9268  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1443  @ epoch 11462 )\n",
      "[Epoch: 11940] train loss: 0.1584, train acc: 0.9394, val loss: 0.1604, val acc: 0.9511  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1443  @ epoch 11462 )\n",
      "[Epoch: 11960] train loss: 0.1555, train acc: 0.9348, val loss: 0.1602, val acc: 0.9511  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1443  @ epoch 11462 )\n",
      "[Epoch: 11980] train loss: 0.1757, train acc: 0.9334, val loss: 0.1587, val acc: 0.9518  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1443  @ epoch 11462 )\n",
      "[Epoch: 12000] train loss: 0.1753, train acc: 0.9385, val loss: 0.1783, val acc: 0.9319  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1443  @ epoch 11462 )\n",
      "[Epoch: 12020] train loss: 0.1623, train acc: 0.9380, val loss: 0.1566, val acc: 0.9450  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1443  @ epoch 11462 )\n",
      "[Epoch: 12040] train loss: 0.1543, train acc: 0.9401, val loss: 0.1798, val acc: 0.9305  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1443  @ epoch 11462 )\n",
      "[Epoch: 12060] train loss: 0.1614, train acc: 0.9420, val loss: 0.1881, val acc: 0.9322  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1443  @ epoch 11462 )\n",
      "[Epoch: 12080] train loss: 0.1516, train acc: 0.9432, val loss: 0.1538, val acc: 0.9494  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1415  @ epoch 12069 )\n",
      "[Epoch: 12100] train loss: 0.1666, train acc: 0.9389, val loss: 0.1683, val acc: 0.9379  (best train acc: 0.9494, best val acc: 0.9582, best train loss: 0.1415  @ epoch 12069 )\n",
      "[Epoch: 12120] train loss: 0.1917, train acc: 0.9277, val loss: 0.1808, val acc: 0.9359  (best train acc: 0.9508, best val acc: 0.9582, best train loss: 0.1415  @ epoch 12069 )\n",
      "[Epoch: 12140] train loss: 0.1842, train acc: 0.9267, val loss: 0.1668, val acc: 0.9474  (best train acc: 0.9508, best val acc: 0.9582, best train loss: 0.1415  @ epoch 12069 )\n",
      "[Epoch: 12160] train loss: 0.1556, train acc: 0.9428, val loss: 0.1706, val acc: 0.9363  (best train acc: 0.9508, best val acc: 0.9582, best train loss: 0.1415  @ epoch 12069 )\n",
      "[Epoch: 12180] train loss: 0.1813, train acc: 0.9258, val loss: 0.1640, val acc: 0.9440  (best train acc: 0.9508, best val acc: 0.9582, best train loss: 0.1415  @ epoch 12069 )\n",
      "[Epoch: 12200] train loss: 0.1647, train acc: 0.9329, val loss: 0.1638, val acc: 0.9454  (best train acc: 0.9508, best val acc: 0.9582, best train loss: 0.1415  @ epoch 12069 )\n",
      "[Epoch: 12220] train loss: 0.1935, train acc: 0.9277, val loss: 0.2063, val acc: 0.9187  (best train acc: 0.9508, best val acc: 0.9582, best train loss: 0.1415  @ epoch 12069 )\n",
      "[Epoch: 12240] train loss: 0.1586, train acc: 0.9405, val loss: 0.1608, val acc: 0.9497  (best train acc: 0.9508, best val acc: 0.9582, best train loss: 0.1409  @ epoch 12238 )\n",
      "[Epoch: 12260] train loss: 0.1696, train acc: 0.9413, val loss: 0.1556, val acc: 0.9494  (best train acc: 0.9508, best val acc: 0.9582, best train loss: 0.1408  @ epoch 12243 )\n",
      "[Epoch: 12280] train loss: 0.1727, train acc: 0.9366, val loss: 0.1559, val acc: 0.9501  (best train acc: 0.9508, best val acc: 0.9582, best train loss: 0.1408  @ epoch 12243 )\n",
      "[Epoch: 12300] train loss: 0.1795, train acc: 0.9305, val loss: 0.1732, val acc: 0.9437  (best train acc: 0.9508, best val acc: 0.9582, best train loss: 0.1408  @ epoch 12243 )\n",
      "[Epoch: 12320] train loss: 0.1888, train acc: 0.9271, val loss: 0.1699, val acc: 0.9447  (best train acc: 0.9508, best val acc: 0.9582, best train loss: 0.1408  @ epoch 12243 )\n",
      "[Epoch: 12340] train loss: 0.1676, train acc: 0.9375, val loss: 0.1835, val acc: 0.9352  (best train acc: 0.9508, best val acc: 0.9582, best train loss: 0.1408  @ epoch 12243 )\n",
      "[Epoch: 12360] train loss: 0.1688, train acc: 0.9399, val loss: 0.1512, val acc: 0.9514  (best train acc: 0.9508, best val acc: 0.9582, best train loss: 0.1408  @ epoch 12243 )\n",
      "[Epoch: 12380] train loss: 0.1461, train acc: 0.9421, val loss: 0.1854, val acc: 0.9272  (best train acc: 0.9508, best val acc: 0.9582, best train loss: 0.1408  @ epoch 12243 )\n",
      "[Epoch: 12400] train loss: 0.1817, train acc: 0.9234, val loss: 0.1538, val acc: 0.9497  (best train acc: 0.9508, best val acc: 0.9582, best train loss: 0.1408  @ epoch 12243 )\n",
      "[Epoch: 12420] train loss: 0.1672, train acc: 0.9336, val loss: 0.1798, val acc: 0.9325  (best train acc: 0.9508, best val acc: 0.9582, best train loss: 0.1408  @ epoch 12243 )\n",
      "[Epoch: 12440] train loss: 0.1727, train acc: 0.9317, val loss: 0.1678, val acc: 0.9379  (best train acc: 0.9508, best val acc: 0.9582, best train loss: 0.1408  @ epoch 12243 )\n",
      "[Epoch: 12460] train loss: 0.1669, train acc: 0.9365, val loss: 0.1584, val acc: 0.9494  (best train acc: 0.9508, best val acc: 0.9582, best train loss: 0.1388  @ epoch 12453 )\n",
      "[Epoch: 12480] train loss: 0.1943, train acc: 0.9242, val loss: 0.1829, val acc: 0.9305  (best train acc: 0.9508, best val acc: 0.9582, best train loss: 0.1388  @ epoch 12453 )\n",
      "[Epoch: 12500] train loss: 0.1624, train acc: 0.9351, val loss: 0.1478, val acc: 0.9494  (best train acc: 0.9508, best val acc: 0.9582, best train loss: 0.1388  @ epoch 12453 )\n",
      "[Epoch: 12520] train loss: 0.1724, train acc: 0.9320, val loss: 0.1743, val acc: 0.9369  (best train acc: 0.9508, best val acc: 0.9582, best train loss: 0.1388  @ epoch 12453 )\n",
      "[Epoch: 12540] train loss: 0.1443, train acc: 0.9432, val loss: 0.1626, val acc: 0.9487  (best train acc: 0.9508, best val acc: 0.9582, best train loss: 0.1388  @ epoch 12453 )\n",
      "[Epoch: 12560] train loss: 0.2081, train acc: 0.9281, val loss: 0.1691, val acc: 0.9406  (best train acc: 0.9508, best val acc: 0.9582, best train loss: 0.1388  @ epoch 12453 )\n",
      "[Epoch: 12580] train loss: 0.1459, train acc: 0.9484, val loss: 0.1536, val acc: 0.9484  (best train acc: 0.9508, best val acc: 0.9589, best train loss: 0.1388  @ epoch 12453 )\n",
      "[Epoch: 12600] train loss: 0.1429, train acc: 0.9460, val loss: 0.1728, val acc: 0.9383  (best train acc: 0.9508, best val acc: 0.9589, best train loss: 0.1388  @ epoch 12453 )\n",
      "[Epoch: 12620] train loss: 0.1556, train acc: 0.9418, val loss: 0.1556, val acc: 0.9528  (best train acc: 0.9508, best val acc: 0.9589, best train loss: 0.1388  @ epoch 12453 )\n",
      "[Epoch: 12640] train loss: 0.1623, train acc: 0.9366, val loss: 0.1751, val acc: 0.9410  (best train acc: 0.9508, best val acc: 0.9589, best train loss: 0.1388  @ epoch 12453 )\n",
      "[Epoch: 12660] train loss: 0.1557, train acc: 0.9413, val loss: 0.2010, val acc: 0.9204  (best train acc: 0.9508, best val acc: 0.9589, best train loss: 0.1388  @ epoch 12453 )\n",
      "[Epoch: 12680] train loss: 0.1666, train acc: 0.9365, val loss: 0.1503, val acc: 0.9477  (best train acc: 0.9508, best val acc: 0.9589, best train loss: 0.1373  @ epoch 12674 )\n",
      "[Epoch: 12700] train loss: 0.1663, train acc: 0.9365, val loss: 0.1798, val acc: 0.9410  (best train acc: 0.9508, best val acc: 0.9589, best train loss: 0.1373  @ epoch 12674 )\n",
      "[Epoch: 12720] train loss: 0.1546, train acc: 0.9416, val loss: 0.1596, val acc: 0.9444  (best train acc: 0.9508, best val acc: 0.9589, best train loss: 0.1373  @ epoch 12674 )\n",
      "[Epoch: 12740] train loss: 0.1532, train acc: 0.9401, val loss: 0.1704, val acc: 0.9430  (best train acc: 0.9508, best val acc: 0.9589, best train loss: 0.1373  @ epoch 12674 )\n",
      "[Epoch: 12760] train loss: 0.1499, train acc: 0.9406, val loss: 0.1527, val acc: 0.9481  (best train acc: 0.9508, best val acc: 0.9589, best train loss: 0.1373  @ epoch 12674 )\n",
      "[Epoch: 12780] train loss: 0.1659, train acc: 0.9350, val loss: 0.1801, val acc: 0.9346  (best train acc: 0.9508, best val acc: 0.9589, best train loss: 0.1373  @ epoch 12674 )\n",
      "[Epoch: 12800] train loss: 0.1514, train acc: 0.9440, val loss: 0.1653, val acc: 0.9474  (best train acc: 0.9508, best val acc: 0.9589, best train loss: 0.1373  @ epoch 12674 )\n",
      "[Epoch: 12820] train loss: 0.2170, train acc: 0.9253, val loss: 0.1712, val acc: 0.9379  (best train acc: 0.9508, best val acc: 0.9589, best train loss: 0.1373  @ epoch 12674 )\n",
      "[Epoch: 12840] train loss: 0.1535, train acc: 0.9427, val loss: 0.1557, val acc: 0.9487  (best train acc: 0.9508, best val acc: 0.9589, best train loss: 0.1373  @ epoch 12674 )\n",
      "[Epoch: 12860] train loss: 0.1606, train acc: 0.9392, val loss: 0.1555, val acc: 0.9457  (best train acc: 0.9508, best val acc: 0.9589, best train loss: 0.1373  @ epoch 12674 )\n",
      "[Epoch: 12880] train loss: 0.1543, train acc: 0.9401, val loss: 0.1525, val acc: 0.9514  (best train acc: 0.9508, best val acc: 0.9589, best train loss: 0.1373  @ epoch 12674 )\n",
      "[Epoch: 12900] train loss: 0.1511, train acc: 0.9380, val loss: 0.1590, val acc: 0.9470  (best train acc: 0.9508, best val acc: 0.9589, best train loss: 0.1373  @ epoch 12674 )\n",
      "[Epoch: 12920] train loss: 0.1490, train acc: 0.9463, val loss: 0.1571, val acc: 0.9467  (best train acc: 0.9508, best val acc: 0.9589, best train loss: 0.1373  @ epoch 12674 )\n",
      "[Epoch: 12940] train loss: 0.1495, train acc: 0.9416, val loss: 0.1793, val acc: 0.9298  (best train acc: 0.9508, best val acc: 0.9589, best train loss: 0.1373  @ epoch 12674 )\n",
      "[Epoch: 12960] train loss: 0.1655, train acc: 0.9343, val loss: 0.1501, val acc: 0.9497  (best train acc: 0.9508, best val acc: 0.9589, best train loss: 0.1373  @ epoch 12674 )\n",
      "[Epoch: 12980] train loss: 0.1660, train acc: 0.9346, val loss: 0.1451, val acc: 0.9477  (best train acc: 0.9508, best val acc: 0.9589, best train loss: 0.1373  @ epoch 12674 )\n",
      "[Epoch: 13000] train loss: 0.1578, train acc: 0.9451, val loss: 0.1575, val acc: 0.9494  (best train acc: 0.9508, best val acc: 0.9589, best train loss: 0.1373  @ epoch 12674 )\n",
      "[Epoch: 13020] train loss: 0.1833, train acc: 0.9330, val loss: 0.1523, val acc: 0.9508  (best train acc: 0.9508, best val acc: 0.9589, best train loss: 0.1373  @ epoch 12674 )\n",
      "[Epoch: 13040] train loss: 0.1837, train acc: 0.9277, val loss: 0.1629, val acc: 0.9531  (best train acc: 0.9508, best val acc: 0.9589, best train loss: 0.1373  @ epoch 12674 )\n",
      "[Epoch: 13060] train loss: 0.1613, train acc: 0.9398, val loss: 0.2178, val acc: 0.9147  (best train acc: 0.9508, best val acc: 0.9595, best train loss: 0.1373  @ epoch 12674 )\n",
      "[Epoch: 13080] train loss: 0.1532, train acc: 0.9414, val loss: 0.1503, val acc: 0.9514  (best train acc: 0.9508, best val acc: 0.9595, best train loss: 0.1373  @ epoch 12674 )\n",
      "[Epoch: 13100] train loss: 0.1792, train acc: 0.9250, val loss: 0.1508, val acc: 0.9514  (best train acc: 0.9508, best val acc: 0.9595, best train loss: 0.1373  @ epoch 12674 )\n",
      "[Epoch: 13120] train loss: 0.1629, train acc: 0.9404, val loss: 0.1688, val acc: 0.9356  (best train acc: 0.9508, best val acc: 0.9595, best train loss: 0.1373  @ epoch 12674 )\n",
      "[Epoch: 13140] train loss: 0.1572, train acc: 0.9365, val loss: 0.1448, val acc: 0.9514  (best train acc: 0.9508, best val acc: 0.9595, best train loss: 0.1373  @ epoch 12674 )\n",
      "[Epoch: 13160] train loss: 0.1557, train acc: 0.9456, val loss: 0.2073, val acc: 0.9187  (best train acc: 0.9508, best val acc: 0.9595, best train loss: 0.1373  @ epoch 12674 )\n",
      "[Epoch: 13180] train loss: 0.1463, train acc: 0.9469, val loss: 0.1668, val acc: 0.9427  (best train acc: 0.9508, best val acc: 0.9595, best train loss: 0.1373  @ epoch 12674 )\n",
      "[Epoch: 13200] train loss: 0.1613, train acc: 0.9396, val loss: 0.1657, val acc: 0.9444  (best train acc: 0.9508, best val acc: 0.9595, best train loss: 0.1373  @ epoch 12674 )\n",
      "[Epoch: 13220] train loss: 0.1568, train acc: 0.9380, val loss: 0.1709, val acc: 0.9349  (best train acc: 0.9508, best val acc: 0.9595, best train loss: 0.1373  @ epoch 12674 )\n",
      "[Epoch: 13240] train loss: 0.1327, train acc: 0.9521, val loss: 0.1662, val acc: 0.9386  (best train acc: 0.9521, best val acc: 0.9595, best train loss: 0.1327  @ epoch 13240 )\n",
      "[Epoch: 13260] train loss: 0.1626, train acc: 0.9383, val loss: 0.1727, val acc: 0.9325  (best train acc: 0.9521, best val acc: 0.9595, best train loss: 0.1327  @ epoch 13240 )\n",
      "[Epoch: 13280] train loss: 0.1638, train acc: 0.9338, val loss: 0.1563, val acc: 0.9430  (best train acc: 0.9521, best val acc: 0.9595, best train loss: 0.1327  @ epoch 13240 )\n",
      "[Epoch: 13300] train loss: 0.2107, train acc: 0.9231, val loss: 0.1670, val acc: 0.9383  (best train acc: 0.9521, best val acc: 0.9595, best train loss: 0.1327  @ epoch 13240 )\n",
      "[Epoch: 13320] train loss: 0.1485, train acc: 0.9418, val loss: 0.1421, val acc: 0.9535  (best train acc: 0.9521, best val acc: 0.9595, best train loss: 0.1327  @ epoch 13240 )\n",
      "[Epoch: 13340] train loss: 0.1523, train acc: 0.9416, val loss: 0.1419, val acc: 0.9514  (best train acc: 0.9521, best val acc: 0.9595, best train loss: 0.1327  @ epoch 13240 )\n",
      "[Epoch: 13360] train loss: 0.1540, train acc: 0.9430, val loss: 0.1718, val acc: 0.9376  (best train acc: 0.9521, best val acc: 0.9595, best train loss: 0.1327  @ epoch 13240 )\n",
      "[Epoch: 13380] train loss: 0.1504, train acc: 0.9444, val loss: 0.1547, val acc: 0.9450  (best train acc: 0.9521, best val acc: 0.9595, best train loss: 0.1327  @ epoch 13240 )\n",
      "[Epoch: 13400] train loss: 0.1549, train acc: 0.9399, val loss: 0.2048, val acc: 0.9248  (best train acc: 0.9521, best val acc: 0.9595, best train loss: 0.1327  @ epoch 13240 )\n",
      "[Epoch: 13420] train loss: 0.1499, train acc: 0.9425, val loss: 0.1566, val acc: 0.9460  (best train acc: 0.9521, best val acc: 0.9595, best train loss: 0.1327  @ epoch 13240 )\n",
      "[Epoch: 13440] train loss: 0.1489, train acc: 0.9442, val loss: 0.1522, val acc: 0.9470  (best train acc: 0.9521, best val acc: 0.9595, best train loss: 0.1327  @ epoch 13240 )\n",
      "[Epoch: 13460] train loss: 0.1631, train acc: 0.9412, val loss: 0.1590, val acc: 0.9497  (best train acc: 0.9521, best val acc: 0.9595, best train loss: 0.1327  @ epoch 13240 )\n",
      "[Epoch: 13480] train loss: 0.1428, train acc: 0.9456, val loss: 0.1791, val acc: 0.9258  (best train acc: 0.9521, best val acc: 0.9595, best train loss: 0.1327  @ epoch 13240 )\n",
      "[Epoch: 13500] train loss: 0.1494, train acc: 0.9412, val loss: 0.1742, val acc: 0.9315  (best train acc: 0.9521, best val acc: 0.9595, best train loss: 0.1327  @ epoch 13240 )\n",
      "[Epoch: 13520] train loss: 0.1458, train acc: 0.9453, val loss: 0.1722, val acc: 0.9309  (best train acc: 0.9521, best val acc: 0.9595, best train loss: 0.1327  @ epoch 13240 )\n",
      "[Epoch: 13540] train loss: 0.1464, train acc: 0.9487, val loss: 0.1526, val acc: 0.9508  (best train acc: 0.9521, best val acc: 0.9595, best train loss: 0.1327  @ epoch 13240 )\n",
      "[Epoch: 13560] train loss: 0.1569, train acc: 0.9388, val loss: 0.1921, val acc: 0.9305  (best train acc: 0.9521, best val acc: 0.9595, best train loss: 0.1327  @ epoch 13240 )\n",
      "[Epoch: 13580] train loss: 0.1751, train acc: 0.9351, val loss: 0.1839, val acc: 0.9282  (best train acc: 0.9521, best val acc: 0.9595, best train loss: 0.1327  @ epoch 13240 )\n",
      "[Epoch: 13600] train loss: 0.1587, train acc: 0.9389, val loss: 0.1406, val acc: 0.9535  (best train acc: 0.9521, best val acc: 0.9595, best train loss: 0.1327  @ epoch 13240 )\n",
      "[Epoch: 13620] train loss: 0.1466, train acc: 0.9431, val loss: 0.1820, val acc: 0.9248  (best train acc: 0.9521, best val acc: 0.9595, best train loss: 0.1327  @ epoch 13240 )\n",
      "[Epoch: 13640] train loss: 0.1546, train acc: 0.9426, val loss: 0.1468, val acc: 0.9528  (best train acc: 0.9521, best val acc: 0.9595, best train loss: 0.1327  @ epoch 13240 )\n",
      "[Epoch: 13660] train loss: 0.1517, train acc: 0.9375, val loss: 0.1554, val acc: 0.9430  (best train acc: 0.9521, best val acc: 0.9595, best train loss: 0.1327  @ epoch 13240 )\n",
      "[Epoch: 13680] train loss: 0.1523, train acc: 0.9409, val loss: 0.1468, val acc: 0.9528  (best train acc: 0.9521, best val acc: 0.9595, best train loss: 0.1325  @ epoch 13661 )\n",
      "[Epoch: 13700] train loss: 0.1502, train acc: 0.9423, val loss: 0.1537, val acc: 0.9487  (best train acc: 0.9521, best val acc: 0.9595, best train loss: 0.1325  @ epoch 13661 )\n",
      "[Epoch: 13720] train loss: 0.1511, train acc: 0.9417, val loss: 0.1566, val acc: 0.9494  (best train acc: 0.9521, best val acc: 0.9595, best train loss: 0.1325  @ epoch 13661 )\n",
      "[Epoch: 13740] train loss: 0.1533, train acc: 0.9389, val loss: 0.1343, val acc: 0.9551  (best train acc: 0.9521, best val acc: 0.9595, best train loss: 0.1325  @ epoch 13661 )\n",
      "[Epoch: 13760] train loss: 0.1394, train acc: 0.9476, val loss: 0.1473, val acc: 0.9535  (best train acc: 0.9521, best val acc: 0.9595, best train loss: 0.1325  @ epoch 13661 )\n",
      "[Epoch: 13780] train loss: 0.1499, train acc: 0.9411, val loss: 0.1510, val acc: 0.9494  (best train acc: 0.9521, best val acc: 0.9595, best train loss: 0.1325  @ epoch 13661 )\n",
      "[Epoch: 13800] train loss: 0.1490, train acc: 0.9451, val loss: 0.1940, val acc: 0.9167  (best train acc: 0.9521, best val acc: 0.9595, best train loss: 0.1325  @ epoch 13661 )\n",
      "[Epoch: 13820] train loss: 0.1498, train acc: 0.9419, val loss: 0.1598, val acc: 0.9460  (best train acc: 0.9521, best val acc: 0.9595, best train loss: 0.1325  @ epoch 13661 )\n",
      "[Epoch: 13840] train loss: 0.1531, train acc: 0.9403, val loss: 0.1629, val acc: 0.9427  (best train acc: 0.9521, best val acc: 0.9595, best train loss: 0.1325  @ epoch 13661 )\n",
      "[Epoch: 13860] train loss: 0.1538, train acc: 0.9426, val loss: 0.1547, val acc: 0.9477  (best train acc: 0.9521, best val acc: 0.9595, best train loss: 0.1325  @ epoch 13661 )\n",
      "[Epoch: 13880] train loss: 0.1459, train acc: 0.9425, val loss: 0.1584, val acc: 0.9383  (best train acc: 0.9521, best val acc: 0.9595, best train loss: 0.1323  @ epoch 13867 )\n",
      "[Epoch: 13900] train loss: 0.1518, train acc: 0.9385, val loss: 0.1722, val acc: 0.9352  (best train acc: 0.9523, best val acc: 0.9595, best train loss: 0.1313  @ epoch 13896 )\n",
      "[Epoch: 13920] train loss: 0.1515, train acc: 0.9458, val loss: 0.1564, val acc: 0.9470  (best train acc: 0.9523, best val acc: 0.9595, best train loss: 0.1313  @ epoch 13896 )\n",
      "[Epoch: 13940] train loss: 0.1601, train acc: 0.9388, val loss: 0.1618, val acc: 0.9376  (best train acc: 0.9523, best val acc: 0.9595, best train loss: 0.1313  @ epoch 13896 )\n",
      "[Epoch: 13960] train loss: 0.1433, train acc: 0.9459, val loss: 0.1563, val acc: 0.9444  (best train acc: 0.9523, best val acc: 0.9595, best train loss: 0.1313  @ epoch 13896 )\n",
      "[Epoch: 13980] train loss: 0.1569, train acc: 0.9432, val loss: 0.1697, val acc: 0.9352  (best train acc: 0.9523, best val acc: 0.9595, best train loss: 0.1313  @ epoch 13896 )\n",
      "[Epoch: 14000] train loss: 0.1434, train acc: 0.9443, val loss: 0.1548, val acc: 0.9477  (best train acc: 0.9523, best val acc: 0.9595, best train loss: 0.1313  @ epoch 13896 )\n",
      "[Epoch: 14020] train loss: 0.1442, train acc: 0.9443, val loss: 0.1512, val acc: 0.9508  (best train acc: 0.9523, best val acc: 0.9595, best train loss: 0.1313  @ epoch 13896 )\n",
      "[Epoch: 14040] train loss: 0.1449, train acc: 0.9464, val loss: 0.1609, val acc: 0.9396  (best train acc: 0.9523, best val acc: 0.9595, best train loss: 0.1313  @ epoch 13896 )\n",
      "[Epoch: 14060] train loss: 0.1363, train acc: 0.9508, val loss: 0.1611, val acc: 0.9376  (best train acc: 0.9523, best val acc: 0.9595, best train loss: 0.1313  @ epoch 13896 )\n",
      "[Epoch: 14080] train loss: 0.1481, train acc: 0.9435, val loss: 0.1923, val acc: 0.9248  (best train acc: 0.9523, best val acc: 0.9595, best train loss: 0.1313  @ epoch 13896 )\n",
      "[Epoch: 14100] train loss: 0.1675, train acc: 0.9361, val loss: 0.1471, val acc: 0.9531  (best train acc: 0.9523, best val acc: 0.9595, best train loss: 0.1313  @ epoch 13896 )\n",
      "[Epoch: 14120] train loss: 0.1477, train acc: 0.9450, val loss: 0.1433, val acc: 0.9474  (best train acc: 0.9523, best val acc: 0.9595, best train loss: 0.1313  @ epoch 13896 )\n",
      "[Epoch: 14140] train loss: 0.1508, train acc: 0.9448, val loss: 0.1515, val acc: 0.9514  (best train acc: 0.9523, best val acc: 0.9595, best train loss: 0.1278  @ epoch 14126 )\n",
      "[Epoch: 14160] train loss: 0.1381, train acc: 0.9448, val loss: 0.1734, val acc: 0.9319  (best train acc: 0.9523, best val acc: 0.9595, best train loss: 0.1278  @ epoch 14126 )\n",
      "[Epoch: 14180] train loss: 0.1402, train acc: 0.9448, val loss: 0.1567, val acc: 0.9396  (best train acc: 0.9523, best val acc: 0.9595, best train loss: 0.1278  @ epoch 14126 )\n",
      "[Epoch: 14200] train loss: 0.1473, train acc: 0.9459, val loss: 0.1470, val acc: 0.9511  (best train acc: 0.9540, best val acc: 0.9595, best train loss: 0.1278  @ epoch 14126 )\n",
      "[Epoch: 14220] train loss: 0.1498, train acc: 0.9424, val loss: 0.1702, val acc: 0.9346  (best train acc: 0.9540, best val acc: 0.9602, best train loss: 0.1278  @ epoch 14126 )\n",
      "[Epoch: 14240] train loss: 0.1357, train acc: 0.9459, val loss: 0.1661, val acc: 0.9373  (best train acc: 0.9540, best val acc: 0.9602, best train loss: 0.1278  @ epoch 14126 )\n",
      "[Epoch: 14260] train loss: 0.1535, train acc: 0.9385, val loss: 0.1788, val acc: 0.9268  (best train acc: 0.9540, best val acc: 0.9602, best train loss: 0.1278  @ epoch 14126 )\n",
      "[Epoch: 14280] train loss: 0.1306, train acc: 0.9524, val loss: 0.1566, val acc: 0.9447  (best train acc: 0.9540, best val acc: 0.9602, best train loss: 0.1278  @ epoch 14126 )\n",
      "[Epoch: 14300] train loss: 0.1490, train acc: 0.9419, val loss: 0.1678, val acc: 0.9315  (best train acc: 0.9540, best val acc: 0.9602, best train loss: 0.1278  @ epoch 14126 )\n",
      "[Epoch: 14320] train loss: 0.1523, train acc: 0.9418, val loss: 0.1470, val acc: 0.9541  (best train acc: 0.9540, best val acc: 0.9602, best train loss: 0.1278  @ epoch 14126 )\n",
      "[Epoch: 14340] train loss: 0.1344, train acc: 0.9508, val loss: 0.1529, val acc: 0.9508  (best train acc: 0.9540, best val acc: 0.9602, best train loss: 0.1278  @ epoch 14126 )\n",
      "[Epoch: 14360] train loss: 0.1384, train acc: 0.9453, val loss: 0.1500, val acc: 0.9454  (best train acc: 0.9540, best val acc: 0.9602, best train loss: 0.1278  @ epoch 14126 )\n",
      "[Epoch: 14380] train loss: 0.1710, train acc: 0.9338, val loss: 0.1873, val acc: 0.9211  (best train acc: 0.9540, best val acc: 0.9602, best train loss: 0.1278  @ epoch 14126 )\n",
      "[Epoch: 14400] train loss: 0.1369, train acc: 0.9496, val loss: 0.1424, val acc: 0.9487  (best train acc: 0.9540, best val acc: 0.9602, best train loss: 0.1278  @ epoch 14126 )\n",
      "[Epoch: 14420] train loss: 0.1500, train acc: 0.9443, val loss: 0.1700, val acc: 0.9352  (best train acc: 0.9540, best val acc: 0.9602, best train loss: 0.1278  @ epoch 14126 )\n",
      "[Epoch: 14440] train loss: 0.1711, train acc: 0.9349, val loss: 0.1616, val acc: 0.9383  (best train acc: 0.9540, best val acc: 0.9602, best train loss: 0.1278  @ epoch 14126 )\n",
      "[Epoch: 14460] train loss: 0.1484, train acc: 0.9468, val loss: 0.1842, val acc: 0.9315  (best train acc: 0.9540, best val acc: 0.9602, best train loss: 0.1278  @ epoch 14126 )\n",
      "[Epoch: 14480] train loss: 0.1458, train acc: 0.9503, val loss: 0.1641, val acc: 0.9410  (best train acc: 0.9540, best val acc: 0.9602, best train loss: 0.1272  @ epoch 14469 )\n",
      "[Epoch: 14500] train loss: 0.1330, train acc: 0.9469, val loss: 0.1597, val acc: 0.9376  (best train acc: 0.9540, best val acc: 0.9602, best train loss: 0.1272  @ epoch 14469 )\n",
      "[Epoch: 14520] train loss: 0.1438, train acc: 0.9434, val loss: 0.1559, val acc: 0.9470  (best train acc: 0.9540, best val acc: 0.9602, best train loss: 0.1272  @ epoch 14469 )\n",
      "[Epoch: 14540] train loss: 0.1413, train acc: 0.9462, val loss: 0.1557, val acc: 0.9383  (best train acc: 0.9540, best val acc: 0.9602, best train loss: 0.1272  @ epoch 14469 )\n",
      "[Epoch: 14560] train loss: 0.1538, train acc: 0.9442, val loss: 0.1436, val acc: 0.9497  (best train acc: 0.9540, best val acc: 0.9602, best train loss: 0.1259  @ epoch 14559 )\n",
      "[Epoch: 14580] train loss: 0.1349, train acc: 0.9475, val loss: 0.1708, val acc: 0.9315  (best train acc: 0.9540, best val acc: 0.9602, best train loss: 0.1259  @ epoch 14559 )\n",
      "[Epoch: 14600] train loss: 0.1458, train acc: 0.9438, val loss: 0.1589, val acc: 0.9454  (best train acc: 0.9540, best val acc: 0.9602, best train loss: 0.1259  @ epoch 14559 )\n",
      "[Epoch: 14620] train loss: 0.1588, train acc: 0.9413, val loss: 0.1793, val acc: 0.9285  (best train acc: 0.9540, best val acc: 0.9602, best train loss: 0.1258  @ epoch 14601 )\n",
      "[Epoch: 14640] train loss: 0.1883, train acc: 0.9312, val loss: 0.2070, val acc: 0.9207  (best train acc: 0.9540, best val acc: 0.9602, best train loss: 0.1258  @ epoch 14601 )\n",
      "[Epoch: 14660] train loss: 0.1537, train acc: 0.9376, val loss: 0.1485, val acc: 0.9518  (best train acc: 0.9540, best val acc: 0.9602, best train loss: 0.1258  @ epoch 14601 )\n",
      "[Epoch: 14680] train loss: 0.1428, train acc: 0.9452, val loss: 0.1852, val acc: 0.9214  (best train acc: 0.9540, best val acc: 0.9602, best train loss: 0.1258  @ epoch 14601 )\n",
      "[Epoch: 14700] train loss: 0.1363, train acc: 0.9463, val loss: 0.1847, val acc: 0.9282  (best train acc: 0.9540, best val acc: 0.9602, best train loss: 0.1258  @ epoch 14601 )\n",
      "[Epoch: 14720] train loss: 0.1366, train acc: 0.9503, val loss: 0.1462, val acc: 0.9514  (best train acc: 0.9540, best val acc: 0.9602, best train loss: 0.1258  @ epoch 14601 )\n",
      "[Epoch: 14740] train loss: 0.1350, train acc: 0.9513, val loss: 0.1663, val acc: 0.9373  (best train acc: 0.9540, best val acc: 0.9602, best train loss: 0.1258  @ epoch 14601 )\n",
      "[Epoch: 14760] train loss: 0.1421, train acc: 0.9442, val loss: 0.1663, val acc: 0.9400  (best train acc: 0.9540, best val acc: 0.9602, best train loss: 0.1258  @ epoch 14601 )\n",
      "[Epoch: 14780] train loss: 0.1562, train acc: 0.9396, val loss: 0.1379, val acc: 0.9541  (best train acc: 0.9540, best val acc: 0.9602, best train loss: 0.1258  @ epoch 14601 )\n",
      "[Epoch: 14800] train loss: 0.1350, train acc: 0.9477, val loss: 0.1812, val acc: 0.9373  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1258  @ epoch 14601 )\n",
      "[Epoch: 14820] train loss: 0.1387, train acc: 0.9474, val loss: 0.1544, val acc: 0.9437  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1258  @ epoch 14601 )\n",
      "[Epoch: 14840] train loss: 0.1499, train acc: 0.9432, val loss: 0.1409, val acc: 0.9531  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1258  @ epoch 14601 )\n",
      "[Epoch: 14860] train loss: 0.1414, train acc: 0.9462, val loss: 0.1436, val acc: 0.9535  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1258  @ epoch 14601 )\n",
      "[Epoch: 14880] train loss: 0.1441, train acc: 0.9487, val loss: 0.1618, val acc: 0.9325  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1258  @ epoch 14601 )\n",
      "[Epoch: 14900] train loss: 0.1534, train acc: 0.9451, val loss: 0.1547, val acc: 0.9514  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1258  @ epoch 14601 )\n",
      "[Epoch: 14920] train loss: 0.1482, train acc: 0.9468, val loss: 0.1333, val acc: 0.9575  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1258  @ epoch 14601 )\n",
      "[Epoch: 14940] train loss: 0.1483, train acc: 0.9457, val loss: 0.1587, val acc: 0.9427  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1258  @ epoch 14601 )\n",
      "[Epoch: 14960] train loss: 0.1433, train acc: 0.9474, val loss: 0.1842, val acc: 0.9258  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1258  @ epoch 14601 )\n",
      "[Epoch: 14980] train loss: 0.1419, train acc: 0.9489, val loss: 0.1526, val acc: 0.9450  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1258  @ epoch 14601 )\n",
      "[Epoch: 15000] train loss: 0.1420, train acc: 0.9443, val loss: 0.1459, val acc: 0.9521  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1258  @ epoch 14601 )\n",
      "[Epoch: 15020] train loss: 0.1381, train acc: 0.9504, val loss: 0.1562, val acc: 0.9457  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1258  @ epoch 14601 )\n",
      "[Epoch: 15040] train loss: 0.1358, train acc: 0.9505, val loss: 0.1479, val acc: 0.9487  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1258  @ epoch 14601 )\n",
      "[Epoch: 15060] train loss: 0.1403, train acc: 0.9464, val loss: 0.1475, val acc: 0.9501  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1258  @ epoch 14601 )\n",
      "[Epoch: 15080] train loss: 0.1393, train acc: 0.9443, val loss: 0.1777, val acc: 0.9325  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1236  @ epoch 15064 )\n",
      "[Epoch: 15100] train loss: 0.1579, train acc: 0.9383, val loss: 0.1662, val acc: 0.9379  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1236  @ epoch 15064 )\n",
      "[Epoch: 15120] train loss: 0.1458, train acc: 0.9393, val loss: 0.1377, val acc: 0.9497  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1236  @ epoch 15064 )\n",
      "[Epoch: 15140] train loss: 0.1580, train acc: 0.9383, val loss: 0.1739, val acc: 0.9329  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1236  @ epoch 15064 )\n",
      "[Epoch: 15160] train loss: 0.1473, train acc: 0.9411, val loss: 0.1623, val acc: 0.9346  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1236  @ epoch 15064 )\n",
      "[Epoch: 15180] train loss: 0.1340, train acc: 0.9511, val loss: 0.1469, val acc: 0.9504  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1236  @ epoch 15064 )\n",
      "[Epoch: 15200] train loss: 0.1339, train acc: 0.9497, val loss: 0.1464, val acc: 0.9487  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1236  @ epoch 15064 )\n",
      "[Epoch: 15220] train loss: 0.1320, train acc: 0.9494, val loss: 0.1409, val acc: 0.9511  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1230  @ epoch 15204 )\n",
      "[Epoch: 15240] train loss: 0.1470, train acc: 0.9419, val loss: 0.1488, val acc: 0.9454  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1230  @ epoch 15204 )\n",
      "[Epoch: 15260] train loss: 0.1585, train acc: 0.9404, val loss: 0.1761, val acc: 0.9325  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1230  @ epoch 15204 )\n",
      "[Epoch: 15280] train loss: 0.1316, train acc: 0.9487, val loss: 0.1491, val acc: 0.9518  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1230  @ epoch 15204 )\n",
      "[Epoch: 15300] train loss: 0.1505, train acc: 0.9448, val loss: 0.1666, val acc: 0.9437  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1230  @ epoch 15204 )\n",
      "[Epoch: 15320] train loss: 0.1506, train acc: 0.9410, val loss: 0.1679, val acc: 0.9349  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1230  @ epoch 15204 )\n",
      "[Epoch: 15340] train loss: 0.1387, train acc: 0.9453, val loss: 0.1440, val acc: 0.9508  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1230  @ epoch 15204 )\n",
      "[Epoch: 15360] train loss: 0.1435, train acc: 0.9445, val loss: 0.1574, val acc: 0.9379  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1230  @ epoch 15204 )\n",
      "[Epoch: 15380] train loss: 0.1350, train acc: 0.9472, val loss: 0.1440, val acc: 0.9470  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1230  @ epoch 15204 )\n",
      "[Epoch: 15400] train loss: 0.1383, train acc: 0.9446, val loss: 0.1603, val acc: 0.9329  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1230  @ epoch 15204 )\n",
      "[Epoch: 15420] train loss: 0.1475, train acc: 0.9455, val loss: 0.1524, val acc: 0.9447  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1230  @ epoch 15204 )\n",
      "[Epoch: 15440] train loss: 0.1331, train acc: 0.9485, val loss: 0.1548, val acc: 0.9403  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1230  @ epoch 15204 )\n",
      "[Epoch: 15460] train loss: 0.1431, train acc: 0.9466, val loss: 0.1477, val acc: 0.9481  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1217  @ epoch 15442 )\n",
      "[Epoch: 15480] train loss: 0.1389, train acc: 0.9511, val loss: 0.1423, val acc: 0.9504  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1217  @ epoch 15442 )\n",
      "[Epoch: 15500] train loss: 0.1297, train acc: 0.9541, val loss: 0.1452, val acc: 0.9535  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1217  @ epoch 15442 )\n",
      "[Epoch: 15520] train loss: 0.1551, train acc: 0.9392, val loss: 0.1662, val acc: 0.9410  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1217  @ epoch 15442 )\n",
      "[Epoch: 15540] train loss: 0.1434, train acc: 0.9490, val loss: 0.1513, val acc: 0.9450  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1217  @ epoch 15442 )\n",
      "[Epoch: 15560] train loss: 0.1413, train acc: 0.9455, val loss: 0.1723, val acc: 0.9332  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1217  @ epoch 15442 )\n",
      "[Epoch: 15580] train loss: 0.1362, train acc: 0.9481, val loss: 0.1500, val acc: 0.9457  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1217  @ epoch 15442 )\n",
      "[Epoch: 15600] train loss: 0.1426, train acc: 0.9454, val loss: 0.1488, val acc: 0.9470  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1217  @ epoch 15442 )\n",
      "[Epoch: 15620] train loss: 0.1409, train acc: 0.9477, val loss: 0.1556, val acc: 0.9440  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1216  @ epoch 15615 )\n",
      "[Epoch: 15640] train loss: 0.1321, train acc: 0.9483, val loss: 0.1379, val acc: 0.9545  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1216  @ epoch 15615 )\n",
      "[Epoch: 15660] train loss: 0.1727, train acc: 0.9390, val loss: 0.1971, val acc: 0.9295  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1216  @ epoch 15615 )\n",
      "[Epoch: 15680] train loss: 0.1418, train acc: 0.9499, val loss: 0.1614, val acc: 0.9413  (best train acc: 0.9547, best val acc: 0.9602, best train loss: 0.1216  @ epoch 15615 )\n",
      "[Epoch: 15700] train loss: 0.1716, train acc: 0.9384, val loss: 0.1911, val acc: 0.9218  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1216  @ epoch 15615 )\n",
      "[Epoch: 15720] train loss: 0.1405, train acc: 0.9466, val loss: 0.1494, val acc: 0.9474  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1216  @ epoch 15615 )\n",
      "[Epoch: 15740] train loss: 0.1465, train acc: 0.9439, val loss: 0.1470, val acc: 0.9508  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1216  @ epoch 15615 )\n",
      "[Epoch: 15760] train loss: 0.1341, train acc: 0.9469, val loss: 0.1451, val acc: 0.9474  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1216  @ epoch 15615 )\n",
      "[Epoch: 15780] train loss: 0.1418, train acc: 0.9470, val loss: 0.1485, val acc: 0.9454  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1216  @ epoch 15615 )\n",
      "[Epoch: 15800] train loss: 0.1336, train acc: 0.9516, val loss: 0.1379, val acc: 0.9562  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1216  @ epoch 15615 )\n",
      "[Epoch: 15820] train loss: 0.1350, train acc: 0.9470, val loss: 0.1549, val acc: 0.9430  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1216  @ epoch 15615 )\n",
      "[Epoch: 15840] train loss: 0.1402, train acc: 0.9478, val loss: 0.1485, val acc: 0.9477  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1216  @ epoch 15615 )\n",
      "[Epoch: 15860] train loss: 0.1556, train acc: 0.9410, val loss: 0.1625, val acc: 0.9390  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1216  @ epoch 15615 )\n",
      "[Epoch: 15880] train loss: 0.1334, train acc: 0.9458, val loss: 0.1554, val acc: 0.9379  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1216  @ epoch 15615 )\n",
      "[Epoch: 15900] train loss: 0.1636, train acc: 0.9363, val loss: 0.1505, val acc: 0.9487  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1216  @ epoch 15615 )\n",
      "[Epoch: 15920] train loss: 0.1363, train acc: 0.9480, val loss: 0.1379, val acc: 0.9531  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1216  @ epoch 15615 )\n",
      "[Epoch: 15940] train loss: 0.1432, train acc: 0.9440, val loss: 0.1539, val acc: 0.9447  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1216  @ epoch 15615 )\n",
      "[Epoch: 15960] train loss: 0.1418, train acc: 0.9454, val loss: 0.1502, val acc: 0.9487  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1216  @ epoch 15615 )\n",
      "[Epoch: 15980] train loss: 0.1435, train acc: 0.9500, val loss: 0.1436, val acc: 0.9501  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1216  @ epoch 15615 )\n",
      "[Epoch: 16000] train loss: 0.1460, train acc: 0.9469, val loss: 0.1552, val acc: 0.9410  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1216  @ epoch 15615 )\n",
      "[Epoch: 16020] train loss: 0.1383, train acc: 0.9481, val loss: 0.1699, val acc: 0.9363  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1216  @ epoch 15615 )\n",
      "[Epoch: 16040] train loss: 0.1398, train acc: 0.9469, val loss: 0.1553, val acc: 0.9423  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1216  @ epoch 15615 )\n",
      "[Epoch: 16060] train loss: 0.1589, train acc: 0.9402, val loss: 0.1531, val acc: 0.9477  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1216  @ epoch 15615 )\n",
      "[Epoch: 16080] train loss: 0.1502, train acc: 0.9417, val loss: 0.1876, val acc: 0.9234  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1216  @ epoch 15615 )\n",
      "[Epoch: 16100] train loss: 0.1481, train acc: 0.9483, val loss: 0.1423, val acc: 0.9518  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1216  @ epoch 15615 )\n",
      "[Epoch: 16120] train loss: 0.1343, train acc: 0.9520, val loss: 0.1506, val acc: 0.9528  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1216  @ epoch 15615 )\n",
      "[Epoch: 16140] train loss: 0.1423, train acc: 0.9470, val loss: 0.1426, val acc: 0.9514  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1216  @ epoch 15615 )\n",
      "[Epoch: 16160] train loss: 0.1241, train acc: 0.9525, val loss: 0.1505, val acc: 0.9460  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1216  @ epoch 15615 )\n",
      "[Epoch: 16180] train loss: 0.1400, train acc: 0.9455, val loss: 0.1464, val acc: 0.9487  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1214  @ epoch 16177 )\n",
      "[Epoch: 16200] train loss: 0.1676, train acc: 0.9336, val loss: 0.2064, val acc: 0.9218  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1214  @ epoch 16177 )\n",
      "[Epoch: 16220] train loss: 0.1311, train acc: 0.9505, val loss: 0.1471, val acc: 0.9538  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1214  @ epoch 16177 )\n",
      "[Epoch: 16240] train loss: 0.1448, train acc: 0.9439, val loss: 0.1547, val acc: 0.9410  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1214  @ epoch 16177 )\n",
      "[Epoch: 16260] train loss: 0.1353, train acc: 0.9505, val loss: 0.1361, val acc: 0.9562  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1214  @ epoch 16177 )\n",
      "[Epoch: 16280] train loss: 0.1237, train acc: 0.9527, val loss: 0.1598, val acc: 0.9427  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1214  @ epoch 16177 )\n",
      "[Epoch: 16300] train loss: 0.1326, train acc: 0.9515, val loss: 0.1696, val acc: 0.9420  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1206  @ epoch 16284 )\n",
      "[Epoch: 16320] train loss: 0.1419, train acc: 0.9428, val loss: 0.1472, val acc: 0.9491  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1206  @ epoch 16284 )\n",
      "[Epoch: 16340] train loss: 0.1330, train acc: 0.9473, val loss: 0.1631, val acc: 0.9366  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1206  @ epoch 16284 )\n",
      "[Epoch: 16360] train loss: 0.1224, train acc: 0.9547, val loss: 0.1487, val acc: 0.9433  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1206  @ epoch 16284 )\n",
      "[Epoch: 16380] train loss: 0.1380, train acc: 0.9493, val loss: 0.1412, val acc: 0.9535  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1206  @ epoch 16284 )\n",
      "[Epoch: 16400] train loss: 0.1406, train acc: 0.9474, val loss: 0.1450, val acc: 0.9487  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1206  @ epoch 16284 )\n",
      "[Epoch: 16420] train loss: 0.1369, train acc: 0.9477, val loss: 0.1646, val acc: 0.9336  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1206  @ epoch 16284 )\n",
      "[Epoch: 16440] train loss: 0.1555, train acc: 0.9427, val loss: 0.1518, val acc: 0.9504  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1206  @ epoch 16284 )\n",
      "[Epoch: 16460] train loss: 0.1346, train acc: 0.9491, val loss: 0.1574, val acc: 0.9400  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1206  @ epoch 16284 )\n",
      "[Epoch: 16480] train loss: 0.1239, train acc: 0.9542, val loss: 0.1644, val acc: 0.9400  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1206  @ epoch 16284 )\n",
      "[Epoch: 16500] train loss: 0.1363, train acc: 0.9494, val loss: 0.1763, val acc: 0.9315  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1206  @ epoch 16284 )\n",
      "[Epoch: 16520] train loss: 0.1457, train acc: 0.9435, val loss: 0.1486, val acc: 0.9477  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1206  @ epoch 16284 )\n",
      "[Epoch: 16540] train loss: 0.1380, train acc: 0.9482, val loss: 0.1434, val acc: 0.9497  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1206  @ epoch 16284 )\n",
      "[Epoch: 16560] train loss: 0.1489, train acc: 0.9396, val loss: 0.1615, val acc: 0.9363  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1206  @ epoch 16284 )\n",
      "[Epoch: 16580] train loss: 0.1399, train acc: 0.9461, val loss: 0.1662, val acc: 0.9315  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1206  @ epoch 16284 )\n",
      "[Epoch: 16600] train loss: 0.1347, train acc: 0.9443, val loss: 0.1803, val acc: 0.9288  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1206  @ epoch 16284 )\n",
      "[Epoch: 16620] train loss: 0.1391, train acc: 0.9469, val loss: 0.1724, val acc: 0.9342  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1206  @ epoch 16284 )\n",
      "[Epoch: 16640] train loss: 0.1322, train acc: 0.9488, val loss: 0.1466, val acc: 0.9454  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1206  @ epoch 16284 )\n",
      "[Epoch: 16660] train loss: 0.1439, train acc: 0.9504, val loss: 0.1540, val acc: 0.9531  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1206  @ epoch 16284 )\n",
      "[Epoch: 16680] train loss: 0.1382, train acc: 0.9456, val loss: 0.1660, val acc: 0.9336  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1206  @ epoch 16284 )\n",
      "[Epoch: 16700] train loss: 0.1329, train acc: 0.9519, val loss: 0.1731, val acc: 0.9366  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1206  @ epoch 16284 )\n",
      "[Epoch: 16720] train loss: 0.1494, train acc: 0.9385, val loss: 0.1433, val acc: 0.9504  (best train acc: 0.9563, best val acc: 0.9602, best train loss: 0.1206  @ epoch 16284 )\n",
      "[Epoch: 16740] train loss: 0.1270, train acc: 0.9524, val loss: 0.1676, val acc: 0.9359  (best train acc: 0.9565, best val acc: 0.9602, best train loss: 0.1206  @ epoch 16284 )\n",
      "[Epoch: 16760] train loss: 0.1291, train acc: 0.9490, val loss: 0.1589, val acc: 0.9423  (best train acc: 0.9565, best val acc: 0.9602, best train loss: 0.1206  @ epoch 16284 )\n",
      "[Epoch: 16780] train loss: 0.1430, train acc: 0.9413, val loss: 0.1414, val acc: 0.9541  (best train acc: 0.9565, best val acc: 0.9602, best train loss: 0.1206  @ epoch 16284 )\n",
      "[Epoch: 16800] train loss: 0.1347, train acc: 0.9475, val loss: 0.1642, val acc: 0.9352  (best train acc: 0.9565, best val acc: 0.9602, best train loss: 0.1206  @ epoch 16284 )\n",
      "[Epoch: 16820] train loss: 0.1296, train acc: 0.9506, val loss: 0.1515, val acc: 0.9430  (best train acc: 0.9565, best val acc: 0.9602, best train loss: 0.1195  @ epoch 16810 )\n",
      "[Epoch: 16840] train loss: 0.1301, train acc: 0.9499, val loss: 0.1385, val acc: 0.9528  (best train acc: 0.9565, best val acc: 0.9602, best train loss: 0.1195  @ epoch 16810 )\n",
      "[Epoch: 16860] train loss: 0.1338, train acc: 0.9508, val loss: 0.1517, val acc: 0.9444  (best train acc: 0.9565, best val acc: 0.9602, best train loss: 0.1195  @ epoch 16810 )\n",
      "[Epoch: 16880] train loss: 0.1343, train acc: 0.9511, val loss: 0.1478, val acc: 0.9494  (best train acc: 0.9568, best val acc: 0.9602, best train loss: 0.1195  @ epoch 16810 )\n",
      "[Epoch: 16900] train loss: 0.1442, train acc: 0.9425, val loss: 0.1520, val acc: 0.9430  (best train acc: 0.9568, best val acc: 0.9602, best train loss: 0.1195  @ epoch 16810 )\n",
      "[Epoch: 16920] train loss: 0.1378, train acc: 0.9491, val loss: 0.1510, val acc: 0.9440  (best train acc: 0.9568, best val acc: 0.9602, best train loss: 0.1195  @ epoch 16810 )\n",
      "[Epoch: 16940] train loss: 0.1283, train acc: 0.9523, val loss: 0.1528, val acc: 0.9427  (best train acc: 0.9569, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 16960] train loss: 0.1327, train acc: 0.9514, val loss: 0.1551, val acc: 0.9467  (best train acc: 0.9569, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 16980] train loss: 0.1361, train acc: 0.9467, val loss: 0.1645, val acc: 0.9325  (best train acc: 0.9569, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17000] train loss: 0.1429, train acc: 0.9456, val loss: 0.1433, val acc: 0.9511  (best train acc: 0.9569, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17020] train loss: 0.1381, train acc: 0.9476, val loss: 0.1690, val acc: 0.9383  (best train acc: 0.9569, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17040] train loss: 0.1364, train acc: 0.9482, val loss: 0.1730, val acc: 0.9295  (best train acc: 0.9569, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17060] train loss: 0.1256, train acc: 0.9501, val loss: 0.1485, val acc: 0.9487  (best train acc: 0.9569, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17080] train loss: 0.1464, train acc: 0.9477, val loss: 0.1522, val acc: 0.9440  (best train acc: 0.9569, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17100] train loss: 0.1511, train acc: 0.9430, val loss: 0.1605, val acc: 0.9417  (best train acc: 0.9569, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17120] train loss: 0.1536, train acc: 0.9386, val loss: 0.1578, val acc: 0.9403  (best train acc: 0.9569, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17140] train loss: 0.1424, train acc: 0.9441, val loss: 0.1310, val acc: 0.9595  (best train acc: 0.9569, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17160] train loss: 0.1448, train acc: 0.9435, val loss: 0.1531, val acc: 0.9467  (best train acc: 0.9569, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17180] train loss: 0.1493, train acc: 0.9455, val loss: 0.1662, val acc: 0.9295  (best train acc: 0.9569, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17200] train loss: 0.1390, train acc: 0.9472, val loss: 0.1417, val acc: 0.9504  (best train acc: 0.9569, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17220] train loss: 0.1344, train acc: 0.9454, val loss: 0.1650, val acc: 0.9383  (best train acc: 0.9571, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17240] train loss: 0.1334, train acc: 0.9511, val loss: 0.1357, val acc: 0.9548  (best train acc: 0.9571, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17260] train loss: 0.1231, train acc: 0.9535, val loss: 0.1410, val acc: 0.9524  (best train acc: 0.9571, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17280] train loss: 0.1304, train acc: 0.9524, val loss: 0.1408, val acc: 0.9531  (best train acc: 0.9571, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17300] train loss: 0.1388, train acc: 0.9485, val loss: 0.1485, val acc: 0.9524  (best train acc: 0.9571, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17320] train loss: 0.1319, train acc: 0.9507, val loss: 0.1274, val acc: 0.9592  (best train acc: 0.9571, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17340] train loss: 0.1514, train acc: 0.9406, val loss: 0.1504, val acc: 0.9467  (best train acc: 0.9571, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17360] train loss: 0.1352, train acc: 0.9472, val loss: 0.1456, val acc: 0.9521  (best train acc: 0.9571, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17380] train loss: 0.1630, train acc: 0.9393, val loss: 0.1443, val acc: 0.9535  (best train acc: 0.9571, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17400] train loss: 0.1617, train acc: 0.9409, val loss: 0.1576, val acc: 0.9376  (best train acc: 0.9571, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17420] train loss: 0.1255, train acc: 0.9513, val loss: 0.1413, val acc: 0.9545  (best train acc: 0.9571, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17440] train loss: 0.1372, train acc: 0.9448, val loss: 0.1533, val acc: 0.9454  (best train acc: 0.9571, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17460] train loss: 0.1440, train acc: 0.9477, val loss: 0.1337, val acc: 0.9562  (best train acc: 0.9571, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17480] train loss: 0.1484, train acc: 0.9430, val loss: 0.1613, val acc: 0.9376  (best train acc: 0.9571, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17500] train loss: 0.1462, train acc: 0.9426, val loss: 0.1392, val acc: 0.9551  (best train acc: 0.9571, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17520] train loss: 0.1408, train acc: 0.9441, val loss: 0.1497, val acc: 0.9491  (best train acc: 0.9571, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17540] train loss: 0.1235, train acc: 0.9513, val loss: 0.1318, val acc: 0.9562  (best train acc: 0.9571, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17560] train loss: 0.1374, train acc: 0.9472, val loss: 0.1563, val acc: 0.9386  (best train acc: 0.9571, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17580] train loss: 0.1234, train acc: 0.9529, val loss: 0.1551, val acc: 0.9427  (best train acc: 0.9571, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17600] train loss: 0.1394, train acc: 0.9478, val loss: 0.1734, val acc: 0.9339  (best train acc: 0.9571, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17620] train loss: 0.1352, train acc: 0.9501, val loss: 0.1399, val acc: 0.9565  (best train acc: 0.9571, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17640] train loss: 0.1326, train acc: 0.9501, val loss: 0.1399, val acc: 0.9565  (best train acc: 0.9581, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17660] train loss: 0.1462, train acc: 0.9472, val loss: 0.1562, val acc: 0.9410  (best train acc: 0.9581, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17680] train loss: 0.1311, train acc: 0.9532, val loss: 0.1658, val acc: 0.9396  (best train acc: 0.9581, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17700] train loss: 0.1305, train acc: 0.9527, val loss: 0.1555, val acc: 0.9410  (best train acc: 0.9581, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17720] train loss: 0.1412, train acc: 0.9432, val loss: 0.1427, val acc: 0.9528  (best train acc: 0.9581, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17740] train loss: 0.1300, train acc: 0.9517, val loss: 0.1468, val acc: 0.9535  (best train acc: 0.9581, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17760] train loss: 0.1580, train acc: 0.9370, val loss: 0.1719, val acc: 0.9346  (best train acc: 0.9581, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17780] train loss: 0.1372, train acc: 0.9456, val loss: 0.1404, val acc: 0.9508  (best train acc: 0.9581, best val acc: 0.9602, best train loss: 0.1168  @ epoch 16936 )\n",
      "[Epoch: 17800] train loss: 0.1402, train acc: 0.9469, val loss: 0.1546, val acc: 0.9460  (best train acc: 0.9581, best val acc: 0.9602, best train loss: 0.1166  @ epoch 17784 )\n",
      "[Epoch: 17820] train loss: 0.1265, train acc: 0.9541, val loss: 0.1403, val acc: 0.9538  (best train acc: 0.9581, best val acc: 0.9602, best train loss: 0.1166  @ epoch 17784 )\n",
      "[Epoch: 17840] train loss: 0.1172, train acc: 0.9571, val loss: 0.1443, val acc: 0.9423  (best train acc: 0.9581, best val acc: 0.9602, best train loss: 0.1166  @ epoch 17784 )\n",
      "[Epoch: 17860] train loss: 0.1320, train acc: 0.9512, val loss: 0.1540, val acc: 0.9430  (best train acc: 0.9581, best val acc: 0.9602, best train loss: 0.1133  @ epoch 17856 )\n",
      "[Epoch: 17880] train loss: 0.1264, train acc: 0.9523, val loss: 0.1337, val acc: 0.9551  (best train acc: 0.9581, best val acc: 0.9602, best train loss: 0.1133  @ epoch 17856 )\n",
      "[Epoch: 17900] train loss: 0.1324, train acc: 0.9480, val loss: 0.1435, val acc: 0.9508  (best train acc: 0.9581, best val acc: 0.9602, best train loss: 0.1133  @ epoch 17856 )\n",
      "[Epoch: 17920] train loss: 0.1205, train acc: 0.9555, val loss: 0.1645, val acc: 0.9430  (best train acc: 0.9581, best val acc: 0.9602, best train loss: 0.1133  @ epoch 17856 )\n",
      "[Epoch: 17940] train loss: 0.1243, train acc: 0.9526, val loss: 0.1310, val acc: 0.9585  (best train acc: 0.9591, best val acc: 0.9602, best train loss: 0.1128  @ epoch 17937 )\n",
      "[Epoch: 17960] train loss: 0.1401, train acc: 0.9448, val loss: 0.1404, val acc: 0.9565  (best train acc: 0.9591, best val acc: 0.9602, best train loss: 0.1128  @ epoch 17937 )\n",
      "[Epoch: 17980] train loss: 0.1218, train acc: 0.9513, val loss: 0.1479, val acc: 0.9444  (best train acc: 0.9591, best val acc: 0.9602, best train loss: 0.1128  @ epoch 17937 )\n",
      "[Epoch: 18000] train loss: 0.1258, train acc: 0.9536, val loss: 0.1547, val acc: 0.9450  (best train acc: 0.9591, best val acc: 0.9602, best train loss: 0.1128  @ epoch 17937 )\n",
      "[Epoch: 18020] train loss: 0.1249, train acc: 0.9519, val loss: 0.1557, val acc: 0.9460  (best train acc: 0.9591, best val acc: 0.9602, best train loss: 0.1128  @ epoch 17937 )\n",
      "[Epoch: 18040] train loss: 0.1259, train acc: 0.9510, val loss: 0.1509, val acc: 0.9457  (best train acc: 0.9591, best val acc: 0.9609, best train loss: 0.1128  @ epoch 17937 )\n",
      "[Epoch: 18060] train loss: 0.1543, train acc: 0.9440, val loss: 0.1541, val acc: 0.9444  (best train acc: 0.9591, best val acc: 0.9609, best train loss: 0.1128  @ epoch 17937 )\n",
      "[Epoch: 18080] train loss: 0.1338, train acc: 0.9471, val loss: 0.1624, val acc: 0.9427  (best train acc: 0.9591, best val acc: 0.9609, best train loss: 0.1128  @ epoch 17937 )\n",
      "[Epoch: 18100] train loss: 0.1422, train acc: 0.9475, val loss: 0.1609, val acc: 0.9410  (best train acc: 0.9591, best val acc: 0.9609, best train loss: 0.1128  @ epoch 17937 )\n",
      "[Epoch: 18120] train loss: 0.1348, train acc: 0.9521, val loss: 0.1563, val acc: 0.9437  (best train acc: 0.9591, best val acc: 0.9609, best train loss: 0.1128  @ epoch 17937 )\n",
      "[Epoch: 18140] train loss: 0.1292, train acc: 0.9547, val loss: 0.1337, val acc: 0.9541  (best train acc: 0.9598, best val acc: 0.9609, best train loss: 0.1128  @ epoch 17937 )\n",
      "[Epoch: 18160] train loss: 0.1220, train acc: 0.9557, val loss: 0.1506, val acc: 0.9487  (best train acc: 0.9598, best val acc: 0.9609, best train loss: 0.1128  @ epoch 17937 )\n",
      "[Epoch: 18180] train loss: 0.1281, train acc: 0.9506, val loss: 0.1320, val acc: 0.9568  (best train acc: 0.9598, best val acc: 0.9609, best train loss: 0.1128  @ epoch 17937 )\n",
      "[Epoch: 18200] train loss: 0.1340, train acc: 0.9491, val loss: 0.1403, val acc: 0.9504  (best train acc: 0.9598, best val acc: 0.9609, best train loss: 0.1128  @ epoch 17937 )\n",
      "[Epoch: 18220] train loss: 0.1343, train acc: 0.9540, val loss: 0.1550, val acc: 0.9454  (best train acc: 0.9598, best val acc: 0.9609, best train loss: 0.1128  @ epoch 17937 )\n",
      "[Epoch: 18240] train loss: 0.1188, train acc: 0.9550, val loss: 0.1456, val acc: 0.9501  (best train acc: 0.9610, best val acc: 0.9609, best train loss: 0.1100  @ epoch 18230 )\n",
      "[Epoch: 18260] train loss: 0.1274, train acc: 0.9504, val loss: 0.1455, val acc: 0.9504  (best train acc: 0.9610, best val acc: 0.9609, best train loss: 0.1100  @ epoch 18230 )\n",
      "[Epoch: 18280] train loss: 0.1578, train acc: 0.9434, val loss: 0.1873, val acc: 0.9292  (best train acc: 0.9610, best val acc: 0.9609, best train loss: 0.1100  @ epoch 18230 )\n",
      "[Epoch: 18300] train loss: 0.1251, train acc: 0.9521, val loss: 0.1604, val acc: 0.9423  (best train acc: 0.9610, best val acc: 0.9609, best train loss: 0.1100  @ epoch 18230 )\n",
      "[Epoch: 18320] train loss: 0.1292, train acc: 0.9513, val loss: 0.1627, val acc: 0.9410  (best train acc: 0.9610, best val acc: 0.9609, best train loss: 0.1100  @ epoch 18230 )\n",
      "[Epoch: 18340] train loss: 0.1273, train acc: 0.9542, val loss: 0.1449, val acc: 0.9474  (best train acc: 0.9610, best val acc: 0.9609, best train loss: 0.1100  @ epoch 18230 )\n",
      "[Epoch: 18360] train loss: 0.1751, train acc: 0.9380, val loss: 0.1859, val acc: 0.9298  (best train acc: 0.9610, best val acc: 0.9609, best train loss: 0.1100  @ epoch 18230 )\n",
      "[Epoch: 18380] train loss: 0.1445, train acc: 0.9491, val loss: 0.1571, val acc: 0.9433  (best train acc: 0.9610, best val acc: 0.9609, best train loss: 0.1100  @ epoch 18230 )\n",
      "[Epoch: 18400] train loss: 0.1308, train acc: 0.9513, val loss: 0.1499, val acc: 0.9474  (best train acc: 0.9610, best val acc: 0.9609, best train loss: 0.1100  @ epoch 18230 )\n",
      "[Epoch: 18420] train loss: 0.1321, train acc: 0.9476, val loss: 0.1471, val acc: 0.9491  (best train acc: 0.9610, best val acc: 0.9609, best train loss: 0.1100  @ epoch 18230 )\n",
      "[Epoch: 18440] train loss: 0.1351, train acc: 0.9482, val loss: 0.1439, val acc: 0.9484  (best train acc: 0.9610, best val acc: 0.9609, best train loss: 0.1100  @ epoch 18230 )\n",
      "[Epoch: 18460] train loss: 0.1310, train acc: 0.9493, val loss: 0.1305, val acc: 0.9585  (best train acc: 0.9610, best val acc: 0.9609, best train loss: 0.1100  @ epoch 18230 )\n",
      "[Epoch: 18480] train loss: 0.1259, train acc: 0.9508, val loss: 0.1393, val acc: 0.9494  (best train acc: 0.9610, best val acc: 0.9609, best train loss: 0.1087  @ epoch 18464 )\n",
      "[Epoch: 18500] train loss: 0.1331, train acc: 0.9541, val loss: 0.1406, val acc: 0.9531  (best train acc: 0.9610, best val acc: 0.9609, best train loss: 0.1087  @ epoch 18464 )\n",
      "[Epoch: 18520] train loss: 0.1583, train acc: 0.9396, val loss: 0.1297, val acc: 0.9595  (best train acc: 0.9610, best val acc: 0.9609, best train loss: 0.1087  @ epoch 18464 )\n",
      "[Epoch: 18540] train loss: 0.1350, train acc: 0.9507, val loss: 0.1275, val acc: 0.9582  (best train acc: 0.9610, best val acc: 0.9609, best train loss: 0.1087  @ epoch 18464 )\n",
      "[Epoch: 18560] train loss: 0.1183, train acc: 0.9583, val loss: 0.1445, val acc: 0.9518  (best train acc: 0.9610, best val acc: 0.9609, best train loss: 0.1087  @ epoch 18464 )\n",
      "[Epoch: 18580] train loss: 0.1357, train acc: 0.9472, val loss: 0.1345, val acc: 0.9551  (best train acc: 0.9610, best val acc: 0.9609, best train loss: 0.1083  @ epoch 18569 )\n",
      "[Epoch: 18600] train loss: 0.1364, train acc: 0.9494, val loss: 0.1518, val acc: 0.9437  (best train acc: 0.9610, best val acc: 0.9609, best train loss: 0.1083  @ epoch 18569 )\n",
      "[Epoch: 18620] train loss: 0.1197, train acc: 0.9542, val loss: 0.1305, val acc: 0.9582  (best train acc: 0.9610, best val acc: 0.9609, best train loss: 0.1083  @ epoch 18569 )\n",
      "[Epoch: 18640] train loss: 0.1362, train acc: 0.9474, val loss: 0.1476, val acc: 0.9477  (best train acc: 0.9610, best val acc: 0.9612, best train loss: 0.1083  @ epoch 18569 )\n",
      "[Epoch: 18660] train loss: 0.1367, train acc: 0.9453, val loss: 0.1266, val acc: 0.9575  (best train acc: 0.9610, best val acc: 0.9612, best train loss: 0.1083  @ epoch 18569 )\n",
      "[Epoch: 18680] train loss: 0.1349, train acc: 0.9464, val loss: 0.1334, val acc: 0.9535  (best train acc: 0.9610, best val acc: 0.9612, best train loss: 0.1083  @ epoch 18569 )\n",
      "[Epoch: 18700] train loss: 0.1407, train acc: 0.9434, val loss: 0.1341, val acc: 0.9538  (best train acc: 0.9610, best val acc: 0.9612, best train loss: 0.1083  @ epoch 18569 )\n",
      "[Epoch: 18720] train loss: 0.1207, train acc: 0.9543, val loss: 0.1302, val acc: 0.9568  (best train acc: 0.9610, best val acc: 0.9612, best train loss: 0.1083  @ epoch 18569 )\n",
      "[Epoch: 18740] train loss: 0.1326, train acc: 0.9487, val loss: 0.1404, val acc: 0.9558  (best train acc: 0.9610, best val acc: 0.9612, best train loss: 0.1083  @ epoch 18569 )\n",
      "[Epoch: 18760] train loss: 0.1389, train acc: 0.9506, val loss: 0.1642, val acc: 0.9403  (best train acc: 0.9610, best val acc: 0.9612, best train loss: 0.1083  @ epoch 18569 )\n",
      "[Epoch: 18780] train loss: 0.1164, train acc: 0.9547, val loss: 0.1372, val acc: 0.9545  (best train acc: 0.9610, best val acc: 0.9612, best train loss: 0.1083  @ epoch 18569 )\n",
      "[Epoch: 18800] train loss: 0.1303, train acc: 0.9476, val loss: 0.1461, val acc: 0.9484  (best train acc: 0.9610, best val acc: 0.9612, best train loss: 0.1080  @ epoch 18793 )\n",
      "[Epoch: 18820] train loss: 0.1351, train acc: 0.9508, val loss: 0.1527, val acc: 0.9491  (best train acc: 0.9610, best val acc: 0.9612, best train loss: 0.1080  @ epoch 18793 )\n",
      "[Epoch: 18840] train loss: 0.1351, train acc: 0.9492, val loss: 0.1434, val acc: 0.9511  (best train acc: 0.9610, best val acc: 0.9612, best train loss: 0.1080  @ epoch 18793 )\n",
      "[Epoch: 18860] train loss: 0.1546, train acc: 0.9434, val loss: 0.1720, val acc: 0.9363  (best train acc: 0.9610, best val acc: 0.9612, best train loss: 0.1080  @ epoch 18793 )\n",
      "[Epoch: 18880] train loss: 0.1243, train acc: 0.9527, val loss: 0.1453, val acc: 0.9497  (best train acc: 0.9610, best val acc: 0.9612, best train loss: 0.1080  @ epoch 18793 )\n",
      "[Epoch: 18900] train loss: 0.1205, train acc: 0.9524, val loss: 0.1635, val acc: 0.9390  (best train acc: 0.9610, best val acc: 0.9612, best train loss: 0.1080  @ epoch 18793 )\n",
      "[Epoch: 18920] train loss: 0.1179, train acc: 0.9579, val loss: 0.1434, val acc: 0.9528  (best train acc: 0.9610, best val acc: 0.9612, best train loss: 0.1080  @ epoch 18793 )\n",
      "[Epoch: 18940] train loss: 0.1511, train acc: 0.9479, val loss: 0.1666, val acc: 0.9481  (best train acc: 0.9610, best val acc: 0.9612, best train loss: 0.1080  @ epoch 18793 )\n",
      "[Epoch: 18960] train loss: 0.1134, train acc: 0.9566, val loss: 0.1519, val acc: 0.9474  (best train acc: 0.9610, best val acc: 0.9612, best train loss: 0.1080  @ epoch 18793 )\n",
      "[Epoch: 18980] train loss: 0.1233, train acc: 0.9519, val loss: 0.1600, val acc: 0.9450  (best train acc: 0.9610, best val acc: 0.9612, best train loss: 0.1080  @ epoch 18793 )\n",
      "[Epoch: 19000] train loss: 0.1198, train acc: 0.9553, val loss: 0.1421, val acc: 0.9538  (best train acc: 0.9610, best val acc: 0.9612, best train loss: 0.1080  @ epoch 18793 )\n",
      "[Epoch: 19020] train loss: 0.1269, train acc: 0.9534, val loss: 0.1345, val acc: 0.9578  (best train acc: 0.9610, best val acc: 0.9612, best train loss: 0.1080  @ epoch 18793 )\n",
      "[Epoch: 19040] train loss: 0.1273, train acc: 0.9525, val loss: 0.1329, val acc: 0.9589  (best train acc: 0.9610, best val acc: 0.9612, best train loss: 0.1080  @ epoch 18793 )\n",
      "[Epoch: 19060] train loss: 0.1211, train acc: 0.9537, val loss: 0.1333, val acc: 0.9565  (best train acc: 0.9610, best val acc: 0.9612, best train loss: 0.1080  @ epoch 18793 )\n",
      "[Epoch: 19080] train loss: 0.1194, train acc: 0.9552, val loss: 0.1334, val acc: 0.9541  (best train acc: 0.9610, best val acc: 0.9619, best train loss: 0.1080  @ epoch 18793 )\n",
      "[Epoch: 19100] train loss: 0.1230, train acc: 0.9526, val loss: 0.1465, val acc: 0.9541  (best train acc: 0.9610, best val acc: 0.9619, best train loss: 0.1080  @ epoch 18793 )\n",
      "[Epoch: 19120] train loss: 0.1225, train acc: 0.9552, val loss: 0.1380, val acc: 0.9578  (best train acc: 0.9610, best val acc: 0.9619, best train loss: 0.1080  @ epoch 18793 )\n",
      "[Epoch: 19140] train loss: 0.1169, train acc: 0.9589, val loss: 0.1424, val acc: 0.9541  (best train acc: 0.9610, best val acc: 0.9619, best train loss: 0.1080  @ epoch 18793 )\n",
      "[Epoch: 19160] train loss: 0.1231, train acc: 0.9520, val loss: 0.1363, val acc: 0.9558  (best train acc: 0.9610, best val acc: 0.9619, best train loss: 0.1080  @ epoch 18793 )\n",
      "[Epoch: 19180] train loss: 0.1184, train acc: 0.9536, val loss: 0.1290, val acc: 0.9585  (best train acc: 0.9610, best val acc: 0.9619, best train loss: 0.1080  @ epoch 18793 )\n",
      "[Epoch: 19200] train loss: 0.1156, train acc: 0.9560, val loss: 0.1582, val acc: 0.9474  (best train acc: 0.9610, best val acc: 0.9619, best train loss: 0.1080  @ epoch 18793 )\n",
      "[Epoch: 19220] train loss: 0.1230, train acc: 0.9512, val loss: 0.1506, val acc: 0.9464  (best train acc: 0.9610, best val acc: 0.9619, best train loss: 0.1080  @ epoch 18793 )\n",
      "[Epoch: 19240] train loss: 0.1270, train acc: 0.9472, val loss: 0.1509, val acc: 0.9484  (best train acc: 0.9610, best val acc: 0.9619, best train loss: 0.1080  @ epoch 18793 )\n",
      "[Epoch: 19260] train loss: 0.1194, train acc: 0.9534, val loss: 0.1489, val acc: 0.9481  (best train acc: 0.9610, best val acc: 0.9619, best train loss: 0.1080  @ epoch 18793 )\n",
      "[Epoch: 19280] train loss: 0.1256, train acc: 0.9521, val loss: 0.1433, val acc: 0.9538  (best train acc: 0.9610, best val acc: 0.9619, best train loss: 0.1080  @ epoch 18793 )\n",
      "[Epoch: 19300] train loss: 0.1165, train acc: 0.9556, val loss: 0.1561, val acc: 0.9460  (best train acc: 0.9610, best val acc: 0.9619, best train loss: 0.1080  @ epoch 18793 )\n",
      "[Epoch: 19320] train loss: 0.1161, train acc: 0.9559, val loss: 0.1517, val acc: 0.9504  (best train acc: 0.9610, best val acc: 0.9619, best train loss: 0.1080  @ epoch 18793 )\n",
      "[Epoch: 19340] train loss: 0.1177, train acc: 0.9560, val loss: 0.1318, val acc: 0.9572  (best train acc: 0.9610, best val acc: 0.9619, best train loss: 0.1080  @ epoch 18793 )\n",
      "[Epoch: 19360] train loss: 0.1202, train acc: 0.9544, val loss: 0.1374, val acc: 0.9524  (best train acc: 0.9610, best val acc: 0.9619, best train loss: 0.1080  @ epoch 18793 )\n",
      "[Epoch: 19380] train loss: 0.1234, train acc: 0.9558, val loss: 0.1435, val acc: 0.9528  (best train acc: 0.9610, best val acc: 0.9619, best train loss: 0.1070  @ epoch 19371 )\n",
      "[Epoch: 19400] train loss: 0.1352, train acc: 0.9503, val loss: 0.1440, val acc: 0.9487  (best train acc: 0.9610, best val acc: 0.9619, best train loss: 0.1070  @ epoch 19371 )\n",
      "[Epoch: 19420] train loss: 0.1162, train acc: 0.9553, val loss: 0.1347, val acc: 0.9551  (best train acc: 0.9610, best val acc: 0.9619, best train loss: 0.1070  @ epoch 19371 )\n",
      "[Epoch: 19440] train loss: 0.1272, train acc: 0.9531, val loss: 0.1482, val acc: 0.9467  (best train acc: 0.9610, best val acc: 0.9619, best train loss: 0.1070  @ epoch 19371 )\n",
      "[Epoch: 19460] train loss: 0.1364, train acc: 0.9489, val loss: 0.1628, val acc: 0.9457  (best train acc: 0.9610, best val acc: 0.9619, best train loss: 0.1070  @ epoch 19371 )\n",
      "[Epoch: 19480] train loss: 0.1398, train acc: 0.9444, val loss: 0.1922, val acc: 0.9349  (best train acc: 0.9610, best val acc: 0.9619, best train loss: 0.1070  @ epoch 19371 )\n",
      "[Epoch: 19500] train loss: 0.1633, train acc: 0.9497, val loss: 0.1359, val acc: 0.9595  (best train acc: 0.9610, best val acc: 0.9619, best train loss: 0.1070  @ epoch 19371 )\n",
      "[Epoch: 19520] train loss: 0.1146, train acc: 0.9552, val loss: 0.1450, val acc: 0.9535  (best train acc: 0.9610, best val acc: 0.9619, best train loss: 0.1070  @ epoch 19371 )\n",
      "[Epoch: 19540] train loss: 0.1342, train acc: 0.9473, val loss: 0.1436, val acc: 0.9504  (best train acc: 0.9619, best val acc: 0.9619, best train loss: 0.1070  @ epoch 19371 )\n",
      "[Epoch: 19560] train loss: 0.1220, train acc: 0.9537, val loss: 0.1599, val acc: 0.9481  (best train acc: 0.9619, best val acc: 0.9619, best train loss: 0.1070  @ epoch 19371 )\n",
      "[Epoch: 19580] train loss: 0.1258, train acc: 0.9480, val loss: 0.1408, val acc: 0.9521  (best train acc: 0.9619, best val acc: 0.9619, best train loss: 0.1055  @ epoch 19570 )\n",
      "[Epoch: 19600] train loss: 0.1249, train acc: 0.9508, val loss: 0.1633, val acc: 0.9491  (best train acc: 0.9619, best val acc: 0.9619, best train loss: 0.1055  @ epoch 19570 )\n",
      "[Epoch: 19620] train loss: 0.1249, train acc: 0.9513, val loss: 0.1381, val acc: 0.9548  (best train acc: 0.9619, best val acc: 0.9619, best train loss: 0.1055  @ epoch 19570 )\n",
      "[Epoch: 19640] train loss: 0.1144, train acc: 0.9563, val loss: 0.1294, val acc: 0.9585  (best train acc: 0.9619, best val acc: 0.9619, best train loss: 0.1055  @ epoch 19570 )\n",
      "[Epoch: 19660] train loss: 0.1230, train acc: 0.9542, val loss: 0.1510, val acc: 0.9501  (best train acc: 0.9619, best val acc: 0.9619, best train loss: 0.1055  @ epoch 19570 )\n",
      "[Epoch: 19680] train loss: 0.1311, train acc: 0.9473, val loss: 0.1541, val acc: 0.9501  (best train acc: 0.9619, best val acc: 0.9619, best train loss: 0.1055  @ epoch 19570 )\n",
      "[Epoch: 19700] train loss: 0.1212, train acc: 0.9565, val loss: 0.1569, val acc: 0.9457  (best train acc: 0.9619, best val acc: 0.9619, best train loss: 0.1055  @ epoch 19570 )\n",
      "[Epoch: 19720] train loss: 0.1132, train acc: 0.9582, val loss: 0.1359, val acc: 0.9589  (best train acc: 0.9619, best val acc: 0.9619, best train loss: 0.1055  @ epoch 19570 )\n",
      "[Epoch: 19740] train loss: 0.1217, train acc: 0.9553, val loss: 0.1688, val acc: 0.9420  (best train acc: 0.9619, best val acc: 0.9619, best train loss: 0.1055  @ epoch 19570 )\n",
      "[Epoch: 19760] train loss: 0.1142, train acc: 0.9584, val loss: 0.1499, val acc: 0.9454  (best train acc: 0.9619, best val acc: 0.9619, best train loss: 0.1055  @ epoch 19570 )\n",
      "[Epoch: 19780] train loss: 0.1267, train acc: 0.9549, val loss: 0.1356, val acc: 0.9545  (best train acc: 0.9619, best val acc: 0.9619, best train loss: 0.1039  @ epoch 19762 )\n",
      "[Epoch: 19800] train loss: 0.1155, train acc: 0.9545, val loss: 0.1274, val acc: 0.9592  (best train acc: 0.9619, best val acc: 0.9619, best train loss: 0.1039  @ epoch 19762 )\n",
      "[Epoch: 19820] train loss: 0.1300, train acc: 0.9507, val loss: 0.1522, val acc: 0.9514  (best train acc: 0.9619, best val acc: 0.9619, best train loss: 0.1039  @ epoch 19762 )\n",
      "[Epoch: 19840] train loss: 0.1047, train acc: 0.9607, val loss: 0.1326, val acc: 0.9609  (best train acc: 0.9619, best val acc: 0.9619, best train loss: 0.1039  @ epoch 19762 )\n",
      "[Epoch: 19860] train loss: 0.1177, train acc: 0.9549, val loss: 0.1244, val acc: 0.9605  (best train acc: 0.9619, best val acc: 0.9619, best train loss: 0.1039  @ epoch 19762 )\n",
      "[Epoch: 19880] train loss: 0.1196, train acc: 0.9541, val loss: 0.1409, val acc: 0.9545  (best train acc: 0.9619, best val acc: 0.9619, best train loss: 0.1039  @ epoch 19762 )\n",
      "[Epoch: 19900] train loss: 0.1159, train acc: 0.9593, val loss: 0.1523, val acc: 0.9528  (best train acc: 0.9619, best val acc: 0.9619, best train loss: 0.1039  @ epoch 19762 )\n",
      "[Epoch: 19920] train loss: 0.1200, train acc: 0.9535, val loss: 0.1442, val acc: 0.9545  (best train acc: 0.9620, best val acc: 0.9619, best train loss: 0.1039  @ epoch 19762 )\n",
      "[Epoch: 19940] train loss: 0.1222, train acc: 0.9573, val loss: 0.1428, val acc: 0.9528  (best train acc: 0.9620, best val acc: 0.9619, best train loss: 0.1039  @ epoch 19762 )\n",
      "[Epoch: 19960] train loss: 0.1128, train acc: 0.9580, val loss: 0.1615, val acc: 0.9437  (best train acc: 0.9620, best val acc: 0.9619, best train loss: 0.1039  @ epoch 19762 )\n",
      "[Epoch: 19980] train loss: 0.1247, train acc: 0.9529, val loss: 0.1550, val acc: 0.9467  (best train acc: 0.9620, best val acc: 0.9619, best train loss: 0.1039  @ epoch 19762 )\n",
      "[Epoch: 20000] train loss: 0.1232, train acc: 0.9560, val loss: 0.1424, val acc: 0.9589  (best train acc: 0.9620, best val acc: 0.9619, best train loss: 0.1039  @ epoch 19762 )\n",
      "[Epoch: 20020] train loss: 0.1251, train acc: 0.9532, val loss: 0.1271, val acc: 0.9572  (best train acc: 0.9620, best val acc: 0.9619, best train loss: 0.1039  @ epoch 19762 )\n",
      "[Epoch: 20040] train loss: 0.1173, train acc: 0.9579, val loss: 0.1588, val acc: 0.9481  (best train acc: 0.9620, best val acc: 0.9619, best train loss: 0.1039  @ epoch 19762 )\n",
      "[Epoch: 20060] train loss: 0.1094, train acc: 0.9592, val loss: 0.1570, val acc: 0.9487  (best train acc: 0.9620, best val acc: 0.9619, best train loss: 0.1039  @ epoch 19762 )\n",
      "[Epoch: 20080] train loss: 0.1182, train acc: 0.9563, val loss: 0.1429, val acc: 0.9538  (best train acc: 0.9620, best val acc: 0.9619, best train loss: 0.1039  @ epoch 19762 )\n",
      "[Epoch: 20100] train loss: 0.1267, train acc: 0.9541, val loss: 0.1512, val acc: 0.9524  (best train acc: 0.9620, best val acc: 0.9619, best train loss: 0.1039  @ epoch 19762 )\n",
      "[Epoch: 20120] train loss: 0.1211, train acc: 0.9558, val loss: 0.1732, val acc: 0.9400  (best train acc: 0.9620, best val acc: 0.9629, best train loss: 0.1039  @ epoch 19762 )\n",
      "[Epoch: 20140] train loss: 0.1191, train acc: 0.9549, val loss: 0.1398, val acc: 0.9612  (best train acc: 0.9620, best val acc: 0.9629, best train loss: 0.1039  @ epoch 19762 )\n",
      "[Epoch: 20160] train loss: 0.1233, train acc: 0.9544, val loss: 0.1359, val acc: 0.9578  (best train acc: 0.9620, best val acc: 0.9629, best train loss: 0.1039  @ epoch 19762 )\n",
      "[Epoch: 20180] train loss: 0.1095, train acc: 0.9581, val loss: 0.1328, val acc: 0.9582  (best train acc: 0.9620, best val acc: 0.9629, best train loss: 0.1039  @ epoch 19762 )\n",
      "[Epoch: 20200] train loss: 0.1115, train acc: 0.9605, val loss: 0.1422, val acc: 0.9585  (best train acc: 0.9636, best val acc: 0.9629, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20220] train loss: 0.1140, train acc: 0.9568, val loss: 0.1519, val acc: 0.9524  (best train acc: 0.9636, best val acc: 0.9629, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20240] train loss: 0.1122, train acc: 0.9598, val loss: 0.1341, val acc: 0.9599  (best train acc: 0.9636, best val acc: 0.9629, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20260] train loss: 0.1160, train acc: 0.9568, val loss: 0.1376, val acc: 0.9578  (best train acc: 0.9636, best val acc: 0.9629, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20280] train loss: 0.1101, train acc: 0.9589, val loss: 0.1404, val acc: 0.9565  (best train acc: 0.9636, best val acc: 0.9629, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20300] train loss: 0.1297, train acc: 0.9472, val loss: 0.1368, val acc: 0.9595  (best train acc: 0.9636, best val acc: 0.9629, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20320] train loss: 0.1398, train acc: 0.9456, val loss: 0.1618, val acc: 0.9501  (best train acc: 0.9636, best val acc: 0.9629, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20340] train loss: 0.1192, train acc: 0.9559, val loss: 0.1730, val acc: 0.9417  (best train acc: 0.9636, best val acc: 0.9629, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20360] train loss: 0.1324, train acc: 0.9468, val loss: 0.1749, val acc: 0.9477  (best train acc: 0.9636, best val acc: 0.9629, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20380] train loss: 0.1294, train acc: 0.9478, val loss: 0.1527, val acc: 0.9460  (best train acc: 0.9636, best val acc: 0.9629, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20400] train loss: 0.1338, train acc: 0.9482, val loss: 0.1697, val acc: 0.9470  (best train acc: 0.9636, best val acc: 0.9629, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20420] train loss: 0.1272, train acc: 0.9495, val loss: 0.1418, val acc: 0.9582  (best train acc: 0.9636, best val acc: 0.9629, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20440] train loss: 0.1197, train acc: 0.9555, val loss: 0.1394, val acc: 0.9562  (best train acc: 0.9636, best val acc: 0.9629, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20460] train loss: 0.1099, train acc: 0.9605, val loss: 0.1395, val acc: 0.9582  (best train acc: 0.9636, best val acc: 0.9629, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20480] train loss: 0.1229, train acc: 0.9524, val loss: 0.1445, val acc: 0.9572  (best train acc: 0.9636, best val acc: 0.9629, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20500] train loss: 0.1252, train acc: 0.9505, val loss: 0.1556, val acc: 0.9535  (best train acc: 0.9636, best val acc: 0.9629, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20520] train loss: 0.1259, train acc: 0.9526, val loss: 0.1520, val acc: 0.9535  (best train acc: 0.9636, best val acc: 0.9629, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20540] train loss: 0.1130, train acc: 0.9557, val loss: 0.1363, val acc: 0.9589  (best train acc: 0.9636, best val acc: 0.9629, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20560] train loss: 0.1090, train acc: 0.9599, val loss: 0.1362, val acc: 0.9605  (best train acc: 0.9636, best val acc: 0.9629, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20580] train loss: 0.1163, train acc: 0.9580, val loss: 0.1660, val acc: 0.9467  (best train acc: 0.9636, best val acc: 0.9629, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20600] train loss: 0.1097, train acc: 0.9559, val loss: 0.1378, val acc: 0.9585  (best train acc: 0.9636, best val acc: 0.9629, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20620] train loss: 0.1210, train acc: 0.9555, val loss: 0.1416, val acc: 0.9632  (best train acc: 0.9636, best val acc: 0.9632, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20640] train loss: 0.1299, train acc: 0.9464, val loss: 0.1424, val acc: 0.9568  (best train acc: 0.9636, best val acc: 0.9632, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20660] train loss: 0.1308, train acc: 0.9516, val loss: 0.1525, val acc: 0.9518  (best train acc: 0.9636, best val acc: 0.9632, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20680] train loss: 0.1237, train acc: 0.9566, val loss: 0.1607, val acc: 0.9504  (best train acc: 0.9636, best val acc: 0.9632, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20700] train loss: 0.1143, train acc: 0.9557, val loss: 0.1436, val acc: 0.9585  (best train acc: 0.9636, best val acc: 0.9632, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20720] train loss: 0.1116, train acc: 0.9597, val loss: 0.1393, val acc: 0.9605  (best train acc: 0.9636, best val acc: 0.9632, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20740] train loss: 0.1383, train acc: 0.9448, val loss: 0.1829, val acc: 0.9417  (best train acc: 0.9636, best val acc: 0.9632, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20760] train loss: 0.1261, train acc: 0.9563, val loss: 0.1437, val acc: 0.9572  (best train acc: 0.9636, best val acc: 0.9632, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20780] train loss: 0.1325, train acc: 0.9526, val loss: 0.1496, val acc: 0.9538  (best train acc: 0.9636, best val acc: 0.9632, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20800] train loss: 0.1292, train acc: 0.9506, val loss: 0.1729, val acc: 0.9413  (best train acc: 0.9636, best val acc: 0.9632, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20820] train loss: 0.1273, train acc: 0.9524, val loss: 0.1449, val acc: 0.9599  (best train acc: 0.9636, best val acc: 0.9632, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20840] train loss: 0.1090, train acc: 0.9612, val loss: 0.1608, val acc: 0.9457  (best train acc: 0.9636, best val acc: 0.9632, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20860] train loss: 0.1228, train acc: 0.9550, val loss: 0.1416, val acc: 0.9562  (best train acc: 0.9636, best val acc: 0.9632, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20880] train loss: 0.1124, train acc: 0.9583, val loss: 0.1472, val acc: 0.9565  (best train acc: 0.9636, best val acc: 0.9632, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20900] train loss: 0.1197, train acc: 0.9545, val loss: 0.1467, val acc: 0.9589  (best train acc: 0.9636, best val acc: 0.9632, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20920] train loss: 0.1172, train acc: 0.9553, val loss: 0.1470, val acc: 0.9565  (best train acc: 0.9636, best val acc: 0.9632, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20940] train loss: 0.1173, train acc: 0.9570, val loss: 0.1358, val acc: 0.9602  (best train acc: 0.9636, best val acc: 0.9632, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20960] train loss: 0.1102, train acc: 0.9579, val loss: 0.1502, val acc: 0.9541  (best train acc: 0.9636, best val acc: 0.9632, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 20980] train loss: 0.1204, train acc: 0.9565, val loss: 0.1354, val acc: 0.9562  (best train acc: 0.9636, best val acc: 0.9632, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 21000] train loss: 0.1182, train acc: 0.9556, val loss: 0.1393, val acc: 0.9609  (best train acc: 0.9636, best val acc: 0.9632, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 21020] train loss: 0.1159, train acc: 0.9581, val loss: 0.1459, val acc: 0.9511  (best train acc: 0.9636, best val acc: 0.9642, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 21040] train loss: 0.1315, train acc: 0.9466, val loss: 0.2398, val acc: 0.9228  (best train acc: 0.9636, best val acc: 0.9642, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 21060] train loss: 0.1185, train acc: 0.9573, val loss: 0.1465, val acc: 0.9518  (best train acc: 0.9636, best val acc: 0.9642, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 21080] train loss: 0.1252, train acc: 0.9514, val loss: 0.1498, val acc: 0.9585  (best train acc: 0.9636, best val acc: 0.9642, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 21100] train loss: 0.1089, train acc: 0.9599, val loss: 0.1432, val acc: 0.9585  (best train acc: 0.9636, best val acc: 0.9642, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 21120] train loss: 0.1182, train acc: 0.9569, val loss: 0.1379, val acc: 0.9599  (best train acc: 0.9636, best val acc: 0.9642, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 21140] train loss: 0.1142, train acc: 0.9576, val loss: 0.1511, val acc: 0.9545  (best train acc: 0.9636, best val acc: 0.9642, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 21160] train loss: 0.1607, train acc: 0.9378, val loss: 0.1472, val acc: 0.9518  (best train acc: 0.9636, best val acc: 0.9642, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 21180] train loss: 0.1136, train acc: 0.9576, val loss: 0.1437, val acc: 0.9592  (best train acc: 0.9636, best val acc: 0.9642, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 21200] train loss: 0.1329, train acc: 0.9473, val loss: 0.1360, val acc: 0.9605  (best train acc: 0.9636, best val acc: 0.9642, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 21220] train loss: 0.1089, train acc: 0.9605, val loss: 0.1433, val acc: 0.9551  (best train acc: 0.9636, best val acc: 0.9642, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 21240] train loss: 0.1012, train acc: 0.9637, val loss: 0.1419, val acc: 0.9612  (best train acc: 0.9637, best val acc: 0.9642, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 21260] train loss: 0.1178, train acc: 0.9559, val loss: 0.1460, val acc: 0.9585  (best train acc: 0.9637, best val acc: 0.9642, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 21280] train loss: 0.1142, train acc: 0.9576, val loss: 0.1462, val acc: 0.9524  (best train acc: 0.9637, best val acc: 0.9642, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 21300] train loss: 0.1227, train acc: 0.9541, val loss: 0.1389, val acc: 0.9619  (best train acc: 0.9637, best val acc: 0.9642, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 21320] train loss: 0.1178, train acc: 0.9581, val loss: 0.1439, val acc: 0.9578  (best train acc: 0.9637, best val acc: 0.9642, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 21340] train loss: 0.1207, train acc: 0.9542, val loss: 0.1547, val acc: 0.9487  (best train acc: 0.9637, best val acc: 0.9642, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 21360] train loss: 0.1128, train acc: 0.9571, val loss: 0.1810, val acc: 0.9420  (best train acc: 0.9637, best val acc: 0.9642, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 21380] train loss: 0.1043, train acc: 0.9588, val loss: 0.1335, val acc: 0.9548  (best train acc: 0.9637, best val acc: 0.9642, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 21400] train loss: 0.1191, train acc: 0.9542, val loss: 0.1528, val acc: 0.9497  (best train acc: 0.9637, best val acc: 0.9642, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 21420] train loss: 0.1111, train acc: 0.9602, val loss: 0.1424, val acc: 0.9531  (best train acc: 0.9637, best val acc: 0.9642, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 21440] train loss: 0.1116, train acc: 0.9582, val loss: 0.1496, val acc: 0.9548  (best train acc: 0.9637, best val acc: 0.9642, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 21460] train loss: 0.1182, train acc: 0.9527, val loss: 0.1627, val acc: 0.9531  (best train acc: 0.9637, best val acc: 0.9642, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 21480] train loss: 0.1122, train acc: 0.9593, val loss: 0.1412, val acc: 0.9609  (best train acc: 0.9637, best val acc: 0.9642, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 21500] train loss: 0.1534, train acc: 0.9455, val loss: 0.1558, val acc: 0.9514  (best train acc: 0.9637, best val acc: 0.9642, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 21520] train loss: 0.1043, train acc: 0.9631, val loss: 0.1462, val acc: 0.9589  (best train acc: 0.9637, best val acc: 0.9642, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 21540] train loss: 0.1169, train acc: 0.9605, val loss: 0.1395, val acc: 0.9599  (best train acc: 0.9637, best val acc: 0.9642, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 21560] train loss: 0.1404, train acc: 0.9456, val loss: 0.1612, val acc: 0.9565  (best train acc: 0.9637, best val acc: 0.9642, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 21580] train loss: 0.1164, train acc: 0.9559, val loss: 0.1607, val acc: 0.9548  (best train acc: 0.9637, best val acc: 0.9642, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 21600] train loss: 0.1159, train acc: 0.9573, val loss: 0.1768, val acc: 0.9470  (best train acc: 0.9637, best val acc: 0.9642, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 21620] train loss: 0.1225, train acc: 0.9537, val loss: 0.1856, val acc: 0.9356  (best train acc: 0.9637, best val acc: 0.9642, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 21640] train loss: 0.1226, train acc: 0.9529, val loss: 0.1821, val acc: 0.9430  (best train acc: 0.9637, best val acc: 0.9642, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 21660] train loss: 0.1036, train acc: 0.9628, val loss: 0.1435, val acc: 0.9551  (best train acc: 0.9637, best val acc: 0.9642, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 21680] train loss: 0.1230, train acc: 0.9524, val loss: 0.1866, val acc: 0.9396  (best train acc: 0.9637, best val acc: 0.9642, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 21700] train loss: 0.1151, train acc: 0.9563, val loss: 0.1683, val acc: 0.9457  (best train acc: 0.9637, best val acc: 0.9642, best train loss: 0.1003  @ epoch 20182 )\n",
      "[Epoch: 21720] train loss: 0.1113, train acc: 0.9576, val loss: 0.1452, val acc: 0.9535  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0984  @ epoch 21712 )\n",
      "[Epoch: 21740] train loss: 0.1124, train acc: 0.9589, val loss: 0.1364, val acc: 0.9605  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0984  @ epoch 21712 )\n",
      "[Epoch: 21760] train loss: 0.1184, train acc: 0.9526, val loss: 0.1401, val acc: 0.9582  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0984  @ epoch 21712 )\n",
      "[Epoch: 21780] train loss: 0.1116, train acc: 0.9600, val loss: 0.1416, val acc: 0.9612  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0984  @ epoch 21712 )\n",
      "[Epoch: 21800] train loss: 0.1310, train acc: 0.9493, val loss: 0.1852, val acc: 0.9430  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0984  @ epoch 21712 )\n",
      "[Epoch: 21820] train loss: 0.1156, train acc: 0.9575, val loss: 0.1433, val acc: 0.9582  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0984  @ epoch 21712 )\n",
      "[Epoch: 21840] train loss: 0.1070, train acc: 0.9602, val loss: 0.1384, val acc: 0.9599  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0984  @ epoch 21712 )\n",
      "[Epoch: 21860] train loss: 0.1055, train acc: 0.9615, val loss: 0.1535, val acc: 0.9562  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0984  @ epoch 21712 )\n",
      "[Epoch: 21880] train loss: 0.1102, train acc: 0.9612, val loss: 0.1359, val acc: 0.9595  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0984  @ epoch 21712 )\n",
      "[Epoch: 21900] train loss: 0.1187, train acc: 0.9552, val loss: 0.1543, val acc: 0.9568  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0984  @ epoch 21712 )\n",
      "[Epoch: 21920] train loss: 0.1105, train acc: 0.9573, val loss: 0.1499, val acc: 0.9548  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0984  @ epoch 21712 )\n",
      "[Epoch: 21940] train loss: 0.1036, train acc: 0.9602, val loss: 0.1567, val acc: 0.9511  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0984  @ epoch 21712 )\n",
      "[Epoch: 21960] train loss: 0.1135, train acc: 0.9579, val loss: 0.1590, val acc: 0.9545  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0984  @ epoch 21712 )\n",
      "[Epoch: 21980] train loss: 0.1086, train acc: 0.9606, val loss: 0.1545, val acc: 0.9504  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0984  @ epoch 21712 )\n",
      "[Epoch: 22000] train loss: 0.1140, train acc: 0.9550, val loss: 0.1446, val acc: 0.9626  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0984  @ epoch 21712 )\n",
      "[Epoch: 22020] train loss: 0.1089, train acc: 0.9590, val loss: 0.1650, val acc: 0.9504  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0984  @ epoch 21712 )\n",
      "[Epoch: 22040] train loss: 0.1149, train acc: 0.9574, val loss: 0.1601, val acc: 0.9508  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0984  @ epoch 21712 )\n",
      "[Epoch: 22060] train loss: 0.1132, train acc: 0.9556, val loss: 0.1689, val acc: 0.9487  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0984  @ epoch 21712 )\n",
      "[Epoch: 22080] train loss: 0.1219, train acc: 0.9550, val loss: 0.1754, val acc: 0.9467  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0984  @ epoch 21712 )\n",
      "[Epoch: 22100] train loss: 0.1064, train acc: 0.9592, val loss: 0.1417, val acc: 0.9585  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0984  @ epoch 21712 )\n",
      "[Epoch: 22120] train loss: 0.1151, train acc: 0.9550, val loss: 0.1641, val acc: 0.9514  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0984  @ epoch 21712 )\n",
      "[Epoch: 22140] train loss: 0.1415, train acc: 0.9498, val loss: 0.1627, val acc: 0.9528  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0984  @ epoch 21712 )\n",
      "[Epoch: 22160] train loss: 0.0998, train acc: 0.9641, val loss: 0.1377, val acc: 0.9589  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0984  @ epoch 21712 )\n",
      "[Epoch: 22180] train loss: 0.1119, train acc: 0.9602, val loss: 0.1589, val acc: 0.9487  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0984  @ epoch 21712 )\n",
      "[Epoch: 22200] train loss: 0.1149, train acc: 0.9584, val loss: 0.1514, val acc: 0.9531  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0984  @ epoch 21712 )\n",
      "[Epoch: 22220] train loss: 0.1296, train acc: 0.9526, val loss: 0.1525, val acc: 0.9514  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0984  @ epoch 21712 )\n",
      "[Epoch: 22240] train loss: 0.1336, train acc: 0.9508, val loss: 0.1452, val acc: 0.9599  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0984  @ epoch 21712 )\n",
      "[Epoch: 22260] train loss: 0.1017, train acc: 0.9603, val loss: 0.1409, val acc: 0.9602  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0984  @ epoch 21712 )\n",
      "[Epoch: 22280] train loss: 0.1057, train acc: 0.9581, val loss: 0.1675, val acc: 0.9535  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0984  @ epoch 21712 )\n",
      "[Epoch: 22300] train loss: 0.1124, train acc: 0.9592, val loss: 0.1497, val acc: 0.9589  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0984  @ epoch 21712 )\n",
      "[Epoch: 22320] train loss: 0.1303, train acc: 0.9471, val loss: 0.1480, val acc: 0.9572  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0984  @ epoch 21712 )\n",
      "[Epoch: 22340] train loss: 0.1123, train acc: 0.9566, val loss: 0.1585, val acc: 0.9585  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0974  @ epoch 22328 )\n",
      "[Epoch: 22360] train loss: 0.1210, train acc: 0.9582, val loss: 0.1454, val acc: 0.9572  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0974  @ epoch 22328 )\n",
      "[Epoch: 22380] train loss: 0.1104, train acc: 0.9616, val loss: 0.1490, val acc: 0.9592  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0974  @ epoch 22328 )\n",
      "[Epoch: 22400] train loss: 0.1116, train acc: 0.9580, val loss: 0.1367, val acc: 0.9585  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0974  @ epoch 22328 )\n",
      "[Epoch: 22420] train loss: 0.1303, train acc: 0.9506, val loss: 0.1499, val acc: 0.9558  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0974  @ epoch 22328 )\n",
      "[Epoch: 22440] train loss: 0.1292, train acc: 0.9514, val loss: 0.1651, val acc: 0.9531  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0974  @ epoch 22328 )\n",
      "[Epoch: 22460] train loss: 0.0974, train acc: 0.9650, val loss: 0.1358, val acc: 0.9609  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0974  @ epoch 22328 )\n",
      "[Epoch: 22480] train loss: 0.1242, train acc: 0.9557, val loss: 0.1515, val acc: 0.9585  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0974  @ epoch 22328 )\n",
      "[Epoch: 22500] train loss: 0.1215, train acc: 0.9516, val loss: 0.1524, val acc: 0.9538  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0974  @ epoch 22328 )\n",
      "[Epoch: 22520] train loss: 0.1263, train acc: 0.9566, val loss: 0.1762, val acc: 0.9390  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0954  @ epoch 22509 )\n",
      "[Epoch: 22540] train loss: 0.1148, train acc: 0.9553, val loss: 0.1579, val acc: 0.9535  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0954  @ epoch 22509 )\n",
      "[Epoch: 22560] train loss: 0.1171, train acc: 0.9608, val loss: 0.1467, val acc: 0.9568  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0954  @ epoch 22509 )\n",
      "[Epoch: 22580] train loss: 0.1068, train acc: 0.9592, val loss: 0.1474, val acc: 0.9551  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0954  @ epoch 22509 )\n",
      "[Epoch: 22600] train loss: 0.1125, train acc: 0.9594, val loss: 0.1708, val acc: 0.9501  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0954  @ epoch 22509 )\n",
      "[Epoch: 22620] train loss: 0.1148, train acc: 0.9544, val loss: 0.1490, val acc: 0.9545  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0954  @ epoch 22509 )\n",
      "[Epoch: 22640] train loss: 0.1231, train acc: 0.9492, val loss: 0.1957, val acc: 0.9376  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0954  @ epoch 22509 )\n",
      "[Epoch: 22660] train loss: 0.1104, train acc: 0.9580, val loss: 0.1433, val acc: 0.9545  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0954  @ epoch 22509 )\n",
      "[Epoch: 22680] train loss: 0.1044, train acc: 0.9610, val loss: 0.1737, val acc: 0.9460  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0954  @ epoch 22509 )\n",
      "[Epoch: 22700] train loss: 0.1217, train acc: 0.9538, val loss: 0.1536, val acc: 0.9572  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0954  @ epoch 22509 )\n",
      "[Epoch: 22720] train loss: 0.1274, train acc: 0.9539, val loss: 0.1463, val acc: 0.9578  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0954  @ epoch 22509 )\n",
      "[Epoch: 22740] train loss: 0.1092, train acc: 0.9594, val loss: 0.1554, val acc: 0.9582  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0954  @ epoch 22509 )\n",
      "[Epoch: 22760] train loss: 0.1132, train acc: 0.9566, val loss: 0.1691, val acc: 0.9508  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0954  @ epoch 22509 )\n",
      "[Epoch: 22780] train loss: 0.1062, train acc: 0.9620, val loss: 0.1537, val acc: 0.9541  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0954  @ epoch 22509 )\n",
      "[Epoch: 22800] train loss: 0.1114, train acc: 0.9594, val loss: 0.1602, val acc: 0.9514  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0954  @ epoch 22509 )\n",
      "[Epoch: 22820] train loss: 0.1031, train acc: 0.9609, val loss: 0.1492, val acc: 0.9551  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0954  @ epoch 22509 )\n",
      "[Epoch: 22840] train loss: 0.1067, train acc: 0.9588, val loss: 0.1504, val acc: 0.9545  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0954  @ epoch 22509 )\n",
      "[Epoch: 22860] train loss: 0.1176, train acc: 0.9503, val loss: 0.1418, val acc: 0.9589  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0954  @ epoch 22509 )\n",
      "[Epoch: 22880] train loss: 0.1080, train acc: 0.9560, val loss: 0.1522, val acc: 0.9538  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0954  @ epoch 22509 )\n",
      "[Epoch: 22900] train loss: 0.1062, train acc: 0.9599, val loss: 0.1438, val acc: 0.9602  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0954  @ epoch 22509 )\n",
      "[Epoch: 22920] train loss: 0.1259, train acc: 0.9550, val loss: 0.1411, val acc: 0.9565  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0954  @ epoch 22509 )\n",
      "[Epoch: 22940] train loss: 0.1150, train acc: 0.9568, val loss: 0.1404, val acc: 0.9592  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0954  @ epoch 22509 )\n",
      "[Epoch: 22960] train loss: 0.1217, train acc: 0.9579, val loss: 0.1535, val acc: 0.9551  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0954  @ epoch 22509 )\n",
      "[Epoch: 22980] train loss: 0.1088, train acc: 0.9602, val loss: 0.1525, val acc: 0.9589  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0954  @ epoch 22509 )\n",
      "[Epoch: 23000] train loss: 0.1055, train acc: 0.9607, val loss: 0.1479, val acc: 0.9585  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0954  @ epoch 22509 )\n",
      "[Epoch: 23020] train loss: 0.1114, train acc: 0.9598, val loss: 0.1528, val acc: 0.9524  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0954  @ epoch 22509 )\n",
      "[Epoch: 23040] train loss: 0.1395, train acc: 0.9482, val loss: 0.1522, val acc: 0.9589  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0954  @ epoch 22509 )\n",
      "[Epoch: 23060] train loss: 0.1103, train acc: 0.9596, val loss: 0.1426, val acc: 0.9565  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0950  @ epoch 23049 )\n",
      "[Epoch: 23080] train loss: 0.1063, train acc: 0.9593, val loss: 0.1585, val acc: 0.9524  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0950  @ epoch 23049 )\n",
      "[Epoch: 23100] train loss: 0.1382, train acc: 0.9447, val loss: 0.1568, val acc: 0.9582  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0950  @ epoch 23049 )\n",
      "[Epoch: 23120] train loss: 0.1034, train acc: 0.9616, val loss: 0.1585, val acc: 0.9558  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0950  @ epoch 23049 )\n",
      "[Epoch: 23140] train loss: 0.1142, train acc: 0.9589, val loss: 0.1395, val acc: 0.9626  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0950  @ epoch 23049 )\n",
      "[Epoch: 23160] train loss: 0.1204, train acc: 0.9547, val loss: 0.1562, val acc: 0.9562  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0950  @ epoch 23049 )\n",
      "[Epoch: 23180] train loss: 0.0982, train acc: 0.9631, val loss: 0.1389, val acc: 0.9575  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0950  @ epoch 23049 )\n",
      "[Epoch: 23200] train loss: 0.0965, train acc: 0.9618, val loss: 0.1493, val acc: 0.9602  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0950  @ epoch 23049 )\n",
      "[Epoch: 23220] train loss: 0.1045, train acc: 0.9621, val loss: 0.1496, val acc: 0.9524  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0950  @ epoch 23049 )\n",
      "[Epoch: 23240] train loss: 0.1225, train acc: 0.9552, val loss: 0.1512, val acc: 0.9558  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0950  @ epoch 23049 )\n",
      "[Epoch: 23260] train loss: 0.1114, train acc: 0.9597, val loss: 0.1389, val acc: 0.9599  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0950  @ epoch 23049 )\n",
      "[Epoch: 23280] train loss: 0.1016, train acc: 0.9640, val loss: 0.1380, val acc: 0.9575  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0950  @ epoch 23049 )\n",
      "[Epoch: 23300] train loss: 0.1233, train acc: 0.9504, val loss: 0.1558, val acc: 0.9548  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0950  @ epoch 23049 )\n",
      "[Epoch: 23320] train loss: 0.1099, train acc: 0.9551, val loss: 0.1382, val acc: 0.9589  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0950  @ epoch 23049 )\n",
      "[Epoch: 23340] train loss: 0.1129, train acc: 0.9566, val loss: 0.1488, val acc: 0.9582  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0950  @ epoch 23049 )\n",
      "[Epoch: 23360] train loss: 0.1045, train acc: 0.9616, val loss: 0.1646, val acc: 0.9528  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0941  @ epoch 23355 )\n",
      "[Epoch: 23380] train loss: 0.1014, train acc: 0.9611, val loss: 0.1610, val acc: 0.9484  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0941  @ epoch 23355 )\n",
      "[Epoch: 23400] train loss: 0.1145, train acc: 0.9571, val loss: 0.1501, val acc: 0.9551  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0941  @ epoch 23355 )\n",
      "[Epoch: 23420] train loss: 0.1083, train acc: 0.9591, val loss: 0.1402, val acc: 0.9599  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0941  @ epoch 23355 )\n",
      "[Epoch: 23440] train loss: 0.1162, train acc: 0.9559, val loss: 0.1341, val acc: 0.9602  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0941  @ epoch 23355 )\n",
      "[Epoch: 23460] train loss: 0.1077, train acc: 0.9583, val loss: 0.1659, val acc: 0.9470  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0941  @ epoch 23355 )\n",
      "[Epoch: 23480] train loss: 0.1197, train acc: 0.9560, val loss: 0.1620, val acc: 0.9528  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0941  @ epoch 23355 )\n",
      "[Epoch: 23500] train loss: 0.1225, train acc: 0.9563, val loss: 0.1801, val acc: 0.9332  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0941  @ epoch 23355 )\n",
      "[Epoch: 23520] train loss: 0.1127, train acc: 0.9581, val loss: 0.1380, val acc: 0.9572  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0941  @ epoch 23355 )\n",
      "[Epoch: 23540] train loss: 0.1072, train acc: 0.9599, val loss: 0.1583, val acc: 0.9518  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0941  @ epoch 23355 )\n",
      "[Epoch: 23560] train loss: 0.1357, train acc: 0.9494, val loss: 0.1499, val acc: 0.9545  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0941  @ epoch 23355 )\n",
      "[Epoch: 23580] train loss: 0.1661, train acc: 0.9432, val loss: 0.1649, val acc: 0.9558  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0941  @ epoch 23355 )\n",
      "[Epoch: 23600] train loss: 0.1303, train acc: 0.9505, val loss: 0.1358, val acc: 0.9595  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0941  @ epoch 23355 )\n",
      "[Epoch: 23620] train loss: 0.1195, train acc: 0.9552, val loss: 0.1410, val acc: 0.9578  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0941  @ epoch 23355 )\n",
      "[Epoch: 23640] train loss: 0.1092, train acc: 0.9560, val loss: 0.1552, val acc: 0.9514  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0941  @ epoch 23355 )\n",
      "[Epoch: 23660] train loss: 0.1071, train acc: 0.9598, val loss: 0.1501, val acc: 0.9558  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0941  @ epoch 23355 )\n",
      "[Epoch: 23680] train loss: 0.1076, train acc: 0.9596, val loss: 0.1759, val acc: 0.9444  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0941  @ epoch 23355 )\n",
      "[Epoch: 23700] train loss: 0.1064, train acc: 0.9591, val loss: 0.1398, val acc: 0.9568  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0941  @ epoch 23355 )\n",
      "[Epoch: 23720] train loss: 0.1148, train acc: 0.9565, val loss: 0.1439, val acc: 0.9585  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0941  @ epoch 23355 )\n",
      "[Epoch: 23740] train loss: 0.0999, train acc: 0.9614, val loss: 0.1737, val acc: 0.9494  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0941  @ epoch 23355 )\n",
      "[Epoch: 23760] train loss: 0.1134, train acc: 0.9550, val loss: 0.1567, val acc: 0.9518  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0941  @ epoch 23355 )\n",
      "[Epoch: 23780] train loss: 0.1087, train acc: 0.9631, val loss: 0.1794, val acc: 0.9444  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0941  @ epoch 23355 )\n",
      "[Epoch: 23800] train loss: 0.1174, train acc: 0.9566, val loss: 0.1584, val acc: 0.9568  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0941  @ epoch 23355 )\n",
      "[Epoch: 23820] train loss: 0.1073, train acc: 0.9592, val loss: 0.1564, val acc: 0.9551  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0941  @ epoch 23355 )\n",
      "[Epoch: 23840] train loss: 0.1070, train acc: 0.9599, val loss: 0.1731, val acc: 0.9433  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0941  @ epoch 23355 )\n",
      "[Epoch: 23860] train loss: 0.1042, train acc: 0.9612, val loss: 0.1412, val acc: 0.9592  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0941  @ epoch 23355 )\n",
      "[Epoch: 23880] train loss: 0.0988, train acc: 0.9635, val loss: 0.1455, val acc: 0.9568  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0941  @ epoch 23355 )\n",
      "[Epoch: 23900] train loss: 0.0982, train acc: 0.9646, val loss: 0.1549, val acc: 0.9568  (best train acc: 0.9661, best val acc: 0.9642, best train loss: 0.0941  @ epoch 23355 )\n",
      "[Epoch: 23920] train loss: 0.1031, train acc: 0.9610, val loss: 0.1408, val acc: 0.9565  (best train acc: 0.9665, best val acc: 0.9642, best train loss: 0.0928  @ epoch 23911 )\n",
      "[Epoch: 23940] train loss: 0.0991, train acc: 0.9645, val loss: 0.1557, val acc: 0.9531  (best train acc: 0.9665, best val acc: 0.9642, best train loss: 0.0928  @ epoch 23911 )\n",
      "[Epoch: 23960] train loss: 0.1052, train acc: 0.9602, val loss: 0.1625, val acc: 0.9528  (best train acc: 0.9665, best val acc: 0.9642, best train loss: 0.0928  @ epoch 23911 )\n",
      "[Epoch: 23980] train loss: 0.1082, train acc: 0.9626, val loss: 0.1444, val acc: 0.9531  (best train acc: 0.9665, best val acc: 0.9642, best train loss: 0.0928  @ epoch 23911 )\n",
      "[Epoch: 24000] train loss: 0.0955, train acc: 0.9638, val loss: 0.1446, val acc: 0.9609  (best train acc: 0.9665, best val acc: 0.9642, best train loss: 0.0928  @ epoch 23911 )\n",
      "[Epoch: 24020] train loss: 0.1080, train acc: 0.9616, val loss: 0.1420, val acc: 0.9535  (best train acc: 0.9665, best val acc: 0.9642, best train loss: 0.0928  @ epoch 23911 )\n",
      "[Epoch: 24040] train loss: 0.1207, train acc: 0.9524, val loss: 0.1399, val acc: 0.9592  (best train acc: 0.9665, best val acc: 0.9642, best train loss: 0.0928  @ epoch 23911 )\n",
      "[Epoch: 24060] train loss: 0.1165, train acc: 0.9542, val loss: 0.1518, val acc: 0.9521  (best train acc: 0.9665, best val acc: 0.9642, best train loss: 0.0928  @ epoch 23911 )\n",
      "[Epoch: 24080] train loss: 0.1109, train acc: 0.9603, val loss: 0.1549, val acc: 0.9528  (best train acc: 0.9665, best val acc: 0.9642, best train loss: 0.0928  @ epoch 23911 )\n",
      "[Epoch: 24100] train loss: 0.1045, train acc: 0.9628, val loss: 0.1405, val acc: 0.9585  (best train acc: 0.9665, best val acc: 0.9642, best train loss: 0.0928  @ epoch 23911 )\n",
      "[Epoch: 24120] train loss: 0.1017, train acc: 0.9638, val loss: 0.1396, val acc: 0.9589  (best train acc: 0.9665, best val acc: 0.9642, best train loss: 0.0928  @ epoch 23911 )\n",
      "[Epoch: 24140] train loss: 0.0948, train acc: 0.9653, val loss: 0.1444, val acc: 0.9555  (best train acc: 0.9665, best val acc: 0.9642, best train loss: 0.0928  @ epoch 23911 )\n",
      "[Epoch: 24160] train loss: 0.1030, train acc: 0.9628, val loss: 0.1408, val acc: 0.9535  (best train acc: 0.9665, best val acc: 0.9642, best train loss: 0.0928  @ epoch 23911 )\n",
      "[Epoch: 24180] train loss: 0.1093, train acc: 0.9592, val loss: 0.1440, val acc: 0.9562  (best train acc: 0.9665, best val acc: 0.9642, best train loss: 0.0928  @ epoch 23911 )\n",
      "[Epoch: 24200] train loss: 0.1028, train acc: 0.9631, val loss: 0.1457, val acc: 0.9589  (best train acc: 0.9665, best val acc: 0.9642, best train loss: 0.0928  @ epoch 23911 )\n",
      "[Epoch: 24220] train loss: 0.1020, train acc: 0.9633, val loss: 0.1413, val acc: 0.9595  (best train acc: 0.9665, best val acc: 0.9642, best train loss: 0.0928  @ epoch 23911 )\n",
      "[Epoch: 24240] train loss: 0.0991, train acc: 0.9647, val loss: 0.1498, val acc: 0.9575  (best train acc: 0.9665, best val acc: 0.9642, best train loss: 0.0928  @ epoch 23911 )\n",
      "[Epoch: 24260] train loss: 0.1187, train acc: 0.9570, val loss: 0.1391, val acc: 0.9605  (best train acc: 0.9665, best val acc: 0.9642, best train loss: 0.0928  @ epoch 23911 )\n",
      "[Epoch: 24280] train loss: 0.1154, train acc: 0.9563, val loss: 0.1628, val acc: 0.9450  (best train acc: 0.9665, best val acc: 0.9642, best train loss: 0.0928  @ epoch 23911 )\n",
      "[Epoch: 24300] train loss: 0.1015, train acc: 0.9598, val loss: 0.1769, val acc: 0.9470  (best train acc: 0.9665, best val acc: 0.9642, best train loss: 0.0928  @ epoch 23911 )\n",
      "[Epoch: 24320] train loss: 0.1175, train acc: 0.9561, val loss: 0.1646, val acc: 0.9514  (best train acc: 0.9665, best val acc: 0.9642, best train loss: 0.0928  @ epoch 23911 )\n",
      "[Epoch: 24340] train loss: 0.0951, train acc: 0.9631, val loss: 0.1422, val acc: 0.9609  (best train acc: 0.9665, best val acc: 0.9642, best train loss: 0.0928  @ epoch 23911 )\n",
      "[Epoch: 24360] train loss: 0.1151, train acc: 0.9543, val loss: 0.1545, val acc: 0.9551  (best train acc: 0.9665, best val acc: 0.9642, best train loss: 0.0928  @ epoch 23911 )\n",
      "[Epoch: 24380] train loss: 0.1128, train acc: 0.9608, val loss: 0.1451, val acc: 0.9555  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 24400] train loss: 0.1008, train acc: 0.9630, val loss: 0.1391, val acc: 0.9612  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 24420] train loss: 0.1423, train acc: 0.9492, val loss: 0.1587, val acc: 0.9541  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 24440] train loss: 0.1115, train acc: 0.9596, val loss: 0.1396, val acc: 0.9548  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 24460] train loss: 0.1215, train acc: 0.9521, val loss: 0.1478, val acc: 0.9595  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 24480] train loss: 0.1050, train acc: 0.9624, val loss: 0.1557, val acc: 0.9548  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 24500] train loss: 0.1100, train acc: 0.9570, val loss: 0.1413, val acc: 0.9575  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 24520] train loss: 0.1163, train acc: 0.9511, val loss: 0.1506, val acc: 0.9565  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 24540] train loss: 0.1021, train acc: 0.9602, val loss: 0.1615, val acc: 0.9548  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 24560] train loss: 0.1302, train acc: 0.9511, val loss: 0.1503, val acc: 0.9565  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 24580] train loss: 0.1106, train acc: 0.9560, val loss: 0.1649, val acc: 0.9508  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 24600] train loss: 0.1143, train acc: 0.9535, val loss: 0.1521, val acc: 0.9535  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 24620] train loss: 0.1136, train acc: 0.9586, val loss: 0.1459, val acc: 0.9592  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 24640] train loss: 0.1106, train acc: 0.9576, val loss: 0.1400, val acc: 0.9636  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 24660] train loss: 0.1278, train acc: 0.9526, val loss: 0.1632, val acc: 0.9491  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 24680] train loss: 0.1053, train acc: 0.9612, val loss: 0.1477, val acc: 0.9582  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 24700] train loss: 0.1058, train acc: 0.9574, val loss: 0.1741, val acc: 0.9460  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 24720] train loss: 0.1117, train acc: 0.9589, val loss: 0.1510, val acc: 0.9582  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 24740] train loss: 0.1044, train acc: 0.9626, val loss: 0.1715, val acc: 0.9494  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 24760] train loss: 0.1112, train acc: 0.9583, val loss: 0.1346, val acc: 0.9565  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 24780] train loss: 0.0930, train acc: 0.9649, val loss: 0.1491, val acc: 0.9551  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 24800] train loss: 0.1027, train acc: 0.9614, val loss: 0.1327, val acc: 0.9612  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 24820] train loss: 0.1368, train acc: 0.9500, val loss: 0.1596, val acc: 0.9538  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 24840] train loss: 0.1129, train acc: 0.9563, val loss: 0.1393, val acc: 0.9575  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 24860] train loss: 0.1131, train acc: 0.9547, val loss: 0.1351, val acc: 0.9575  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 24880] train loss: 0.1024, train acc: 0.9610, val loss: 0.1639, val acc: 0.9521  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 24900] train loss: 0.1204, train acc: 0.9589, val loss: 0.1523, val acc: 0.9548  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 24920] train loss: 0.1027, train acc: 0.9609, val loss: 0.1488, val acc: 0.9572  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 24940] train loss: 0.1037, train acc: 0.9631, val loss: 0.1523, val acc: 0.9572  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 24960] train loss: 0.1120, train acc: 0.9544, val loss: 0.1393, val acc: 0.9612  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 24980] train loss: 0.1026, train acc: 0.9631, val loss: 0.1645, val acc: 0.9491  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 25000] train loss: 0.1095, train acc: 0.9560, val loss: 0.1353, val acc: 0.9626  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 25020] train loss: 0.1092, train acc: 0.9559, val loss: 0.1653, val acc: 0.9457  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 25040] train loss: 0.1133, train acc: 0.9549, val loss: 0.1434, val acc: 0.9592  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 25060] train loss: 0.0917, train acc: 0.9652, val loss: 0.1431, val acc: 0.9558  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 25080] train loss: 0.0921, train acc: 0.9662, val loss: 0.1445, val acc: 0.9589  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 25100] train loss: 0.0994, train acc: 0.9628, val loss: 0.1417, val acc: 0.9541  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 25120] train loss: 0.1064, train acc: 0.9633, val loss: 0.1316, val acc: 0.9599  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 25140] train loss: 0.1129, train acc: 0.9587, val loss: 0.1598, val acc: 0.9524  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 25160] train loss: 0.1138, train acc: 0.9558, val loss: 0.1397, val acc: 0.9562  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 25180] train loss: 0.1005, train acc: 0.9626, val loss: 0.1392, val acc: 0.9575  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 25200] train loss: 0.1049, train acc: 0.9612, val loss: 0.1382, val acc: 0.9572  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 25220] train loss: 0.1083, train acc: 0.9612, val loss: 0.1600, val acc: 0.9484  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 25240] train loss: 0.1001, train acc: 0.9641, val loss: 0.1566, val acc: 0.9551  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 25260] train loss: 0.0988, train acc: 0.9625, val loss: 0.1322, val acc: 0.9565  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 25280] train loss: 0.1045, train acc: 0.9585, val loss: 0.1450, val acc: 0.9548  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 25300] train loss: 0.1127, train acc: 0.9539, val loss: 0.1652, val acc: 0.9511  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 25320] train loss: 0.1134, train acc: 0.9575, val loss: 0.1359, val acc: 0.9585  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 25340] train loss: 0.0983, train acc: 0.9674, val loss: 0.1545, val acc: 0.9568  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 25360] train loss: 0.1125, train acc: 0.9576, val loss: 0.1800, val acc: 0.9481  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 25380] train loss: 0.1051, train acc: 0.9620, val loss: 0.1499, val acc: 0.9497  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 25400] train loss: 0.1136, train acc: 0.9591, val loss: 0.1525, val acc: 0.9562  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 25420] train loss: 0.1191, train acc: 0.9553, val loss: 0.1681, val acc: 0.9477  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 25440] train loss: 0.1035, train acc: 0.9597, val loss: 0.1281, val acc: 0.9592  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 25460] train loss: 0.1097, train acc: 0.9589, val loss: 0.1602, val acc: 0.9518  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 25480] train loss: 0.1010, train acc: 0.9600, val loss: 0.1419, val acc: 0.9578  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 25500] train loss: 0.0917, train acc: 0.9651, val loss: 0.1566, val acc: 0.9518  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 25520] train loss: 0.1223, train acc: 0.9550, val loss: 0.1532, val acc: 0.9514  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 25540] train loss: 0.1028, train acc: 0.9605, val loss: 0.1545, val acc: 0.9501  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 25560] train loss: 0.1054, train acc: 0.9620, val loss: 0.1446, val acc: 0.9562  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0867  @ epoch 24368 )\n",
      "[Epoch: 25580] train loss: 0.1115, train acc: 0.9547, val loss: 0.1812, val acc: 0.9447  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0856  @ epoch 25570 )\n",
      "[Epoch: 25600] train loss: 0.0945, train acc: 0.9632, val loss: 0.1354, val acc: 0.9575  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0856  @ epoch 25570 )\n",
      "[Epoch: 25620] train loss: 0.0984, train acc: 0.9610, val loss: 0.1503, val acc: 0.9528  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0856  @ epoch 25570 )\n",
      "[Epoch: 25640] train loss: 0.0975, train acc: 0.9629, val loss: 0.1289, val acc: 0.9609  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0856  @ epoch 25570 )\n",
      "[Epoch: 25660] train loss: 0.1041, train acc: 0.9616, val loss: 0.1483, val acc: 0.9582  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0856  @ epoch 25570 )\n",
      "[Epoch: 25680] train loss: 0.1025, train acc: 0.9597, val loss: 0.1567, val acc: 0.9481  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0856  @ epoch 25570 )\n",
      "[Epoch: 25700] train loss: 0.0936, train acc: 0.9636, val loss: 0.1479, val acc: 0.9545  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0856  @ epoch 25570 )\n",
      "[Epoch: 25720] train loss: 0.1029, train acc: 0.9594, val loss: 0.1617, val acc: 0.9531  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0856  @ epoch 25570 )\n",
      "[Epoch: 25740] train loss: 0.1023, train acc: 0.9628, val loss: 0.1349, val acc: 0.9602  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0856  @ epoch 25570 )\n",
      "[Epoch: 25760] train loss: 0.1114, train acc: 0.9612, val loss: 0.1422, val acc: 0.9531  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0856  @ epoch 25570 )\n",
      "[Epoch: 25780] train loss: 0.0944, train acc: 0.9653, val loss: 0.1509, val acc: 0.9541  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0856  @ epoch 25570 )\n",
      "[Epoch: 25800] train loss: 0.0988, train acc: 0.9596, val loss: 0.1608, val acc: 0.9528  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0856  @ epoch 25570 )\n",
      "[Epoch: 25820] train loss: 0.1088, train acc: 0.9604, val loss: 0.1412, val acc: 0.9572  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0856  @ epoch 25570 )\n",
      "[Epoch: 25840] train loss: 0.1043, train acc: 0.9592, val loss: 0.1466, val acc: 0.9558  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0856  @ epoch 25570 )\n",
      "[Epoch: 25860] train loss: 0.1114, train acc: 0.9571, val loss: 0.1503, val acc: 0.9551  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0856  @ epoch 25570 )\n",
      "[Epoch: 25880] train loss: 0.1237, train acc: 0.9545, val loss: 0.1792, val acc: 0.9430  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0856  @ epoch 25570 )\n",
      "[Epoch: 25900] train loss: 0.1021, train acc: 0.9605, val loss: 0.1447, val acc: 0.9545  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0856  @ epoch 25570 )\n",
      "[Epoch: 25920] train loss: 0.0982, train acc: 0.9633, val loss: 0.1419, val acc: 0.9568  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0856  @ epoch 25570 )\n",
      "[Epoch: 25940] train loss: 0.0988, train acc: 0.9626, val loss: 0.1516, val acc: 0.9551  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0856  @ epoch 25570 )\n",
      "[Epoch: 25960] train loss: 0.1028, train acc: 0.9612, val loss: 0.1351, val acc: 0.9568  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0856  @ epoch 25570 )\n",
      "[Epoch: 25980] train loss: 0.0940, train acc: 0.9673, val loss: 0.1793, val acc: 0.9410  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0856  @ epoch 25570 )\n",
      "[Epoch: 26000] train loss: 0.0965, train acc: 0.9659, val loss: 0.1437, val acc: 0.9572  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0856  @ epoch 25570 )\n",
      "[Epoch: 26020] train loss: 0.0966, train acc: 0.9644, val loss: 0.1446, val acc: 0.9531  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0856  @ epoch 25570 )\n",
      "[Epoch: 26040] train loss: 0.0952, train acc: 0.9646, val loss: 0.1680, val acc: 0.9481  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0856  @ epoch 25570 )\n",
      "[Epoch: 26060] train loss: 0.0977, train acc: 0.9623, val loss: 0.1383, val acc: 0.9551  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0856  @ epoch 25570 )\n",
      "[Epoch: 26080] train loss: 0.1028, train acc: 0.9593, val loss: 0.1636, val acc: 0.9484  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0856  @ epoch 25570 )\n",
      "[Epoch: 26100] train loss: 0.1262, train acc: 0.9516, val loss: 0.1410, val acc: 0.9551  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0856  @ epoch 25570 )\n",
      "[Epoch: 26120] train loss: 0.0965, train acc: 0.9641, val loss: 0.1300, val acc: 0.9585  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0856  @ epoch 25570 )\n",
      "[Epoch: 26140] train loss: 0.1033, train acc: 0.9594, val loss: 0.1434, val acc: 0.9558  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0856  @ epoch 25570 )\n",
      "[Epoch: 26160] train loss: 0.1020, train acc: 0.9646, val loss: 0.1584, val acc: 0.9501  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0856  @ epoch 25570 )\n",
      "[Epoch: 26180] train loss: 0.1198, train acc: 0.9584, val loss: 0.1458, val acc: 0.9548  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0856  @ epoch 25570 )\n",
      "[Epoch: 26200] train loss: 0.0995, train acc: 0.9637, val loss: 0.1579, val acc: 0.9497  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0856  @ epoch 25570 )\n",
      "[Epoch: 26220] train loss: 0.0995, train acc: 0.9633, val loss: 0.1306, val acc: 0.9609  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0856  @ epoch 25570 )\n",
      "[Epoch: 26240] train loss: 0.0997, train acc: 0.9626, val loss: 0.1379, val acc: 0.9555  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0856  @ epoch 25570 )\n",
      "[Epoch: 26260] train loss: 0.0966, train acc: 0.9656, val loss: 0.1408, val acc: 0.9595  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0856  @ epoch 25570 )\n",
      "[Epoch: 26280] train loss: 0.0881, train acc: 0.9670, val loss: 0.1578, val acc: 0.9528  (best train acc: 0.9689, best val acc: 0.9642, best train loss: 0.0856  @ epoch 25570 )\n",
      "[Epoch: 26300] train loss: 0.1016, train acc: 0.9631, val loss: 0.1447, val acc: 0.9585  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 26320] train loss: 0.0914, train acc: 0.9667, val loss: 0.1593, val acc: 0.9528  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 26340] train loss: 0.0912, train acc: 0.9630, val loss: 0.1491, val acc: 0.9521  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 26360] train loss: 0.1125, train acc: 0.9571, val loss: 0.1653, val acc: 0.9497  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 26380] train loss: 0.0973, train acc: 0.9629, val loss: 0.1411, val acc: 0.9578  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 26400] train loss: 0.0975, train acc: 0.9602, val loss: 0.1406, val acc: 0.9575  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 26420] train loss: 0.1172, train acc: 0.9564, val loss: 0.1424, val acc: 0.9589  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 26440] train loss: 0.0964, train acc: 0.9667, val loss: 0.1398, val acc: 0.9568  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 26460] train loss: 0.0963, train acc: 0.9652, val loss: 0.1327, val acc: 0.9602  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 26480] train loss: 0.0905, train acc: 0.9663, val loss: 0.1459, val acc: 0.9605  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 26500] train loss: 0.0938, train acc: 0.9649, val loss: 0.1472, val acc: 0.9555  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 26520] train loss: 0.0941, train acc: 0.9632, val loss: 0.1429, val acc: 0.9558  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 26540] train loss: 0.1087, train acc: 0.9567, val loss: 0.1361, val acc: 0.9555  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 26560] train loss: 0.1100, train acc: 0.9570, val loss: 0.1643, val acc: 0.9514  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 26580] train loss: 0.1308, train acc: 0.9517, val loss: 0.1301, val acc: 0.9589  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 26600] train loss: 0.1007, train acc: 0.9633, val loss: 0.1452, val acc: 0.9535  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 26620] train loss: 0.0941, train acc: 0.9653, val loss: 0.1531, val acc: 0.9524  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 26640] train loss: 0.1167, train acc: 0.9561, val loss: 0.1401, val acc: 0.9568  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 26660] train loss: 0.1101, train acc: 0.9560, val loss: 0.1479, val acc: 0.9558  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 26680] train loss: 0.1145, train acc: 0.9569, val loss: 0.1412, val acc: 0.9538  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 26700] train loss: 0.1082, train acc: 0.9595, val loss: 0.1585, val acc: 0.9487  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 26720] train loss: 0.0926, train acc: 0.9637, val loss: 0.1861, val acc: 0.9349  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 26740] train loss: 0.0958, train acc: 0.9645, val loss: 0.1412, val acc: 0.9575  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 26760] train loss: 0.1206, train acc: 0.9571, val loss: 0.1556, val acc: 0.9531  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 26780] train loss: 0.1004, train acc: 0.9616, val loss: 0.1602, val acc: 0.9491  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 26800] train loss: 0.1015, train acc: 0.9602, val loss: 0.1707, val acc: 0.9501  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 26820] train loss: 0.1053, train acc: 0.9614, val loss: 0.1459, val acc: 0.9528  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 26840] train loss: 0.0949, train acc: 0.9672, val loss: 0.1388, val acc: 0.9555  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 26860] train loss: 0.0923, train acc: 0.9632, val loss: 0.1319, val acc: 0.9605  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 26880] train loss: 0.0985, train acc: 0.9618, val loss: 0.1632, val acc: 0.9531  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 26900] train loss: 0.0932, train acc: 0.9623, val loss: 0.1496, val acc: 0.9551  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 26920] train loss: 0.1045, train acc: 0.9594, val loss: 0.1351, val acc: 0.9568  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 26940] train loss: 0.1178, train acc: 0.9532, val loss: 0.1416, val acc: 0.9555  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 26960] train loss: 0.0950, train acc: 0.9646, val loss: 0.1525, val acc: 0.9521  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 26980] train loss: 0.0914, train acc: 0.9655, val loss: 0.1393, val acc: 0.9572  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 27000] train loss: 0.1073, train acc: 0.9587, val loss: 0.1742, val acc: 0.9460  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 27020] train loss: 0.1250, train acc: 0.9505, val loss: 0.1587, val acc: 0.9457  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 27040] train loss: 0.0882, train acc: 0.9654, val loss: 0.1379, val acc: 0.9575  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 27060] train loss: 0.1204, train acc: 0.9524, val loss: 0.1363, val acc: 0.9595  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 27080] train loss: 0.0919, train acc: 0.9657, val loss: 0.1585, val acc: 0.9531  (best train acc: 0.9699, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 27100] train loss: 0.0948, train acc: 0.9637, val loss: 0.1519, val acc: 0.9535  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 27120] train loss: 0.0913, train acc: 0.9632, val loss: 0.1395, val acc: 0.9555  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 27140] train loss: 0.1303, train acc: 0.9547, val loss: 0.1485, val acc: 0.9558  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 27160] train loss: 0.0947, train acc: 0.9646, val loss: 0.1634, val acc: 0.9484  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 27180] train loss: 0.1135, train acc: 0.9603, val loss: 0.1387, val acc: 0.9572  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 27200] train loss: 0.1031, train acc: 0.9583, val loss: 0.1474, val acc: 0.9562  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 27220] train loss: 0.1099, train acc: 0.9578, val loss: 0.1618, val acc: 0.9494  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 27240] train loss: 0.1080, train acc: 0.9568, val loss: 0.1519, val acc: 0.9578  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 27260] train loss: 0.1056, train acc: 0.9588, val loss: 0.1347, val acc: 0.9592  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 27280] train loss: 0.1000, train acc: 0.9606, val loss: 0.1584, val acc: 0.9551  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 27300] train loss: 0.0939, train acc: 0.9644, val loss: 0.1767, val acc: 0.9444  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 27320] train loss: 0.0908, train acc: 0.9631, val loss: 0.1774, val acc: 0.9430  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 27340] train loss: 0.0973, train acc: 0.9606, val loss: 0.1590, val acc: 0.9508  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 27360] train loss: 0.0962, train acc: 0.9635, val loss: 0.1552, val acc: 0.9568  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 27380] train loss: 0.0959, train acc: 0.9645, val loss: 0.1433, val acc: 0.9572  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 27400] train loss: 0.1004, train acc: 0.9597, val loss: 0.1591, val acc: 0.9528  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 27420] train loss: 0.1091, train acc: 0.9605, val loss: 0.1665, val acc: 0.9528  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 27440] train loss: 0.1077, train acc: 0.9576, val loss: 0.1415, val acc: 0.9572  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 27460] train loss: 0.0878, train acc: 0.9678, val loss: 0.1656, val acc: 0.9460  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 27480] train loss: 0.0930, train acc: 0.9652, val loss: 0.1433, val acc: 0.9548  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 27500] train loss: 0.0905, train acc: 0.9655, val loss: 0.1344, val acc: 0.9592  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 27520] train loss: 0.0930, train acc: 0.9639, val loss: 0.1562, val acc: 0.9487  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 27540] train loss: 0.0915, train acc: 0.9652, val loss: 0.1510, val acc: 0.9589  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 27560] train loss: 0.1037, train acc: 0.9582, val loss: 0.1435, val acc: 0.9578  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 27580] train loss: 0.0906, train acc: 0.9654, val loss: 0.1499, val acc: 0.9565  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 27600] train loss: 0.0900, train acc: 0.9651, val loss: 0.1536, val acc: 0.9558  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 27620] train loss: 0.0908, train acc: 0.9650, val loss: 0.1488, val acc: 0.9555  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 27640] train loss: 0.1020, train acc: 0.9598, val loss: 0.1692, val acc: 0.9467  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0826  @ epoch 26282 )\n",
      "[Epoch: 27660] train loss: 0.0882, train acc: 0.9679, val loss: 0.1480, val acc: 0.9568  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0819  @ epoch 27653 )\n",
      "[Epoch: 27680] train loss: 0.1004, train acc: 0.9617, val loss: 0.1359, val acc: 0.9568  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0819  @ epoch 27653 )\n",
      "[Epoch: 27700] train loss: 0.0906, train acc: 0.9670, val loss: 0.1569, val acc: 0.9494  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0819  @ epoch 27653 )\n",
      "[Epoch: 27720] train loss: 0.0922, train acc: 0.9656, val loss: 0.1342, val acc: 0.9572  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0819  @ epoch 27653 )\n",
      "[Epoch: 27740] train loss: 0.0960, train acc: 0.9602, val loss: 0.1490, val acc: 0.9568  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0819  @ epoch 27653 )\n",
      "[Epoch: 27760] train loss: 0.1031, train acc: 0.9607, val loss: 0.1427, val acc: 0.9568  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0819  @ epoch 27653 )\n",
      "[Epoch: 27780] train loss: 0.1133, train acc: 0.9587, val loss: 0.1487, val acc: 0.9497  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0819  @ epoch 27653 )\n",
      "[Epoch: 27800] train loss: 0.0928, train acc: 0.9672, val loss: 0.1399, val acc: 0.9585  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0819  @ epoch 27653 )\n",
      "[Epoch: 27820] train loss: 0.1027, train acc: 0.9584, val loss: 0.1745, val acc: 0.9481  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0819  @ epoch 27653 )\n",
      "[Epoch: 27840] train loss: 0.0947, train acc: 0.9620, val loss: 0.1397, val acc: 0.9578  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0819  @ epoch 27653 )\n",
      "[Epoch: 27860] train loss: 0.0942, train acc: 0.9635, val loss: 0.1339, val acc: 0.9589  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0819  @ epoch 27653 )\n",
      "[Epoch: 27880] train loss: 0.0964, train acc: 0.9621, val loss: 0.1315, val acc: 0.9582  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0819  @ epoch 27653 )\n",
      "[Epoch: 27900] train loss: 0.0998, train acc: 0.9651, val loss: 0.1342, val acc: 0.9582  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0819  @ epoch 27653 )\n",
      "[Epoch: 27920] train loss: 0.0897, train acc: 0.9654, val loss: 0.1353, val acc: 0.9575  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0819  @ epoch 27653 )\n",
      "[Epoch: 27940] train loss: 0.0975, train acc: 0.9637, val loss: 0.1418, val acc: 0.9585  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0819  @ epoch 27653 )\n",
      "[Epoch: 27960] train loss: 0.1192, train acc: 0.9560, val loss: 0.1454, val acc: 0.9562  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0819  @ epoch 27653 )\n",
      "[Epoch: 27980] train loss: 0.0874, train acc: 0.9677, val loss: 0.1322, val acc: 0.9538  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0819  @ epoch 27653 )\n",
      "[Epoch: 28000] train loss: 0.0924, train acc: 0.9662, val loss: 0.1646, val acc: 0.9497  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0819  @ epoch 27653 )\n",
      "[Epoch: 28020] train loss: 0.0956, train acc: 0.9621, val loss: 0.1473, val acc: 0.9565  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0819  @ epoch 27653 )\n",
      "[Epoch: 28040] train loss: 0.1019, train acc: 0.9599, val loss: 0.1479, val acc: 0.9541  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0819  @ epoch 27653 )\n",
      "[Epoch: 28060] train loss: 0.0859, train acc: 0.9669, val loss: 0.1356, val acc: 0.9602  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0819  @ epoch 27653 )\n",
      "[Epoch: 28080] train loss: 0.0968, train acc: 0.9628, val loss: 0.1538, val acc: 0.9511  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0819  @ epoch 27653 )\n",
      "[Epoch: 28100] train loss: 0.0898, train acc: 0.9670, val loss: 0.1582, val acc: 0.9551  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0819  @ epoch 27653 )\n",
      "[Epoch: 28120] train loss: 0.0875, train acc: 0.9685, val loss: 0.1306, val acc: 0.9582  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0819  @ epoch 27653 )\n",
      "[Epoch: 28140] train loss: 0.1077, train acc: 0.9559, val loss: 0.1362, val acc: 0.9572  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0819  @ epoch 27653 )\n",
      "[Epoch: 28160] train loss: 0.0984, train acc: 0.9635, val loss: 0.1401, val acc: 0.9605  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0819  @ epoch 27653 )\n",
      "[Epoch: 28180] train loss: 0.0969, train acc: 0.9653, val loss: 0.1468, val acc: 0.9555  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0819  @ epoch 27653 )\n",
      "[Epoch: 28200] train loss: 0.0806, train acc: 0.9688, val loss: 0.1461, val acc: 0.9589  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0806  @ epoch 28200 )\n",
      "[Epoch: 28220] train loss: 0.0954, train acc: 0.9628, val loss: 0.1580, val acc: 0.9504  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0806  @ epoch 28200 )\n",
      "[Epoch: 28240] train loss: 0.0877, train acc: 0.9664, val loss: 0.1482, val acc: 0.9568  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0806  @ epoch 28200 )\n",
      "[Epoch: 28260] train loss: 0.1001, train acc: 0.9618, val loss: 0.1434, val acc: 0.9541  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0806  @ epoch 28200 )\n",
      "[Epoch: 28280] train loss: 0.0961, train acc: 0.9623, val loss: 0.1519, val acc: 0.9545  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0806  @ epoch 28200 )\n",
      "[Epoch: 28300] train loss: 0.1031, train acc: 0.9586, val loss: 0.1481, val acc: 0.9551  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0806  @ epoch 28200 )\n",
      "[Epoch: 28320] train loss: 0.0874, train acc: 0.9687, val loss: 0.1434, val acc: 0.9575  (best train acc: 0.9709, best val acc: 0.9642, best train loss: 0.0806  @ epoch 28200 )\n",
      "[Epoch: 28340] train loss: 0.0928, train acc: 0.9675, val loss: 0.1770, val acc: 0.9474  (best train acc: 0.9719, best val acc: 0.9642, best train loss: 0.0806  @ epoch 28200 )\n",
      "[Epoch: 28360] train loss: 0.0955, train acc: 0.9643, val loss: 0.1524, val acc: 0.9551  (best train acc: 0.9719, best val acc: 0.9642, best train loss: 0.0806  @ epoch 28200 )\n",
      "[Epoch: 28380] train loss: 0.0969, train acc: 0.9608, val loss: 0.1514, val acc: 0.9531  (best train acc: 0.9719, best val acc: 0.9642, best train loss: 0.0806  @ epoch 28200 )\n",
      "[Epoch: 28400] train loss: 0.1015, train acc: 0.9615, val loss: 0.1524, val acc: 0.9531  (best train acc: 0.9719, best val acc: 0.9642, best train loss: 0.0806  @ epoch 28200 )\n",
      "[Epoch: 28420] train loss: 0.1062, train acc: 0.9586, val loss: 0.1529, val acc: 0.9568  (best train acc: 0.9719, best val acc: 0.9642, best train loss: 0.0806  @ epoch 28200 )\n",
      "[Epoch: 28440] train loss: 0.0884, train acc: 0.9673, val loss: 0.1377, val acc: 0.9562  (best train acc: 0.9719, best val acc: 0.9642, best train loss: 0.0806  @ epoch 28200 )\n",
      "[Epoch: 28460] train loss: 0.0954, train acc: 0.9654, val loss: 0.1390, val acc: 0.9578  (best train acc: 0.9719, best val acc: 0.9642, best train loss: 0.0806  @ epoch 28200 )\n",
      "[Epoch: 28480] train loss: 0.0878, train acc: 0.9671, val loss: 0.1474, val acc: 0.9575  (best train acc: 0.9719, best val acc: 0.9642, best train loss: 0.0806  @ epoch 28200 )\n",
      "[Epoch: 28500] train loss: 0.0979, train acc: 0.9626, val loss: 0.1619, val acc: 0.9528  (best train acc: 0.9719, best val acc: 0.9642, best train loss: 0.0806  @ epoch 28200 )\n",
      "[Epoch: 28520] train loss: 0.0983, train acc: 0.9619, val loss: 0.1406, val acc: 0.9578  (best train acc: 0.9719, best val acc: 0.9642, best train loss: 0.0806  @ epoch 28200 )\n",
      "[Epoch: 28540] train loss: 0.0979, train acc: 0.9631, val loss: 0.1643, val acc: 0.9484  (best train acc: 0.9719, best val acc: 0.9642, best train loss: 0.0806  @ epoch 28200 )\n",
      "[Epoch: 28560] train loss: 0.0884, train acc: 0.9690, val loss: 0.1515, val acc: 0.9528  (best train acc: 0.9719, best val acc: 0.9642, best train loss: 0.0806  @ epoch 28200 )\n",
      "[Epoch: 28580] train loss: 0.0880, train acc: 0.9675, val loss: 0.1421, val acc: 0.9585  (best train acc: 0.9719, best val acc: 0.9642, best train loss: 0.0806  @ epoch 28200 )\n",
      "[Epoch: 28600] train loss: 0.0957, train acc: 0.9642, val loss: 0.1300, val acc: 0.9578  (best train acc: 0.9719, best val acc: 0.9642, best train loss: 0.0800  @ epoch 28594 )\n",
      "[Epoch: 28620] train loss: 0.1349, train acc: 0.9529, val loss: 0.1523, val acc: 0.9504  (best train acc: 0.9719, best val acc: 0.9642, best train loss: 0.0800  @ epoch 28594 )\n",
      "[Epoch: 28640] train loss: 0.0910, train acc: 0.9651, val loss: 0.1432, val acc: 0.9548  (best train acc: 0.9719, best val acc: 0.9642, best train loss: 0.0800  @ epoch 28594 )\n",
      "[Epoch: 28660] train loss: 0.0857, train acc: 0.9677, val loss: 0.1453, val acc: 0.9524  (best train acc: 0.9719, best val acc: 0.9642, best train loss: 0.0800  @ epoch 28594 )\n",
      "[Epoch: 28680] train loss: 0.0923, train acc: 0.9639, val loss: 0.1455, val acc: 0.9555  (best train acc: 0.9719, best val acc: 0.9642, best train loss: 0.0800  @ epoch 28594 )\n",
      "[Epoch: 28700] train loss: 0.0825, train acc: 0.9701, val loss: 0.1462, val acc: 0.9551  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0800  @ epoch 28594 )\n",
      "[Epoch: 28720] train loss: 0.0824, train acc: 0.9692, val loss: 0.1369, val acc: 0.9551  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0800  @ epoch 28594 )\n",
      "[Epoch: 28740] train loss: 0.1061, train acc: 0.9609, val loss: 0.1547, val acc: 0.9545  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0800  @ epoch 28594 )\n",
      "[Epoch: 28760] train loss: 0.0971, train acc: 0.9658, val loss: 0.1324, val acc: 0.9585  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0800  @ epoch 28594 )\n",
      "[Epoch: 28780] train loss: 0.0860, train acc: 0.9663, val loss: 0.1488, val acc: 0.9521  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0800  @ epoch 28594 )\n",
      "[Epoch: 28800] train loss: 0.0860, train acc: 0.9661, val loss: 0.1613, val acc: 0.9497  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0800  @ epoch 28594 )\n",
      "[Epoch: 28820] train loss: 0.1205, train acc: 0.9550, val loss: 0.1511, val acc: 0.9562  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0800  @ epoch 28594 )\n",
      "[Epoch: 28840] train loss: 0.0881, train acc: 0.9675, val loss: 0.1420, val acc: 0.9585  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0798  @ epoch 28830 )\n",
      "[Epoch: 28860] train loss: 0.1054, train acc: 0.9597, val loss: 0.1634, val acc: 0.9454  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0798  @ epoch 28830 )\n",
      "[Epoch: 28880] train loss: 0.1224, train acc: 0.9585, val loss: 0.1536, val acc: 0.9511  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0798  @ epoch 28830 )\n",
      "[Epoch: 28900] train loss: 0.0909, train acc: 0.9688, val loss: 0.1560, val acc: 0.9528  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0798  @ epoch 28830 )\n",
      "[Epoch: 28920] train loss: 0.0898, train acc: 0.9661, val loss: 0.1443, val acc: 0.9592  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0798  @ epoch 28830 )\n",
      "[Epoch: 28940] train loss: 0.0909, train acc: 0.9667, val loss: 0.1405, val acc: 0.9585  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0796  @ epoch 28932 )\n",
      "[Epoch: 28960] train loss: 0.0910, train acc: 0.9662, val loss: 0.1327, val acc: 0.9568  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0796  @ epoch 28932 )\n",
      "[Epoch: 28980] train loss: 0.1001, train acc: 0.9645, val loss: 0.1693, val acc: 0.9454  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0796  @ epoch 28932 )\n",
      "[Epoch: 29000] train loss: 0.0940, train acc: 0.9647, val loss: 0.1596, val acc: 0.9508  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0796  @ epoch 28932 )\n",
      "[Epoch: 29020] train loss: 0.0923, train acc: 0.9652, val loss: 0.1452, val acc: 0.9619  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0796  @ epoch 28932 )\n",
      "[Epoch: 29040] train loss: 0.0818, train acc: 0.9697, val loss: 0.1711, val acc: 0.9504  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0796  @ epoch 28932 )\n",
      "[Epoch: 29060] train loss: 0.1020, train acc: 0.9631, val loss: 0.1428, val acc: 0.9595  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0796  @ epoch 28932 )\n",
      "[Epoch: 29080] train loss: 0.0922, train acc: 0.9649, val loss: 0.1612, val acc: 0.9518  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0796  @ epoch 28932 )\n",
      "[Epoch: 29100] train loss: 0.1004, train acc: 0.9621, val loss: 0.1414, val acc: 0.9562  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0796  @ epoch 28932 )\n",
      "[Epoch: 29120] train loss: 0.0921, train acc: 0.9645, val loss: 0.1436, val acc: 0.9565  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0796  @ epoch 28932 )\n",
      "[Epoch: 29140] train loss: 0.0893, train acc: 0.9682, val loss: 0.1321, val acc: 0.9582  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0796  @ epoch 28932 )\n",
      "[Epoch: 29160] train loss: 0.0854, train acc: 0.9678, val loss: 0.1496, val acc: 0.9541  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0796  @ epoch 28932 )\n",
      "[Epoch: 29180] train loss: 0.0937, train acc: 0.9652, val loss: 0.1370, val acc: 0.9575  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0796  @ epoch 28932 )\n",
      "[Epoch: 29200] train loss: 0.0854, train acc: 0.9690, val loss: 0.1548, val acc: 0.9541  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0796  @ epoch 28932 )\n",
      "[Epoch: 29220] train loss: 0.0887, train acc: 0.9665, val loss: 0.1719, val acc: 0.9460  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0796  @ epoch 28932 )\n",
      "[Epoch: 29240] train loss: 0.0956, train acc: 0.9639, val loss: 0.1598, val acc: 0.9501  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0796  @ epoch 28932 )\n",
      "[Epoch: 29260] train loss: 0.0794, train acc: 0.9700, val loss: 0.1356, val acc: 0.9589  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0794  @ epoch 29260 )\n",
      "[Epoch: 29280] train loss: 0.0879, train acc: 0.9655, val loss: 0.1524, val acc: 0.9538  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0794  @ epoch 29260 )\n",
      "[Epoch: 29300] train loss: 0.0953, train acc: 0.9635, val loss: 0.1496, val acc: 0.9531  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0783  @ epoch 29290 )\n",
      "[Epoch: 29320] train loss: 0.0806, train acc: 0.9715, val loss: 0.1586, val acc: 0.9504  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0783  @ epoch 29290 )\n",
      "[Epoch: 29340] train loss: 0.0845, train acc: 0.9687, val loss: 0.1508, val acc: 0.9575  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0783  @ epoch 29290 )\n",
      "[Epoch: 29360] train loss: 0.0962, train acc: 0.9612, val loss: 0.1340, val acc: 0.9548  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0783  @ epoch 29290 )\n",
      "[Epoch: 29380] train loss: 0.0772, train acc: 0.9700, val loss: 0.1436, val acc: 0.9592  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0772  @ epoch 29380 )\n",
      "[Epoch: 29400] train loss: 0.0924, train acc: 0.9644, val loss: 0.1427, val acc: 0.9592  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0772  @ epoch 29380 )\n",
      "[Epoch: 29420] train loss: 0.0942, train acc: 0.9630, val loss: 0.1468, val acc: 0.9558  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0772  @ epoch 29380 )\n",
      "[Epoch: 29440] train loss: 0.0821, train acc: 0.9694, val loss: 0.1424, val acc: 0.9575  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0772  @ epoch 29380 )\n",
      "[Epoch: 29460] train loss: 0.0914, train acc: 0.9633, val loss: 0.1509, val acc: 0.9572  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0772  @ epoch 29380 )\n",
      "[Epoch: 29480] train loss: 0.0988, train acc: 0.9602, val loss: 0.1484, val acc: 0.9585  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0772  @ epoch 29380 )\n",
      "[Epoch: 29500] train loss: 0.0928, train acc: 0.9652, val loss: 0.1442, val acc: 0.9578  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0772  @ epoch 29380 )\n",
      "[Epoch: 29520] train loss: 0.0901, train acc: 0.9662, val loss: 0.1376, val acc: 0.9589  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0772  @ epoch 29380 )\n",
      "[Epoch: 29540] train loss: 0.0865, train acc: 0.9675, val loss: 0.1453, val acc: 0.9578  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0772  @ epoch 29380 )\n",
      "[Epoch: 29560] train loss: 0.0870, train acc: 0.9683, val loss: 0.1581, val acc: 0.9508  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0772  @ epoch 29380 )\n",
      "[Epoch: 29580] train loss: 0.1039, train acc: 0.9610, val loss: 0.1474, val acc: 0.9562  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0772  @ epoch 29380 )\n",
      "[Epoch: 29600] train loss: 0.0911, train acc: 0.9665, val loss: 0.1737, val acc: 0.9470  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 29620] train loss: 0.0849, train acc: 0.9678, val loss: 0.1463, val acc: 0.9595  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 29640] train loss: 0.0844, train acc: 0.9699, val loss: 0.1432, val acc: 0.9592  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 29660] train loss: 0.1022, train acc: 0.9639, val loss: 0.1616, val acc: 0.9524  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 29680] train loss: 0.0910, train acc: 0.9659, val loss: 0.1490, val acc: 0.9575  (best train acc: 0.9720, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 29700] train loss: 0.0915, train acc: 0.9639, val loss: 0.1614, val acc: 0.9514  (best train acc: 0.9722, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 29720] train loss: 0.1048, train acc: 0.9607, val loss: 0.1587, val acc: 0.9545  (best train acc: 0.9722, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 29740] train loss: 0.0874, train acc: 0.9675, val loss: 0.1748, val acc: 0.9487  (best train acc: 0.9722, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 29760] train loss: 0.1022, train acc: 0.9604, val loss: 0.1484, val acc: 0.9528  (best train acc: 0.9722, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 29780] train loss: 0.0848, train acc: 0.9686, val loss: 0.1543, val acc: 0.9565  (best train acc: 0.9722, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 29800] train loss: 0.1151, train acc: 0.9550, val loss: 0.1592, val acc: 0.9521  (best train acc: 0.9722, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 29820] train loss: 0.0943, train acc: 0.9625, val loss: 0.1458, val acc: 0.9518  (best train acc: 0.9722, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 29840] train loss: 0.0831, train acc: 0.9696, val loss: 0.1467, val acc: 0.9545  (best train acc: 0.9722, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 29860] train loss: 0.0953, train acc: 0.9626, val loss: 0.1420, val acc: 0.9572  (best train acc: 0.9722, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 29880] train loss: 0.1071, train acc: 0.9581, val loss: 0.1471, val acc: 0.9602  (best train acc: 0.9722, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 29900] train loss: 0.1174, train acc: 0.9597, val loss: 0.1367, val acc: 0.9582  (best train acc: 0.9722, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 29920] train loss: 0.1032, train acc: 0.9592, val loss: 0.1503, val acc: 0.9568  (best train acc: 0.9722, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 29940] train loss: 0.0854, train acc: 0.9675, val loss: 0.1394, val acc: 0.9578  (best train acc: 0.9722, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 29960] train loss: 0.0972, train acc: 0.9608, val loss: 0.1498, val acc: 0.9589  (best train acc: 0.9722, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 29980] train loss: 0.0901, train acc: 0.9676, val loss: 0.1428, val acc: 0.9558  (best train acc: 0.9722, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30000] train loss: 0.0965, train acc: 0.9617, val loss: 0.1587, val acc: 0.9541  (best train acc: 0.9722, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30020] train loss: 0.0841, train acc: 0.9693, val loss: 0.1675, val acc: 0.9487  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30040] train loss: 0.0833, train acc: 0.9694, val loss: 0.1488, val acc: 0.9524  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30060] train loss: 0.0847, train acc: 0.9691, val loss: 0.1468, val acc: 0.9592  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30080] train loss: 0.0956, train acc: 0.9638, val loss: 0.1689, val acc: 0.9504  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30100] train loss: 0.0816, train acc: 0.9683, val loss: 0.1568, val acc: 0.9538  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30120] train loss: 0.1126, train acc: 0.9562, val loss: 0.1339, val acc: 0.9535  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30140] train loss: 0.1077, train acc: 0.9633, val loss: 0.1449, val acc: 0.9535  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30160] train loss: 0.1035, train acc: 0.9598, val loss: 0.1538, val acc: 0.9551  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30180] train loss: 0.0930, train acc: 0.9639, val loss: 0.1646, val acc: 0.9494  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30200] train loss: 0.0851, train acc: 0.9679, val loss: 0.1543, val acc: 0.9531  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30220] train loss: 0.1057, train acc: 0.9609, val loss: 0.1508, val acc: 0.9531  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30240] train loss: 0.1067, train acc: 0.9573, val loss: 0.1554, val acc: 0.9585  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30260] train loss: 0.0925, train acc: 0.9660, val loss: 0.1451, val acc: 0.9548  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30280] train loss: 0.0815, train acc: 0.9711, val loss: 0.1497, val acc: 0.9562  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30300] train loss: 0.0840, train acc: 0.9688, val loss: 0.1425, val acc: 0.9558  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30320] train loss: 0.0872, train acc: 0.9663, val loss: 0.1948, val acc: 0.9450  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30340] train loss: 0.0883, train acc: 0.9662, val loss: 0.1581, val acc: 0.9524  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30360] train loss: 0.0860, train acc: 0.9674, val loss: 0.1469, val acc: 0.9582  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30380] train loss: 0.0879, train acc: 0.9656, val loss: 0.1552, val acc: 0.9538  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30400] train loss: 0.0879, train acc: 0.9663, val loss: 0.1651, val acc: 0.9487  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30420] train loss: 0.0905, train acc: 0.9649, val loss: 0.1517, val acc: 0.9562  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30440] train loss: 0.0868, train acc: 0.9686, val loss: 0.1528, val acc: 0.9568  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30460] train loss: 0.0956, train acc: 0.9660, val loss: 0.1512, val acc: 0.9545  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30480] train loss: 0.0843, train acc: 0.9688, val loss: 0.1485, val acc: 0.9518  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30500] train loss: 0.1073, train acc: 0.9614, val loss: 0.1479, val acc: 0.9605  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30520] train loss: 0.0805, train acc: 0.9685, val loss: 0.1597, val acc: 0.9535  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30540] train loss: 0.0974, train acc: 0.9652, val loss: 0.1400, val acc: 0.9578  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30560] train loss: 0.0973, train acc: 0.9649, val loss: 0.1378, val acc: 0.9592  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30580] train loss: 0.0966, train acc: 0.9644, val loss: 0.1498, val acc: 0.9589  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30600] train loss: 0.0869, train acc: 0.9664, val loss: 0.1673, val acc: 0.9501  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30620] train loss: 0.0947, train acc: 0.9656, val loss: 0.1349, val acc: 0.9585  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30640] train loss: 0.1161, train acc: 0.9565, val loss: 0.1426, val acc: 0.9538  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30660] train loss: 0.0939, train acc: 0.9636, val loss: 0.1448, val acc: 0.9572  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30680] train loss: 0.1001, train acc: 0.9647, val loss: 0.1449, val acc: 0.9602  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30700] train loss: 0.1000, train acc: 0.9633, val loss: 0.1720, val acc: 0.9474  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30720] train loss: 0.0958, train acc: 0.9649, val loss: 0.1603, val acc: 0.9535  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30740] train loss: 0.0896, train acc: 0.9654, val loss: 0.1559, val acc: 0.9599  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30760] train loss: 0.0864, train acc: 0.9683, val loss: 0.1560, val acc: 0.9518  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30780] train loss: 0.0917, train acc: 0.9665, val loss: 0.1550, val acc: 0.9551  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30800] train loss: 0.0987, train acc: 0.9628, val loss: 0.1584, val acc: 0.9551  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30820] train loss: 0.0861, train acc: 0.9672, val loss: 0.1623, val acc: 0.9524  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30840] train loss: 0.0951, train acc: 0.9626, val loss: 0.1552, val acc: 0.9589  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30860] train loss: 0.0798, train acc: 0.9720, val loss: 0.1493, val acc: 0.9592  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30880] train loss: 0.0846, train acc: 0.9673, val loss: 0.1762, val acc: 0.9504  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30900] train loss: 0.0963, train acc: 0.9629, val loss: 0.1351, val acc: 0.9568  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30920] train loss: 0.0868, train acc: 0.9683, val loss: 0.1424, val acc: 0.9595  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30940] train loss: 0.0919, train acc: 0.9644, val loss: 0.1442, val acc: 0.9578  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30960] train loss: 0.1066, train acc: 0.9607, val loss: 0.1442, val acc: 0.9578  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 30980] train loss: 0.0880, train acc: 0.9672, val loss: 0.1480, val acc: 0.9531  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 31000] train loss: 0.0771, train acc: 0.9700, val loss: 0.1490, val acc: 0.9575  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 31020] train loss: 0.0824, train acc: 0.9708, val loss: 0.1622, val acc: 0.9508  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 31040] train loss: 0.0855, train acc: 0.9697, val loss: 0.1584, val acc: 0.9545  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 31060] train loss: 0.0876, train acc: 0.9670, val loss: 0.1416, val acc: 0.9578  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 31080] train loss: 0.0873, train acc: 0.9679, val loss: 0.1537, val acc: 0.9575  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0756  @ epoch 29587 )\n",
      "[Epoch: 31100] train loss: 0.0860, train acc: 0.9684, val loss: 0.1502, val acc: 0.9558  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0751  @ epoch 31081 )\n",
      "[Epoch: 31120] train loss: 0.0776, train acc: 0.9701, val loss: 0.1620, val acc: 0.9528  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0751  @ epoch 31081 )\n",
      "[Epoch: 31140] train loss: 0.0952, train acc: 0.9649, val loss: 0.1326, val acc: 0.9575  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0751  @ epoch 31081 )\n",
      "[Epoch: 31160] train loss: 0.0998, train acc: 0.9626, val loss: 0.1530, val acc: 0.9440  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0751  @ epoch 31081 )\n",
      "[Epoch: 31180] train loss: 0.0953, train acc: 0.9617, val loss: 0.1449, val acc: 0.9599  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0751  @ epoch 31081 )\n",
      "[Epoch: 31200] train loss: 0.0891, train acc: 0.9662, val loss: 0.1521, val acc: 0.9538  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0751  @ epoch 31081 )\n",
      "[Epoch: 31220] train loss: 0.0999, train acc: 0.9623, val loss: 0.1344, val acc: 0.9572  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0751  @ epoch 31081 )\n",
      "[Epoch: 31240] train loss: 0.0858, train acc: 0.9667, val loss: 0.1471, val acc: 0.9578  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0751  @ epoch 31081 )\n",
      "[Epoch: 31260] train loss: 0.0895, train acc: 0.9649, val loss: 0.1962, val acc: 0.9477  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0751  @ epoch 31081 )\n",
      "[Epoch: 31280] train loss: 0.0999, train acc: 0.9600, val loss: 0.1638, val acc: 0.9494  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0751  @ epoch 31081 )\n",
      "[Epoch: 31300] train loss: 0.0922, train acc: 0.9644, val loss: 0.1705, val acc: 0.9508  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0751  @ epoch 31081 )\n",
      "[Epoch: 31320] train loss: 0.0865, train acc: 0.9663, val loss: 0.1786, val acc: 0.9514  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0751  @ epoch 31081 )\n",
      "[Epoch: 31340] train loss: 0.0855, train acc: 0.9671, val loss: 0.1536, val acc: 0.9565  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0751  @ epoch 31081 )\n",
      "[Epoch: 31360] train loss: 0.0837, train acc: 0.9687, val loss: 0.1530, val acc: 0.9605  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0751  @ epoch 31081 )\n",
      "[Epoch: 31380] train loss: 0.1100, train acc: 0.9592, val loss: 0.1453, val acc: 0.9602  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0751  @ epoch 31081 )\n",
      "[Epoch: 31400] train loss: 0.0892, train acc: 0.9662, val loss: 0.1544, val acc: 0.9528  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0751  @ epoch 31081 )\n",
      "[Epoch: 31420] train loss: 0.0904, train acc: 0.9644, val loss: 0.1621, val acc: 0.9518  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0751  @ epoch 31081 )\n",
      "[Epoch: 31440] train loss: 0.0804, train acc: 0.9707, val loss: 0.1640, val acc: 0.9548  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0751  @ epoch 31081 )\n",
      "[Epoch: 31460] train loss: 0.0762, train acc: 0.9710, val loss: 0.1534, val acc: 0.9548  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0751  @ epoch 31081 )\n",
      "[Epoch: 31480] train loss: 0.0963, train acc: 0.9641, val loss: 0.1792, val acc: 0.9508  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0751  @ epoch 31081 )\n",
      "[Epoch: 31500] train loss: 0.0829, train acc: 0.9704, val loss: 0.1541, val acc: 0.9531  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0751  @ epoch 31081 )\n",
      "[Epoch: 31520] train loss: 0.0870, train acc: 0.9683, val loss: 0.1364, val acc: 0.9558  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0751  @ epoch 31081 )\n",
      "[Epoch: 31540] train loss: 0.0845, train acc: 0.9684, val loss: 0.1786, val acc: 0.9521  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0751  @ epoch 31081 )\n",
      "[Epoch: 31560] train loss: 0.0882, train acc: 0.9652, val loss: 0.1520, val acc: 0.9535  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0751  @ epoch 31081 )\n",
      "[Epoch: 31580] train loss: 0.0835, train acc: 0.9673, val loss: 0.1444, val acc: 0.9545  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 31600] train loss: 0.0815, train acc: 0.9705, val loss: 0.1577, val acc: 0.9551  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 31620] train loss: 0.0896, train acc: 0.9671, val loss: 0.1704, val acc: 0.9494  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 31640] train loss: 0.0992, train acc: 0.9632, val loss: 0.1405, val acc: 0.9589  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 31660] train loss: 0.0851, train acc: 0.9681, val loss: 0.1595, val acc: 0.9538  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 31680] train loss: 0.0926, train acc: 0.9680, val loss: 0.1787, val acc: 0.9501  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 31700] train loss: 0.0929, train acc: 0.9657, val loss: 0.1509, val acc: 0.9582  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 31720] train loss: 0.0846, train acc: 0.9686, val loss: 0.1422, val acc: 0.9578  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 31740] train loss: 0.0834, train acc: 0.9696, val loss: 0.1426, val acc: 0.9568  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 31760] train loss: 0.0855, train acc: 0.9664, val loss: 0.1531, val acc: 0.9568  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 31780] train loss: 0.0826, train acc: 0.9694, val loss: 0.1638, val acc: 0.9504  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 31800] train loss: 0.0802, train acc: 0.9700, val loss: 0.1521, val acc: 0.9602  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 31820] train loss: 0.0930, train acc: 0.9628, val loss: 0.1530, val acc: 0.9474  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 31840] train loss: 0.0966, train acc: 0.9650, val loss: 0.1409, val acc: 0.9599  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 31860] train loss: 0.0812, train acc: 0.9696, val loss: 0.1530, val acc: 0.9565  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 31880] train loss: 0.0883, train acc: 0.9662, val loss: 0.1607, val acc: 0.9541  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 31900] train loss: 0.0810, train acc: 0.9688, val loss: 0.1563, val acc: 0.9565  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 31920] train loss: 0.0797, train acc: 0.9695, val loss: 0.1400, val acc: 0.9575  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 31940] train loss: 0.0979, train acc: 0.9637, val loss: 0.1605, val acc: 0.9605  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 31960] train loss: 0.0860, train acc: 0.9663, val loss: 0.1412, val acc: 0.9589  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 31980] train loss: 0.1034, train acc: 0.9587, val loss: 0.1549, val acc: 0.9518  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 32000] train loss: 0.0807, train acc: 0.9710, val loss: 0.1468, val acc: 0.9592  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 32020] train loss: 0.1038, train acc: 0.9587, val loss: 0.1508, val acc: 0.9541  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 32040] train loss: 0.0841, train acc: 0.9678, val loss: 0.1343, val acc: 0.9592  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 32060] train loss: 0.0992, train acc: 0.9628, val loss: 0.1467, val acc: 0.9605  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 32080] train loss: 0.0871, train acc: 0.9664, val loss: 0.1525, val acc: 0.9521  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 32100] train loss: 0.0829, train acc: 0.9711, val loss: 0.1532, val acc: 0.9528  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 32120] train loss: 0.1022, train acc: 0.9634, val loss: 0.1494, val acc: 0.9518  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 32140] train loss: 0.1108, train acc: 0.9589, val loss: 0.1457, val acc: 0.9518  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 32160] train loss: 0.0841, train acc: 0.9682, val loss: 0.1674, val acc: 0.9548  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 32180] train loss: 0.0888, train acc: 0.9672, val loss: 0.1527, val acc: 0.9551  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 32200] train loss: 0.0829, train acc: 0.9694, val loss: 0.1445, val acc: 0.9595  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 32220] train loss: 0.0881, train acc: 0.9683, val loss: 0.1704, val acc: 0.9521  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 32240] train loss: 0.0890, train acc: 0.9670, val loss: 0.1634, val acc: 0.9518  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 32260] train loss: 0.0859, train acc: 0.9677, val loss: 0.1542, val acc: 0.9551  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 32280] train loss: 0.0906, train acc: 0.9660, val loss: 0.1615, val acc: 0.9538  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 32300] train loss: 0.0829, train acc: 0.9691, val loss: 0.1551, val acc: 0.9565  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 32320] train loss: 0.0852, train acc: 0.9676, val loss: 0.1501, val acc: 0.9595  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 32340] train loss: 0.0898, train acc: 0.9652, val loss: 0.1627, val acc: 0.9521  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 32360] train loss: 0.0851, train acc: 0.9672, val loss: 0.1710, val acc: 0.9501  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 32380] train loss: 0.0852, train acc: 0.9696, val loss: 0.1506, val acc: 0.9558  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 32400] train loss: 0.0952, train acc: 0.9654, val loss: 0.1545, val acc: 0.9538  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0744  @ epoch 31575 )\n",
      "[Epoch: 32420] train loss: 0.0800, train acc: 0.9688, val loss: 0.1483, val acc: 0.9582  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0743  @ epoch 32417 )\n",
      "[Epoch: 32440] train loss: 0.0864, train acc: 0.9678, val loss: 0.1481, val acc: 0.9592  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0743  @ epoch 32417 )\n",
      "[Epoch: 32460] train loss: 0.0809, train acc: 0.9722, val loss: 0.1454, val acc: 0.9565  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0743  @ epoch 32417 )\n",
      "[Epoch: 32480] train loss: 0.0845, train acc: 0.9680, val loss: 0.1422, val acc: 0.9582  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0743  @ epoch 32417 )\n",
      "[Epoch: 32500] train loss: 0.0933, train acc: 0.9622, val loss: 0.1683, val acc: 0.9501  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0743  @ epoch 32417 )\n",
      "[Epoch: 32520] train loss: 0.1013, train acc: 0.9626, val loss: 0.1417, val acc: 0.9589  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0743  @ epoch 32417 )\n",
      "[Epoch: 32540] train loss: 0.0780, train acc: 0.9717, val loss: 0.1734, val acc: 0.9494  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0743  @ epoch 32417 )\n",
      "[Epoch: 32560] train loss: 0.0861, train acc: 0.9704, val loss: 0.1467, val acc: 0.9602  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0743  @ epoch 32417 )\n",
      "[Epoch: 32580] train loss: 0.0819, train acc: 0.9683, val loss: 0.1408, val acc: 0.9562  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0743  @ epoch 32417 )\n",
      "[Epoch: 32600] train loss: 0.0811, train acc: 0.9696, val loss: 0.1585, val acc: 0.9545  (best train acc: 0.9737, best val acc: 0.9642, best train loss: 0.0743  @ epoch 32417 )\n",
      "[Epoch: 32620] train loss: 0.0921, train acc: 0.9643, val loss: 0.1550, val acc: 0.9551  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0732  @ epoch 32604 )\n",
      "[Epoch: 32640] train loss: 0.0987, train acc: 0.9667, val loss: 0.1471, val acc: 0.9622  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0732  @ epoch 32604 )\n",
      "[Epoch: 32660] train loss: 0.0916, train acc: 0.9651, val loss: 0.1596, val acc: 0.9524  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0732  @ epoch 32604 )\n",
      "[Epoch: 32680] train loss: 0.0877, train acc: 0.9674, val loss: 0.1598, val acc: 0.9521  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 32700] train loss: 0.0849, train acc: 0.9669, val loss: 0.1563, val acc: 0.9568  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 32720] train loss: 0.0919, train acc: 0.9653, val loss: 0.1674, val acc: 0.9535  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 32740] train loss: 0.1003, train acc: 0.9656, val loss: 0.1739, val acc: 0.9481  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 32760] train loss: 0.0854, train acc: 0.9683, val loss: 0.1496, val acc: 0.9575  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 32780] train loss: 0.0895, train acc: 0.9674, val loss: 0.1598, val acc: 0.9585  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 32800] train loss: 0.0896, train acc: 0.9667, val loss: 0.1549, val acc: 0.9562  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 32820] train loss: 0.0843, train acc: 0.9695, val loss: 0.2054, val acc: 0.9383  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 32840] train loss: 0.0848, train acc: 0.9680, val loss: 0.1947, val acc: 0.9373  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 32860] train loss: 0.0964, train acc: 0.9652, val loss: 0.1399, val acc: 0.9575  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 32880] train loss: 0.0887, train acc: 0.9662, val loss: 0.1497, val acc: 0.9568  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 32900] train loss: 0.0810, train acc: 0.9690, val loss: 0.1755, val acc: 0.9477  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 32920] train loss: 0.0933, train acc: 0.9640, val loss: 0.1443, val acc: 0.9585  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 32940] train loss: 0.0869, train acc: 0.9682, val loss: 0.1533, val acc: 0.9548  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 32960] train loss: 0.0843, train acc: 0.9701, val loss: 0.1654, val acc: 0.9521  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 32980] train loss: 0.0807, train acc: 0.9686, val loss: 0.1462, val acc: 0.9595  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33000] train loss: 0.1049, train acc: 0.9583, val loss: 0.1543, val acc: 0.9531  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33020] train loss: 0.0894, train acc: 0.9672, val loss: 0.1799, val acc: 0.9497  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33040] train loss: 0.0828, train acc: 0.9688, val loss: 0.1765, val acc: 0.9538  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33060] train loss: 0.0858, train acc: 0.9683, val loss: 0.1568, val acc: 0.9578  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33080] train loss: 0.0818, train acc: 0.9701, val loss: 0.1665, val acc: 0.9541  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33100] train loss: 0.0910, train acc: 0.9672, val loss: 0.1475, val acc: 0.9565  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33120] train loss: 0.0843, train acc: 0.9705, val loss: 0.1679, val acc: 0.9565  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33140] train loss: 0.0850, train acc: 0.9666, val loss: 0.1567, val acc: 0.9555  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33160] train loss: 0.0818, train acc: 0.9699, val loss: 0.1481, val acc: 0.9568  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33180] train loss: 0.0820, train acc: 0.9688, val loss: 0.1561, val acc: 0.9541  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33200] train loss: 0.0969, train acc: 0.9641, val loss: 0.1709, val acc: 0.9528  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33220] train loss: 0.0937, train acc: 0.9628, val loss: 0.1581, val acc: 0.9528  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33240] train loss: 0.0813, train acc: 0.9701, val loss: 0.1794, val acc: 0.9491  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33260] train loss: 0.0912, train acc: 0.9683, val loss: 0.1567, val acc: 0.9528  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33280] train loss: 0.0931, train acc: 0.9637, val loss: 0.1327, val acc: 0.9582  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33300] train loss: 0.0851, train acc: 0.9660, val loss: 0.1651, val acc: 0.9521  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33320] train loss: 0.0979, train acc: 0.9622, val loss: 0.1831, val acc: 0.9511  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33340] train loss: 0.0848, train acc: 0.9672, val loss: 0.1347, val acc: 0.9585  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33360] train loss: 0.0776, train acc: 0.9722, val loss: 0.1646, val acc: 0.9535  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33380] train loss: 0.0821, train acc: 0.9683, val loss: 0.1754, val acc: 0.9467  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33400] train loss: 0.1045, train acc: 0.9627, val loss: 0.1463, val acc: 0.9592  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33420] train loss: 0.0920, train acc: 0.9659, val loss: 0.1639, val acc: 0.9555  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33440] train loss: 0.0786, train acc: 0.9694, val loss: 0.1507, val acc: 0.9582  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33460] train loss: 0.0792, train acc: 0.9714, val loss: 0.1705, val acc: 0.9535  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33480] train loss: 0.0809, train acc: 0.9697, val loss: 0.1431, val acc: 0.9605  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33500] train loss: 0.0837, train acc: 0.9672, val loss: 0.1631, val acc: 0.9562  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33520] train loss: 0.0882, train acc: 0.9657, val loss: 0.1650, val acc: 0.9565  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33540] train loss: 0.0912, train acc: 0.9654, val loss: 0.1516, val acc: 0.9602  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33560] train loss: 0.0804, train acc: 0.9706, val loss: 0.1565, val acc: 0.9585  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33580] train loss: 0.0900, train acc: 0.9655, val loss: 0.1676, val acc: 0.9528  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33600] train loss: 0.0889, train acc: 0.9688, val loss: 0.1629, val acc: 0.9538  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33620] train loss: 0.0894, train acc: 0.9669, val loss: 0.1557, val acc: 0.9578  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33640] train loss: 0.0858, train acc: 0.9692, val loss: 0.1759, val acc: 0.9511  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33660] train loss: 0.0862, train acc: 0.9680, val loss: 0.1579, val acc: 0.9575  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33680] train loss: 0.0841, train acc: 0.9682, val loss: 0.1433, val acc: 0.9575  (best train acc: 0.9738, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33700] train loss: 0.0879, train acc: 0.9667, val loss: 0.1438, val acc: 0.9578  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33720] train loss: 0.0958, train acc: 0.9651, val loss: 0.1614, val acc: 0.9578  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33740] train loss: 0.1104, train acc: 0.9563, val loss: 0.1937, val acc: 0.9440  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33760] train loss: 0.0991, train acc: 0.9636, val loss: 0.1685, val acc: 0.9501  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33780] train loss: 0.0740, train acc: 0.9739, val loss: 0.1701, val acc: 0.9518  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33800] train loss: 0.0927, train acc: 0.9659, val loss: 0.1650, val acc: 0.9551  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33820] train loss: 0.0983, train acc: 0.9658, val loss: 0.1671, val acc: 0.9545  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33840] train loss: 0.0798, train acc: 0.9700, val loss: 0.1605, val acc: 0.9555  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33860] train loss: 0.0838, train acc: 0.9709, val loss: 0.1790, val acc: 0.9541  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33880] train loss: 0.0790, train acc: 0.9699, val loss: 0.1755, val acc: 0.9528  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33900] train loss: 0.0955, train acc: 0.9654, val loss: 0.1698, val acc: 0.9555  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33920] train loss: 0.1044, train acc: 0.9665, val loss: 0.1493, val acc: 0.9575  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33940] train loss: 0.1031, train acc: 0.9599, val loss: 0.1655, val acc: 0.9497  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33960] train loss: 0.1102, train acc: 0.9537, val loss: 0.1662, val acc: 0.9555  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 33980] train loss: 0.0908, train acc: 0.9661, val loss: 0.1507, val acc: 0.9572  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34000] train loss: 0.0856, train acc: 0.9674, val loss: 0.1468, val acc: 0.9568  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34020] train loss: 0.0782, train acc: 0.9708, val loss: 0.1640, val acc: 0.9545  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34040] train loss: 0.0883, train acc: 0.9680, val loss: 0.1544, val acc: 0.9562  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34060] train loss: 0.0799, train acc: 0.9695, val loss: 0.1517, val acc: 0.9568  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34080] train loss: 0.0849, train acc: 0.9675, val loss: 0.1551, val acc: 0.9585  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34100] train loss: 0.1038, train acc: 0.9584, val loss: 0.1689, val acc: 0.9521  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34120] train loss: 0.0882, train acc: 0.9700, val loss: 0.1452, val acc: 0.9602  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34140] train loss: 0.0759, train acc: 0.9719, val loss: 0.1489, val acc: 0.9565  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34160] train loss: 0.0985, train acc: 0.9636, val loss: 0.1707, val acc: 0.9535  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34180] train loss: 0.0878, train acc: 0.9663, val loss: 0.1613, val acc: 0.9555  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34200] train loss: 0.0843, train acc: 0.9693, val loss: 0.1440, val acc: 0.9595  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34220] train loss: 0.1086, train acc: 0.9614, val loss: 0.1505, val acc: 0.9609  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34240] train loss: 0.0956, train acc: 0.9638, val loss: 0.1805, val acc: 0.9514  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34260] train loss: 0.0814, train acc: 0.9717, val loss: 0.1502, val acc: 0.9578  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34280] train loss: 0.0837, train acc: 0.9686, val loss: 0.1596, val acc: 0.9565  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34300] train loss: 0.0868, train acc: 0.9673, val loss: 0.1571, val acc: 0.9541  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34320] train loss: 0.0769, train acc: 0.9706, val loss: 0.1724, val acc: 0.9565  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34340] train loss: 0.1090, train acc: 0.9592, val loss: 0.1536, val acc: 0.9558  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34360] train loss: 0.0844, train acc: 0.9687, val loss: 0.1681, val acc: 0.9538  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34380] train loss: 0.0834, train acc: 0.9679, val loss: 0.1917, val acc: 0.9464  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34400] train loss: 0.0793, train acc: 0.9699, val loss: 0.1698, val acc: 0.9538  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34420] train loss: 0.0796, train acc: 0.9702, val loss: 0.1640, val acc: 0.9548  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34440] train loss: 0.0877, train acc: 0.9688, val loss: 0.1592, val acc: 0.9562  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34460] train loss: 0.0870, train acc: 0.9699, val loss: 0.1825, val acc: 0.9484  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34480] train loss: 0.0796, train acc: 0.9711, val loss: 0.1469, val acc: 0.9589  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34500] train loss: 0.0867, train acc: 0.9678, val loss: 0.1727, val acc: 0.9528  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34520] train loss: 0.0851, train acc: 0.9670, val loss: 0.1590, val acc: 0.9562  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34540] train loss: 0.0828, train acc: 0.9691, val loss: 0.1616, val acc: 0.9545  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34560] train loss: 0.0971, train acc: 0.9629, val loss: 0.1722, val acc: 0.9524  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34580] train loss: 0.0935, train acc: 0.9631, val loss: 0.1932, val acc: 0.9403  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34600] train loss: 0.0950, train acc: 0.9650, val loss: 0.1592, val acc: 0.9521  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34620] train loss: 0.0794, train acc: 0.9717, val loss: 0.1612, val acc: 0.9599  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34640] train loss: 0.0817, train acc: 0.9680, val loss: 0.1612, val acc: 0.9565  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34660] train loss: 0.0950, train acc: 0.9650, val loss: 0.1559, val acc: 0.9558  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34680] train loss: 0.0833, train acc: 0.9685, val loss: 0.1516, val acc: 0.9572  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34700] train loss: 0.0827, train acc: 0.9701, val loss: 0.1534, val acc: 0.9568  (best train acc: 0.9745, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34720] train loss: 0.0789, train acc: 0.9705, val loss: 0.1440, val acc: 0.9578  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34740] train loss: 0.0854, train acc: 0.9679, val loss: 0.2232, val acc: 0.9366  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34760] train loss: 0.0874, train acc: 0.9657, val loss: 0.1438, val acc: 0.9589  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34780] train loss: 0.0851, train acc: 0.9687, val loss: 0.1649, val acc: 0.9528  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34800] train loss: 0.0822, train acc: 0.9686, val loss: 0.1815, val acc: 0.9487  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34820] train loss: 0.0972, train acc: 0.9645, val loss: 0.1652, val acc: 0.9481  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34840] train loss: 0.0910, train acc: 0.9657, val loss: 0.1813, val acc: 0.9491  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34860] train loss: 0.1124, train acc: 0.9594, val loss: 0.1610, val acc: 0.9541  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34880] train loss: 0.0859, train acc: 0.9687, val loss: 0.1529, val acc: 0.9572  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34900] train loss: 0.0894, train acc: 0.9696, val loss: 0.1295, val acc: 0.9602  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34920] train loss: 0.0931, train acc: 0.9654, val loss: 0.1582, val acc: 0.9538  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34940] train loss: 0.1010, train acc: 0.9624, val loss: 0.1823, val acc: 0.9477  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34960] train loss: 0.0935, train acc: 0.9644, val loss: 0.1610, val acc: 0.9575  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 34980] train loss: 0.0848, train acc: 0.9681, val loss: 0.1806, val acc: 0.9508  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 35000] train loss: 0.0827, train acc: 0.9701, val loss: 0.1576, val acc: 0.9582  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 35020] train loss: 0.0822, train acc: 0.9690, val loss: 0.1557, val acc: 0.9562  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 35040] train loss: 0.0792, train acc: 0.9707, val loss: 0.1557, val acc: 0.9562  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 35060] train loss: 0.0837, train acc: 0.9694, val loss: 0.1718, val acc: 0.9481  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 35080] train loss: 0.0838, train acc: 0.9686, val loss: 0.1641, val acc: 0.9548  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 35100] train loss: 0.0915, train acc: 0.9663, val loss: 0.1674, val acc: 0.9531  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 35120] train loss: 0.0850, train acc: 0.9675, val loss: 0.1542, val acc: 0.9548  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 35140] train loss: 0.0843, train acc: 0.9696, val loss: 0.1446, val acc: 0.9589  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 35160] train loss: 0.0784, train acc: 0.9722, val loss: 0.1478, val acc: 0.9585  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 35180] train loss: 0.0874, train acc: 0.9668, val loss: 0.1706, val acc: 0.9541  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 35200] train loss: 0.0819, train acc: 0.9699, val loss: 0.1560, val acc: 0.9545  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 35220] train loss: 0.0921, train acc: 0.9641, val loss: 0.1510, val acc: 0.9578  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 35240] train loss: 0.0910, train acc: 0.9673, val loss: 0.1578, val acc: 0.9524  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 35260] train loss: 0.0911, train acc: 0.9652, val loss: 0.1692, val acc: 0.9511  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 35280] train loss: 0.0868, train acc: 0.9665, val loss: 0.1514, val acc: 0.9592  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 35300] train loss: 0.0817, train acc: 0.9701, val loss: 0.1705, val acc: 0.9524  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 35320] train loss: 0.0803, train acc: 0.9696, val loss: 0.1586, val acc: 0.9585  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 35340] train loss: 0.0927, train acc: 0.9655, val loss: 0.1862, val acc: 0.9470  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 35360] train loss: 0.0830, train acc: 0.9693, val loss: 0.1755, val acc: 0.9524  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 35380] train loss: 0.0824, train acc: 0.9690, val loss: 0.1756, val acc: 0.9487  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 35400] train loss: 0.0810, train acc: 0.9706, val loss: 0.1596, val acc: 0.9592  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 35420] train loss: 0.0860, train acc: 0.9680, val loss: 0.1504, val acc: 0.9545  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 35440] train loss: 0.0850, train acc: 0.9675, val loss: 0.1589, val acc: 0.9575  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 35460] train loss: 0.0778, train acc: 0.9721, val loss: 0.1584, val acc: 0.9578  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 35480] train loss: 0.0751, train acc: 0.9732, val loss: 0.1584, val acc: 0.9575  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 35500] train loss: 0.0801, train acc: 0.9717, val loss: 0.1549, val acc: 0.9585  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 35520] train loss: 0.0805, train acc: 0.9699, val loss: 0.1923, val acc: 0.9457  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 35540] train loss: 0.0845, train acc: 0.9686, val loss: 0.1703, val acc: 0.9545  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 35560] train loss: 0.0951, train acc: 0.9652, val loss: 0.1479, val acc: 0.9585  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 35580] train loss: 0.0908, train acc: 0.9679, val loss: 0.1367, val acc: 0.9609  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 35600] train loss: 0.0860, train acc: 0.9672, val loss: 0.1828, val acc: 0.9477  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 35620] train loss: 0.0864, train acc: 0.9650, val loss: 0.1698, val acc: 0.9531  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 35640] train loss: 0.0828, train acc: 0.9706, val loss: 0.1413, val acc: 0.9595  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 35660] train loss: 0.0814, train acc: 0.9694, val loss: 0.1608, val acc: 0.9585  (best train acc: 0.9746, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 35680] train loss: 0.0859, train acc: 0.9680, val loss: 0.1608, val acc: 0.9538  (best train acc: 0.9750, best val acc: 0.9642, best train loss: 0.0725  @ epoch 32673 )\n",
      "[Epoch: 35700] train loss: 0.0814, train acc: 0.9705, val loss: 0.1620, val acc: 0.9578  (best train acc: 0.9750, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 35720] train loss: 0.0843, train acc: 0.9689, val loss: 0.1604, val acc: 0.9585  (best train acc: 0.9750, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 35740] train loss: 0.0730, train acc: 0.9759, val loss: 0.1669, val acc: 0.9555  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 35760] train loss: 0.0784, train acc: 0.9715, val loss: 0.1785, val acc: 0.9470  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 35780] train loss: 0.0875, train acc: 0.9675, val loss: 0.1807, val acc: 0.9521  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 35800] train loss: 0.0902, train acc: 0.9664, val loss: 0.1704, val acc: 0.9524  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 35820] train loss: 0.0852, train acc: 0.9670, val loss: 0.1689, val acc: 0.9551  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 35840] train loss: 0.0794, train acc: 0.9699, val loss: 0.1483, val acc: 0.9599  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 35860] train loss: 0.0900, train acc: 0.9677, val loss: 0.1601, val acc: 0.9572  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 35880] train loss: 0.0879, train acc: 0.9684, val loss: 0.1652, val acc: 0.9595  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 35900] train loss: 0.0790, train acc: 0.9709, val loss: 0.1514, val acc: 0.9616  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 35920] train loss: 0.0851, train acc: 0.9701, val loss: 0.1499, val acc: 0.9582  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 35940] train loss: 0.0903, train acc: 0.9649, val loss: 0.1552, val acc: 0.9599  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 35960] train loss: 0.0880, train acc: 0.9688, val loss: 0.1507, val acc: 0.9562  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 35980] train loss: 0.0852, train acc: 0.9694, val loss: 0.1610, val acc: 0.9582  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 36000] train loss: 0.0723, train acc: 0.9740, val loss: 0.1836, val acc: 0.9535  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 36020] train loss: 0.0786, train acc: 0.9707, val loss: 0.1649, val acc: 0.9555  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 36040] train loss: 0.0814, train acc: 0.9689, val loss: 0.1588, val acc: 0.9599  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 36060] train loss: 0.0803, train acc: 0.9695, val loss: 0.1757, val acc: 0.9524  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 36080] train loss: 0.0850, train acc: 0.9681, val loss: 0.1591, val acc: 0.9602  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 36100] train loss: 0.0907, train acc: 0.9660, val loss: 0.1605, val acc: 0.9545  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 36120] train loss: 0.0857, train acc: 0.9700, val loss: 0.1416, val acc: 0.9585  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 36140] train loss: 0.0760, train acc: 0.9725, val loss: 0.1569, val acc: 0.9558  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 36160] train loss: 0.0883, train acc: 0.9662, val loss: 0.1730, val acc: 0.9491  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 36180] train loss: 0.0871, train acc: 0.9683, val loss: 0.1985, val acc: 0.9410  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 36200] train loss: 0.0956, train acc: 0.9672, val loss: 0.1524, val acc: 0.9589  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 36220] train loss: 0.0835, train acc: 0.9686, val loss: 0.1641, val acc: 0.9535  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 36240] train loss: 0.0792, train acc: 0.9704, val loss: 0.1602, val acc: 0.9555  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 36260] train loss: 0.0789, train acc: 0.9722, val loss: 0.1752, val acc: 0.9551  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 36280] train loss: 0.0910, train acc: 0.9678, val loss: 0.1615, val acc: 0.9592  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 36300] train loss: 0.1004, train acc: 0.9605, val loss: 0.1506, val acc: 0.9545  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 36320] train loss: 0.0859, train acc: 0.9662, val loss: 0.1721, val acc: 0.9575  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 36340] train loss: 0.0861, train acc: 0.9695, val loss: 0.1558, val acc: 0.9541  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 36360] train loss: 0.1156, train acc: 0.9570, val loss: 0.1680, val acc: 0.9508  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 36380] train loss: 0.0935, train acc: 0.9647, val loss: 0.1308, val acc: 0.9632  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 36400] train loss: 0.0799, train acc: 0.9722, val loss: 0.1699, val acc: 0.9514  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 36420] train loss: 0.0787, train acc: 0.9696, val loss: 0.1547, val acc: 0.9602  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 36440] train loss: 0.0921, train acc: 0.9638, val loss: 0.1762, val acc: 0.9460  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 36460] train loss: 0.0927, train acc: 0.9630, val loss: 0.1696, val acc: 0.9508  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 36480] train loss: 0.0832, train acc: 0.9691, val loss: 0.1532, val acc: 0.9585  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 36500] train loss: 0.0776, train acc: 0.9719, val loss: 0.1693, val acc: 0.9545  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 36520] train loss: 0.0900, train acc: 0.9667, val loss: 0.1618, val acc: 0.9555  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0716  @ epoch 35683 )\n",
      "[Epoch: 36540] train loss: 0.0822, train acc: 0.9686, val loss: 0.1574, val acc: 0.9551  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 36560] train loss: 0.0817, train acc: 0.9691, val loss: 0.1657, val acc: 0.9578  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 36580] train loss: 0.0822, train acc: 0.9695, val loss: 0.1548, val acc: 0.9565  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 36600] train loss: 0.0793, train acc: 0.9708, val loss: 0.1721, val acc: 0.9572  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 36620] train loss: 0.0851, train acc: 0.9690, val loss: 0.1570, val acc: 0.9568  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 36640] train loss: 0.0952, train acc: 0.9652, val loss: 0.1543, val acc: 0.9605  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 36660] train loss: 0.0783, train acc: 0.9704, val loss: 0.1640, val acc: 0.9558  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 36680] train loss: 0.1169, train acc: 0.9589, val loss: 0.1448, val acc: 0.9609  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 36700] train loss: 0.0933, train acc: 0.9650, val loss: 0.1562, val acc: 0.9538  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 36720] train loss: 0.0836, train acc: 0.9691, val loss: 0.1476, val acc: 0.9599  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 36740] train loss: 0.0972, train acc: 0.9606, val loss: 0.1587, val acc: 0.9578  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 36760] train loss: 0.1179, train acc: 0.9559, val loss: 0.1428, val acc: 0.9575  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 36780] train loss: 0.0856, train acc: 0.9685, val loss: 0.1624, val acc: 0.9538  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 36800] train loss: 0.0738, train acc: 0.9723, val loss: 0.1412, val acc: 0.9589  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 36820] train loss: 0.0740, train acc: 0.9714, val loss: 0.1532, val acc: 0.9582  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 36840] train loss: 0.0846, train acc: 0.9684, val loss: 0.1699, val acc: 0.9528  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 36860] train loss: 0.0768, train acc: 0.9717, val loss: 0.1671, val acc: 0.9541  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 36880] train loss: 0.0936, train acc: 0.9635, val loss: 0.1530, val acc: 0.9568  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 36900] train loss: 0.0891, train acc: 0.9669, val loss: 0.1906, val acc: 0.9484  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 36920] train loss: 0.0913, train acc: 0.9640, val loss: 0.1574, val acc: 0.9595  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 36940] train loss: 0.0749, train acc: 0.9725, val loss: 0.1545, val acc: 0.9558  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 36960] train loss: 0.0869, train acc: 0.9679, val loss: 0.1597, val acc: 0.9585  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 36980] train loss: 0.0919, train acc: 0.9667, val loss: 0.1637, val acc: 0.9578  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 37000] train loss: 0.0791, train acc: 0.9699, val loss: 0.1560, val acc: 0.9602  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 37020] train loss: 0.0758, train acc: 0.9734, val loss: 0.1516, val acc: 0.9592  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 37040] train loss: 0.0853, train acc: 0.9713, val loss: 0.1554, val acc: 0.9578  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 37060] train loss: 0.1021, train acc: 0.9641, val loss: 0.1725, val acc: 0.9433  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 37080] train loss: 0.0740, train acc: 0.9733, val loss: 0.1629, val acc: 0.9541  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 37100] train loss: 0.0767, train acc: 0.9719, val loss: 0.1938, val acc: 0.9444  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 37120] train loss: 0.0913, train acc: 0.9658, val loss: 0.1483, val acc: 0.9538  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 37140] train loss: 0.0858, train acc: 0.9658, val loss: 0.2048, val acc: 0.9427  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 37160] train loss: 0.0836, train acc: 0.9683, val loss: 0.1518, val acc: 0.9605  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 37180] train loss: 0.0888, train acc: 0.9679, val loss: 0.1673, val acc: 0.9454  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 37200] train loss: 0.0727, train acc: 0.9743, val loss: 0.1755, val acc: 0.9538  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 37220] train loss: 0.1000, train acc: 0.9615, val loss: 0.1591, val acc: 0.9578  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 37240] train loss: 0.0883, train acc: 0.9652, val loss: 0.1514, val acc: 0.9565  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 37260] train loss: 0.0779, train acc: 0.9727, val loss: 0.1601, val acc: 0.9562  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 37280] train loss: 0.0803, train acc: 0.9722, val loss: 0.1693, val acc: 0.9511  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 37300] train loss: 0.0887, train acc: 0.9649, val loss: 0.1842, val acc: 0.9477  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 37320] train loss: 0.0804, train acc: 0.9711, val loss: 0.1704, val acc: 0.9578  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 37340] train loss: 0.0877, train acc: 0.9692, val loss: 0.1472, val acc: 0.9562  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 37360] train loss: 0.0764, train acc: 0.9699, val loss: 0.1630, val acc: 0.9555  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 37380] train loss: 0.0808, train acc: 0.9712, val loss: 0.1774, val acc: 0.9545  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 37400] train loss: 0.0878, train acc: 0.9666, val loss: 0.1581, val acc: 0.9575  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 37420] train loss: 0.0781, train acc: 0.9692, val loss: 0.1827, val acc: 0.9494  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 37440] train loss: 0.0939, train acc: 0.9668, val loss: 0.1836, val acc: 0.9497  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 37460] train loss: 0.0872, train acc: 0.9696, val loss: 0.1832, val acc: 0.9504  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 37480] train loss: 0.0827, train acc: 0.9715, val loss: 0.1556, val acc: 0.9612  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 37500] train loss: 0.0903, train acc: 0.9674, val loss: 0.1572, val acc: 0.9572  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 37520] train loss: 0.0891, train acc: 0.9659, val loss: 0.1458, val acc: 0.9609  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 37540] train loss: 0.0883, train acc: 0.9664, val loss: 0.1532, val acc: 0.9568  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 37560] train loss: 0.0973, train acc: 0.9669, val loss: 0.1479, val acc: 0.9578  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 37580] train loss: 0.0936, train acc: 0.9630, val loss: 0.1685, val acc: 0.9524  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 37600] train loss: 0.0841, train acc: 0.9665, val loss: 0.1562, val acc: 0.9605  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 37620] train loss: 0.0812, train acc: 0.9701, val loss: 0.1670, val acc: 0.9521  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 37640] train loss: 0.1050, train acc: 0.9601, val loss: 0.1778, val acc: 0.9518  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 37660] train loss: 0.0871, train acc: 0.9675, val loss: 0.1733, val acc: 0.9504  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 37680] train loss: 0.0817, train acc: 0.9708, val loss: 0.1560, val acc: 0.9565  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0710  @ epoch 36526 )\n",
      "[Epoch: 37700] train loss: 0.0833, train acc: 0.9704, val loss: 0.1570, val acc: 0.9568  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0707  @ epoch 37694 )\n",
      "[Epoch: 37720] train loss: 0.0871, train acc: 0.9673, val loss: 0.1858, val acc: 0.9501  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0707  @ epoch 37694 )\n",
      "[Epoch: 37740] train loss: 0.0805, train acc: 0.9699, val loss: 0.1563, val acc: 0.9558  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0707  @ epoch 37694 )\n",
      "[Epoch: 37760] train loss: 0.0842, train acc: 0.9675, val loss: 0.1640, val acc: 0.9545  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0707  @ epoch 37694 )\n",
      "[Epoch: 37780] train loss: 0.0850, train acc: 0.9669, val loss: 0.1567, val acc: 0.9629  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0707  @ epoch 37694 )\n",
      "[Epoch: 37800] train loss: 0.0770, train acc: 0.9725, val loss: 0.1443, val acc: 0.9592  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0707  @ epoch 37694 )\n",
      "[Epoch: 37820] train loss: 0.0797, train acc: 0.9712, val loss: 0.1706, val acc: 0.9562  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0707  @ epoch 37694 )\n",
      "[Epoch: 37840] train loss: 0.0777, train acc: 0.9723, val loss: 0.1559, val acc: 0.9595  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0707  @ epoch 37694 )\n",
      "[Epoch: 37860] train loss: 0.0755, train acc: 0.9733, val loss: 0.1579, val acc: 0.9612  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0707  @ epoch 37694 )\n",
      "[Epoch: 37880] train loss: 0.0980, train acc: 0.9618, val loss: 0.1643, val acc: 0.9524  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0707  @ epoch 37694 )\n",
      "[Epoch: 37900] train loss: 0.0747, train acc: 0.9703, val loss: 0.1645, val acc: 0.9562  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0707  @ epoch 37694 )\n",
      "[Epoch: 37920] train loss: 0.1037, train acc: 0.9627, val loss: 0.1723, val acc: 0.9572  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0707  @ epoch 37694 )\n",
      "[Epoch: 37940] train loss: 0.0831, train acc: 0.9684, val loss: 0.1773, val acc: 0.9555  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0707  @ epoch 37694 )\n",
      "[Epoch: 37960] train loss: 0.0898, train acc: 0.9665, val loss: 0.1616, val acc: 0.9558  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0707  @ epoch 37694 )\n",
      "[Epoch: 37980] train loss: 0.0894, train acc: 0.9659, val loss: 0.1776, val acc: 0.9524  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0707  @ epoch 37694 )\n",
      "[Epoch: 38000] train loss: 0.1029, train acc: 0.9616, val loss: 0.1506, val acc: 0.9609  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0707  @ epoch 37694 )\n",
      "[Epoch: 38020] train loss: 0.0843, train acc: 0.9677, val loss: 0.1620, val acc: 0.9578  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0707  @ epoch 37694 )\n",
      "[Epoch: 38040] train loss: 0.0794, train acc: 0.9704, val loss: 0.1718, val acc: 0.9558  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0707  @ epoch 37694 )\n",
      "[Epoch: 38060] train loss: 0.0743, train acc: 0.9733, val loss: 0.1804, val acc: 0.9511  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0707  @ epoch 37694 )\n",
      "[Epoch: 38080] train loss: 0.1006, train acc: 0.9645, val loss: 0.1839, val acc: 0.9565  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0707  @ epoch 37694 )\n",
      "[Epoch: 38100] train loss: 0.0742, train acc: 0.9728, val loss: 0.1600, val acc: 0.9592  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0707  @ epoch 37694 )\n",
      "[Epoch: 38120] train loss: 0.0735, train acc: 0.9733, val loss: 0.1686, val acc: 0.9609  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0707  @ epoch 37694 )\n",
      "[Epoch: 38140] train loss: 0.0933, train acc: 0.9657, val loss: 0.1670, val acc: 0.9555  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0707  @ epoch 37694 )\n",
      "[Epoch: 38160] train loss: 0.0892, train acc: 0.9637, val loss: 0.1574, val acc: 0.9541  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0707  @ epoch 37694 )\n",
      "[Epoch: 38180] train loss: 0.0909, train acc: 0.9662, val loss: 0.1545, val acc: 0.9589  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0707  @ epoch 37694 )\n",
      "[Epoch: 38200] train loss: 0.0861, train acc: 0.9699, val loss: 0.1670, val acc: 0.9602  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0707  @ epoch 37694 )\n",
      "[Epoch: 38220] train loss: 0.0702, train acc: 0.9753, val loss: 0.1600, val acc: 0.9612  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0702  @ epoch 38220 )\n",
      "[Epoch: 38240] train loss: 0.0953, train acc: 0.9633, val loss: 0.1569, val acc: 0.9562  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0702  @ epoch 38220 )\n",
      "[Epoch: 38260] train loss: 0.0779, train acc: 0.9712, val loss: 0.1648, val acc: 0.9548  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0702  @ epoch 38220 )\n",
      "[Epoch: 38280] train loss: 0.0822, train acc: 0.9687, val loss: 0.2079, val acc: 0.9491  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0702  @ epoch 38220 )\n",
      "[Epoch: 38300] train loss: 0.0835, train acc: 0.9677, val loss: 0.1895, val acc: 0.9477  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0702  @ epoch 38220 )\n",
      "[Epoch: 38320] train loss: 0.0858, train acc: 0.9685, val loss: 0.1652, val acc: 0.9595  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0702  @ epoch 38220 )\n",
      "[Epoch: 38340] train loss: 0.0804, train acc: 0.9697, val loss: 0.1674, val acc: 0.9585  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0702  @ epoch 38220 )\n",
      "[Epoch: 38360] train loss: 0.0743, train acc: 0.9730, val loss: 0.1770, val acc: 0.9535  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0702  @ epoch 38220 )\n",
      "[Epoch: 38380] train loss: 0.0863, train acc: 0.9657, val loss: 0.1937, val acc: 0.9487  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0702  @ epoch 38220 )\n",
      "[Epoch: 38400] train loss: 0.0829, train acc: 0.9678, val loss: 0.1488, val acc: 0.9609  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0702  @ epoch 38220 )\n",
      "[Epoch: 38420] train loss: 0.0893, train acc: 0.9674, val loss: 0.1704, val acc: 0.9575  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0702  @ epoch 38220 )\n",
      "[Epoch: 38440] train loss: 0.0813, train acc: 0.9699, val loss: 0.1704, val acc: 0.9578  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0700  @ epoch 38424 )\n",
      "[Epoch: 38460] train loss: 0.0808, train acc: 0.9701, val loss: 0.1691, val acc: 0.9562  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0700  @ epoch 38424 )\n",
      "[Epoch: 38480] train loss: 0.0747, train acc: 0.9711, val loss: 0.1557, val acc: 0.9605  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0699  @ epoch 38465 )\n",
      "[Epoch: 38500] train loss: 0.0890, train acc: 0.9658, val loss: 0.1456, val acc: 0.9612  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0699  @ epoch 38465 )\n",
      "[Epoch: 38520] train loss: 0.0899, train acc: 0.9652, val loss: 0.1656, val acc: 0.9595  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0699  @ epoch 38465 )\n",
      "[Epoch: 38540] train loss: 0.0797, train acc: 0.9696, val loss: 0.1763, val acc: 0.9518  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0699  @ epoch 38465 )\n",
      "[Epoch: 38560] train loss: 0.0893, train acc: 0.9669, val loss: 0.1487, val acc: 0.9609  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0699  @ epoch 38465 )\n",
      "[Epoch: 38580] train loss: 0.0858, train acc: 0.9688, val loss: 0.1834, val acc: 0.9521  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0699  @ epoch 38465 )\n",
      "[Epoch: 38600] train loss: 0.0865, train acc: 0.9667, val loss: 0.1720, val acc: 0.9548  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0699  @ epoch 38465 )\n",
      "[Epoch: 38620] train loss: 0.0775, train acc: 0.9726, val loss: 0.1624, val acc: 0.9568  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0699  @ epoch 38465 )\n",
      "[Epoch: 38640] train loss: 0.0824, train acc: 0.9689, val loss: 0.1814, val acc: 0.9551  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0699  @ epoch 38465 )\n",
      "[Epoch: 38660] train loss: 0.0942, train acc: 0.9656, val loss: 0.1679, val acc: 0.9562  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0699  @ epoch 38465 )\n",
      "[Epoch: 38680] train loss: 0.0768, train acc: 0.9735, val loss: 0.1561, val acc: 0.9595  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 38700] train loss: 0.0773, train acc: 0.9699, val loss: 0.1665, val acc: 0.9605  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 38720] train loss: 0.0849, train acc: 0.9704, val loss: 0.1735, val acc: 0.9582  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 38740] train loss: 0.0749, train acc: 0.9722, val loss: 0.1531, val acc: 0.9616  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 38760] train loss: 0.0790, train acc: 0.9691, val loss: 0.1639, val acc: 0.9632  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 38780] train loss: 0.0793, train acc: 0.9712, val loss: 0.1653, val acc: 0.9568  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 38800] train loss: 0.0780, train acc: 0.9686, val loss: 0.1825, val acc: 0.9497  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 38820] train loss: 0.0781, train acc: 0.9713, val loss: 0.1745, val acc: 0.9514  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 38840] train loss: 0.0714, train acc: 0.9732, val loss: 0.1913, val acc: 0.9491  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 38860] train loss: 0.0818, train acc: 0.9691, val loss: 0.1952, val acc: 0.9497  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 38880] train loss: 0.0801, train acc: 0.9703, val loss: 0.1793, val acc: 0.9562  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 38900] train loss: 0.1028, train acc: 0.9630, val loss: 0.1692, val acc: 0.9595  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 38920] train loss: 0.0755, train acc: 0.9710, val loss: 0.1618, val acc: 0.9535  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 38940] train loss: 0.0853, train acc: 0.9667, val loss: 0.1458, val acc: 0.9575  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 38960] train loss: 0.0897, train acc: 0.9679, val loss: 0.1592, val acc: 0.9612  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 38980] train loss: 0.0824, train acc: 0.9686, val loss: 0.1768, val acc: 0.9582  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39000] train loss: 0.0764, train acc: 0.9709, val loss: 0.1746, val acc: 0.9511  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39020] train loss: 0.0806, train acc: 0.9720, val loss: 0.1634, val acc: 0.9622  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39040] train loss: 0.0975, train acc: 0.9634, val loss: 0.1606, val acc: 0.9582  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39060] train loss: 0.0746, train acc: 0.9728, val loss: 0.1662, val acc: 0.9582  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39080] train loss: 0.0885, train acc: 0.9672, val loss: 0.1726, val acc: 0.9504  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39100] train loss: 0.0982, train acc: 0.9646, val loss: 0.1505, val acc: 0.9592  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39120] train loss: 0.0742, train acc: 0.9725, val loss: 0.1618, val acc: 0.9599  (best train acc: 0.9759, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39140] train loss: 0.0818, train acc: 0.9711, val loss: 0.1541, val acc: 0.9626  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39160] train loss: 0.0867, train acc: 0.9687, val loss: 0.1802, val acc: 0.9467  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39180] train loss: 0.0827, train acc: 0.9688, val loss: 0.1701, val acc: 0.9609  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39200] train loss: 0.0755, train acc: 0.9708, val loss: 0.1603, val acc: 0.9562  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39220] train loss: 0.0978, train acc: 0.9590, val loss: 0.1760, val acc: 0.9497  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39240] train loss: 0.0778, train acc: 0.9704, val loss: 0.1746, val acc: 0.9555  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39260] train loss: 0.0950, train acc: 0.9620, val loss: 0.1795, val acc: 0.9538  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39280] train loss: 0.0868, train acc: 0.9665, val loss: 0.1933, val acc: 0.9518  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39300] train loss: 0.0898, train acc: 0.9644, val loss: 0.1556, val acc: 0.9565  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39320] train loss: 0.0785, train acc: 0.9711, val loss: 0.1633, val acc: 0.9612  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39340] train loss: 0.0794, train acc: 0.9730, val loss: 0.1602, val acc: 0.9609  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39360] train loss: 0.0962, train acc: 0.9642, val loss: 0.1664, val acc: 0.9511  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39380] train loss: 0.0792, train acc: 0.9716, val loss: 0.1563, val acc: 0.9592  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39400] train loss: 0.0885, train acc: 0.9678, val loss: 0.1805, val acc: 0.9558  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39420] train loss: 0.0979, train acc: 0.9634, val loss: 0.1501, val acc: 0.9599  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39440] train loss: 0.0773, train acc: 0.9722, val loss: 0.1751, val acc: 0.9562  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39460] train loss: 0.0788, train acc: 0.9704, val loss: 0.1964, val acc: 0.9470  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39480] train loss: 0.1008, train acc: 0.9626, val loss: 0.1598, val acc: 0.9501  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39500] train loss: 0.0806, train acc: 0.9700, val loss: 0.1731, val acc: 0.9602  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39520] train loss: 0.0843, train acc: 0.9691, val loss: 0.1727, val acc: 0.9602  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39540] train loss: 0.0936, train acc: 0.9622, val loss: 0.1917, val acc: 0.9491  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39560] train loss: 0.0902, train acc: 0.9659, val loss: 0.1737, val acc: 0.9528  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39580] train loss: 0.0821, train acc: 0.9694, val loss: 0.1586, val acc: 0.9602  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39600] train loss: 0.0807, train acc: 0.9688, val loss: 0.1424, val acc: 0.9595  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39620] train loss: 0.0813, train acc: 0.9712, val loss: 0.1695, val acc: 0.9605  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39640] train loss: 0.0740, train acc: 0.9733, val loss: 0.1800, val acc: 0.9548  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39660] train loss: 0.0763, train acc: 0.9719, val loss: 0.1642, val acc: 0.9592  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39680] train loss: 0.0859, train acc: 0.9676, val loss: 0.1763, val acc: 0.9535  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39700] train loss: 0.0787, train acc: 0.9699, val loss: 0.1721, val acc: 0.9572  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39720] train loss: 0.0876, train acc: 0.9655, val loss: 0.1635, val acc: 0.9595  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39740] train loss: 0.0768, train acc: 0.9731, val loss: 0.1680, val acc: 0.9578  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39760] train loss: 0.0817, train acc: 0.9706, val loss: 0.1715, val acc: 0.9602  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39780] train loss: 0.0774, train acc: 0.9719, val loss: 0.1883, val acc: 0.9538  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39800] train loss: 0.0770, train acc: 0.9733, val loss: 0.1691, val acc: 0.9592  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39820] train loss: 0.0759, train acc: 0.9720, val loss: 0.1674, val acc: 0.9562  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39840] train loss: 0.0832, train acc: 0.9694, val loss: 0.1753, val acc: 0.9508  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39860] train loss: 0.0803, train acc: 0.9711, val loss: 0.1672, val acc: 0.9551  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39880] train loss: 0.0805, train acc: 0.9701, val loss: 0.1670, val acc: 0.9575  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39900] train loss: 0.0834, train acc: 0.9716, val loss: 0.1568, val acc: 0.9602  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39920] train loss: 0.0882, train acc: 0.9660, val loss: 0.1846, val acc: 0.9514  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39940] train loss: 0.0784, train acc: 0.9714, val loss: 0.1660, val acc: 0.9622  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39960] train loss: 0.0813, train acc: 0.9673, val loss: 0.1536, val acc: 0.9612  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 39980] train loss: 0.0947, train acc: 0.9626, val loss: 0.1786, val acc: 0.9565  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n",
      "[Epoch: 40000] train loss: 0.0895, train acc: 0.9674, val loss: 0.1712, val acc: 0.9562  (best train acc: 0.9760, best val acc: 0.9642, best train loss: 0.0692  @ epoch 38678 )\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       154\n",
      "           1       0.96      0.98      0.97       703\n",
      "           2       0.88      0.94      0.91       702\n",
      "           3       0.97      0.88      0.92       703\n",
      "           4       0.99      1.00      1.00       702\n",
      "\n",
      "    accuracy                           0.95      2964\n",
      "   macro avg       0.96      0.96      0.96      2964\n",
      "weighted avg       0.95      0.95      0.95      2964\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gnn_sage = GNN7L_Sage(data_with_nedbit).to(device)\n",
    "pred = train(gnn_sage, data_with_nedbit.to(device), 40000, cm_title='SAGE7L_multiclass_16HC_v2_max', classes=['P', 'LP', 'WN', 'LN', 'RN'], weight_decay=0.0005)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6aeea6fe202f5c7201d5940e4573c0a76b23e4e16f0e3784ac81597546f2b3b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
