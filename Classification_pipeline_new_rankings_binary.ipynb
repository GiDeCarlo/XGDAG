{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GNNTrain import train, predict_from_saved_model\n",
    "from CreateDatasetv2_binary import get_dataset_from_graph\n",
    "from Paths import PATH_TO_GRAPHS, PATH_TO_RANKINGS\n",
    "from GDARanking import get_ranking, predict_candidate_genes, validate_with_extended_dataset, get_ranking_no_LP_intersection, validate_with_extended_dataset_no_LP\n",
    "from GraphSageModel import GNN7L_Sage\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_Id  = 'C0006142'\n",
    "classes     = ['P', 'U']\n",
    "model_name  = 'GraphSAGE_' + disease_Id + '_new_rankings_binary'\n",
    "METHOD     = 'GraphSVX_only'\n",
    "graph_path  = PATH_TO_GRAPHS + 'grafo_nedbit_' + disease_Id + '.gml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Reading graph...ok\n",
      "[+] Creating dataset...ok\n",
      "[i] Elapsed time: 28.879\n"
     ]
    }
   ],
   "source": [
    "dataset, G = get_dataset_from_graph(graph_path, disease_Id, quartile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr              = 0.001\n",
    "# epochs          = 40000\n",
    "# weight_decay    = 0.0005\n",
    "# classes         = ['P', 'U']\n",
    "\n",
    "# model = GNN7L_Sage(dataset)\n",
    "# preds = train(model, dataset, epochs, lr, weight_decay, classes, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get predictions from trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       154\n",
      "           1       1.00      1.00      1.00      2810\n",
      "\n",
      "    accuracy                           1.00      2964\n",
      "   macro avg       1.00      1.00      1.00      2964\n",
      "weighted avg       1.00      1.00      1.00      2964\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds, probs, model = predict_from_saved_model(model_name + '_40000_0_0005', dataset, classes, save_to_file=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('precision_positive_genes.csv') \n",
    "# n_positives = n_positives = df['n_positives'][df[disease_Id].idxmax()]\n",
    "\n",
    "# preds, probs, model = predict_from_saved_model(model_name+'_40000_0_0005', dataset, classes, save_to_file=False, plot_results=False)\n",
    "\n",
    "# ranking = get_ranking(model, dataset, preds, probs, disease_Id, n_positive=n_positives, explanation_nodes_ratio=1, masks_for_seed=10, G=G)\n",
    "\n",
    "# ### Save ranking to file\n",
    "# filename = PATH_TO_RANKINGS + disease_Id + '_' + str(n_positives) + '_new_rankings.txt'\n",
    "# with open(filename, 'w') as f:\n",
    "#     for line in ranking:\n",
    "#         f.write(line + '\\n')\n",
    "\n",
    "# cuts = [25, 50, 100, 200, 500]\n",
    "# for k in cuts:\n",
    "#     precision = validate_with_extended_dataset(ranking[:k], disease_Id, save_ranking_to_file=False)\n",
    "#     print('[+] Precision on top', k, ':', precision/k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking - No LP intersection\n",
    "Considering all genes in explanations subgraph with score >= mean score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('precision_positive_genes.csv') \n",
    "# n_positives = n_positives = df['n_positives'][df[disease_Id].idxmax()]\n",
    "\n",
    "# preds, probs, model = predict_from_saved_model(model_name+'_40000_0_0005', dataset, classes, save_to_file=False, plot_results=False)\n",
    "\n",
    "# ranking = get_ranking_no_LP_intersection(model, dataset, preds, probs, disease_Id, n_positive=n_positives, explanation_nodes_ratio=1, masks_for_seed=10, G=G)\n",
    "\n",
    "# ### Save ranking to file\n",
    "# filename = PATH_TO_RANKINGS + disease_Id + '_' + str(n_positives) + '_new_rankings_no_LP_intersection_10_masks.txt'\n",
    "# with open(filename, 'w') as f:\n",
    "#     for line in ranking:\n",
    "#         f.write(line + '\\n')\n",
    "\n",
    "# cuts = [25, 50, 100, 200, 500]\n",
    "# for k in cuts:\n",
    "#     precision = validate_with_extended_dataset_no_LP(ranking[:k], disease_Id, save_ranking_to_file=True)\n",
    "#     print('[+] Precision on top', k, ':', precision/k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking - No LP intersection all positive genes\n",
    "Considering all genes in explanations subgraph with score >= mean score. We explain all positive test genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranking = get_ranking_from_all_positives_no_LP_intersection(model, dataset, preds, disease_Id, explanation_nodes_ratio=1, masks_for_seed=5, G=G)\n",
    "\n",
    "# ### Save ranking to file\n",
    "# filename = PATH_TO_RANKINGS + disease_Id + '_all_positives_new_ranking_no_lp_intersection.txt'\n",
    "# with open(filename, 'w') as f:\n",
    "#      for line in ranking:\n",
    "#         f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All positive genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all\n",
      "[+] 1025 positive nodes found in the graph\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06353df0c59349c8ad2c78b030367b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1025 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explainations only consider graph structure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:09, 11.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n",
      "Explainations only consider graph structure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:09, 10.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "Explainations only consider graph structure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:30,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334\n",
      "Explainations only consider graph structure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "ranking = predict_candidate_genes(model, dataset, preds, disease_Id, \"graphsvx_only\", explanation_nodes_ratio=1, masks_for_seed=5, num_hops = 1, G=G, num_pos=\"all\", threshold = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save ranking to file\n",
    "filename = PATH_TO_RANKINGS + disease_Id + '_all_positives_new_ranking_' + METHOD.lower().remove(\"_only\") + '.txt'\n",
    "with open(filename, 'w') as f:\n",
    "     for line in ranking:\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Precision on top 25 : 0.68\n",
      "[+] Precision on top 50 : 0.72\n",
      "[+] Precision on top 100 : 0.66\n",
      "[+] Precision on top 200 : 0.645\n",
      "[+] Precision on top 500 : 0.574\n"
     ]
    }
   ],
   "source": [
    "# cuts = [25, 50, 100, 200, 500]\n",
    "# for k in cuts:\n",
    "#     precision = validate_with_extended_dataset(ranking[:k], disease_Id, save_ranking_to_file=False)\n",
    "#     print('[+] Precision on top', k, ':', precision/k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('xgdag')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95e74f98ff60f14b2a40b3a815b098ff5be6a65f27ff7ed052a7e49eb675036c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
